<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module4-vla/chapter1" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 1: Introduction to Vision-Language-Action Models | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://samreensaif.github.io/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://samreensaif.github.io/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://samreensaif.github.io/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter1"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 1: Introduction to Vision-Language-Action Models | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Learning Objectives"><meta data-rh="true" property="og:description" content="Learning Objectives"><link data-rh="true" rel="icon" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://samreensaif.github.io/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter1"><link data-rh="true" rel="alternate" href="https://samreensaif.github.io/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter1" hreflang="en"><link data-rh="true" rel="alternate" href="https://samreensaif.github.io/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter1" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Chapter 1: Introduction to Vision-Language-Action Models","item":"https://samreensaif.github.io/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter1"}]}</script><link rel="alternate" type="application/rss+xml" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Atom Feed">




<script>window.CHATBOT_API_URL="https://hackathon-one-physical-ai-humanoid.onrender.com"</script><link rel="stylesheet" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/assets/css/styles.ad0fe63a.css">
<script src="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/assets/js/runtime~main.61373576.js" defer="defer"></script>
<script src="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/assets/js/main.b522c92f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/"><div class="navbar__logo"><img src="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/img/logo.svg" alt="Physical AI Textbook Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/img/logo.svg" alt="Physical AI Textbook Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module1-ros2/chapter1">Tutorial</a><a class="navbar__item navbar__link" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module1-ros2/chapter1"><span title="Module 1: ROS2" class="categoryLinkLabel_W154">Module 1: ROS2</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module2-simulation/chapter1"><span title="Module 2: Simulation" class="categoryLinkLabel_W154">Module 2: Simulation</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module3-isaac/chapter1"><span title="Module 3: Isaac" class="categoryLinkLabel_W154">Module 3: Isaac</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter1"><span title="Module 4: VLA" class="categoryLinkLabel_W154">Module 4: VLA</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter1"><span title="Chapter 1: Introduction to Vision-Language-Action Models" class="linkLabel_WmDU">Chapter 1: Introduction to Vision-Language-Action Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter2"><span title="Chapter 2: Voice to Action with Whisper" class="linkLabel_WmDU">Chapter 2: Voice to Action with Whisper</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter3"><span title="Chapter 3: Cognitive Planning with LLMs" class="linkLabel_WmDU">Chapter 3: Cognitive Planning with LLMs</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter4"><span title="Chapter 4: Capstone Project â€” The Autonomous Humanoid" class="linkLabel_WmDU">Chapter 4: Capstone Project â€” The Autonomous Humanoid</span></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 4: VLA</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Chapter 1: Introduction to Vision-Language-Action Models</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><div class="container_sRRF"><div class="row_Wfea"><button class="translateBtn_zwOG">ğŸŒ Translate to Urdu</button></div></div>
<header><h1>Chapter 1: Introduction to Vision-Language-Action Models</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">â€‹</a></h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="">Define Vision-Language-Action (VLA) models and explain how they differ from previous approaches to robot control</li>
<li class="">Trace the intellectual lineage from large language models to robot foundation models</li>
<li class="">Describe the architectural components of a VLA model: the vision encoder, language backbone, and action decoder</li>
<li class="">Compare major VLA systems (RT-2, OpenVLA, Ï€0, Octo) across training data, architecture, and capability</li>
<li class="">Explain the Open X-Embodiment dataset and why cross-embodiment training matters</li>
<li class="">Identify the principal unsolved challenges in VLA research: generalisation, speed, safety, and embodiment mismatch</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="11-the-grand-challenge-of-robot-intelligence">1.1 The Grand Challenge of Robot Intelligence<a href="#11-the-grand-challenge-of-robot-intelligence" class="hash-link" aria-label="Direct link to 1.1 The Grand Challenge of Robot Intelligence" title="Direct link to 1.1 The Grand Challenge of Robot Intelligence" translate="no">â€‹</a></h2>
<p>Consider what a two-year-old human child can do that no robot could reliably replicate as recently as 2021:</p>
<ul>
<li class="">Pick up a toy it has never seen before</li>
<li class="">Follow the instruction &quot;put the blue thing near the plant&quot;</li>
<li class="">Recover gracefully when it drops something</li>
<li class="">Understand that &quot;the cup is empty&quot; means it should refill it</li>
</ul>
<p>Each of these requires the seamless integration of <strong>vision</strong> (perceiving the world), <strong>language</strong> (understanding intent), and <strong>action</strong> (physically manipulating objects). No amount of explicit programming captures the full breadth of common-sense understanding these tasks demand.</p>
<p>The field&#x27;s dominant response before 2022 was to treat these as separate engineering problems: a perception module, a natural language parser, a task planner, and a motion controller â€” all hand-designed and hand-integrated. This modular approach produces systems that work well in controlled conditions but fail spectacularly outside their training distribution.</p>
<p><strong>Vision-Language-Action (VLA) models</strong> represent a fundamentally different approach: train a single neural network â€” large enough to absorb enormous amounts of data â€” end-to-end to map raw sensory inputs and language instructions directly to robot actions. The bet is that sufficient scale and diverse data will produce emergent generalisation that no modular pipeline can match.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="12-from-language-models-to-robot-foundation-models">1.2 From Language Models to Robot Foundation Models<a href="#12-from-language-models-to-robot-foundation-models" class="hash-link" aria-label="Direct link to 1.2 From Language Models to Robot Foundation Models" title="Direct link to 1.2 From Language Models to Robot Foundation Models" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-foundation-model-revolution-20172022">The Foundation Model Revolution (2017â€“2022)<a href="#the-foundation-model-revolution-20172022" class="hash-link" aria-label="Direct link to The Foundation Model Revolution (2017â€“2022)" title="Direct link to The Foundation Model Revolution (2017â€“2022)" translate="no">â€‹</a></h3>
<p>The breakthrough that made VLA possible was the development of large <strong>foundation models</strong> â€” neural networks trained on internet-scale data that learn rich, general-purpose representations.</p>
<p>The key milestones:</p>
<table><thead><tr><th>Year</th><th>Model</th><th>Significance</th></tr></thead><tbody><tr><td>2017</td><td>Transformer (Vaswani et al.)</td><td>Architecture that scales to billion+ parameters</td></tr><tr><td>2020</td><td>GPT-3</td><td>First demonstration that scale produces emergent reasoning</td></tr><tr><td>2021</td><td>CLIP (Radford et al.)</td><td>Joint vision-language embedding space via contrastive learning</td></tr><tr><td>2022</td><td>ChatGPT</td><td>Instruction-following via RLHF; language AI becomes mainstream</td></tr><tr><td>2022</td><td><strong>RT-1</strong> (Google)</td><td>First large-scale robot transformer; 130,000 demonstrations</td></tr><tr><td>2023</td><td><strong>RT-2</strong> (Google DeepMind)</td><td>VLM weights initialised from web data, fine-tuned on robot data</td></tr><tr><td>2023</td><td><strong>Octo</strong> (Berkeley)</td><td>Open-source robot foundation model</td></tr><tr><td>2024</td><td><strong>Ï€0 (pi-zero)</strong> (Physical Intelligence)</td><td>First VLA for dexterous manipulation at commercial scale</td></tr><tr><td>2024</td><td><strong>OpenVLA</strong></td><td>Open-source VLA trained on Open X-Embodiment</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-language-matters-for-robots">Why Language Matters for Robots<a href="#why-language-matters-for-robots" class="hash-link" aria-label="Direct link to Why Language Matters for Robots" title="Direct link to Why Language Matters for Robots" translate="no">â€‹</a></h3>
<p>Language is not just a convenience interface. It is the <strong>compressed representation of human intent and world knowledge</strong>. When a model trained on internet text learns that &quot;the mug is to the left of the laptop,&quot; it encodes spatial relationships, object permanence, and contextual understanding that would take years to manually program.</p>
<p>VLA models exploit this by initialising robot controllers from pretrained vision-language models (VLMs), which already understand the visual and semantic structure of the world. Robot-specific fine-tuning then teaches the model to translate this understanding into actions.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="13-the-vla-architecture">1.3 The VLA Architecture<a href="#13-the-vla-architecture" class="hash-link" aria-label="Direct link to 1.3 The VLA Architecture" title="Direct link to 1.3 The VLA Architecture" translate="no">â€‹</a></h2>
<p>A VLA model has three functional components, though they are typically implemented as a single end-to-end model:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                        VLA Model Architecture                         â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                                                                       â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  Inputs:                                                              â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚  Camera Images â”‚  â”‚  Language Instruction                     â”‚   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚  (RGB or RGBD) â”‚  â”‚  &quot;Pick up the red cup and place it on     â”‚   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚  256Ã—256Ã—3     â”‚  â”‚   the tray to the right of the plate&quot;    â”‚   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚          â”‚                              â”‚                             â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚          â–¼                              â–¼                             â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚ Vision Encoderâ”‚              â”‚ Text Tokeniserâ”‚                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚ (ViT-B/16 or  â”‚              â”‚ (BPE, 32k     â”‚                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚  SigLIP)      â”‚              â”‚  vocabulary)  â”‚                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚               â”‚              â”‚               â”‚                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚ Outputs:      â”‚              â”‚ Outputs:      â”‚                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚ 256 patch     â”‚              â”‚ N language    â”‚                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚ tokens        â”‚              â”‚ tokens        â”‚                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚          â”‚                              â”‚                             â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                         â–¼                                             â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  Language Model  â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  Backbone        â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  (LLaMA-7B /     â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚   PaLI-X / etc.) â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚                  â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  Processes joint â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  vision+language â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  token sequence  â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                         â”‚                                             â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                         â–¼                                             â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  Action Head /   â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  Decoder         â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚                  â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  Converts LM     â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  output tokens   â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  to robot joint  â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  positions or    â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  end-effector    â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â”‚  deltas          â”‚                                   â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚                         â”‚                                             â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  Output:                â–¼                                             â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚  Robot Action: [Î”x, Î”y, Î”z, Î”roll, Î”pitch, Î”yaw, gripper]  â”‚    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â”‚  7-dimensional end-effector delta @ 5â€“50 Hz                  â”‚    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-vision-encoder">The Vision Encoder<a href="#the-vision-encoder" class="hash-link" aria-label="Direct link to The Vision Encoder" title="Direct link to The Vision Encoder" translate="no">â€‹</a></h3>
<p>The vision encoder converts pixel images into a sequence of <strong>patch tokens</strong> â€” compact vector representations of image regions. The standard approach is a <strong>Vision Transformer (ViT)</strong>:</p>
<ul>
<li class="">The image (e.g. 224Ã—224) is divided into a grid of 16Ã—16 patches â†’ 196 patches</li>
<li class="">Each patch is projected to a high-dimensional vector (typically 768 or 1024 dimensions)</li>
<li class="">Positional embeddings are added and the sequence is processed by transformer layers</li>
</ul>
<p><strong>SigLIP</strong> (Sigmoid Loss for Language-Image Pre-training) is the vision encoder used in OpenVLA and many recent VLA systems. It produces higher-quality image representations than CLIP for robotic manipulation tasks.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-language-backbone">The Language Backbone<a href="#the-language-backbone" class="hash-link" aria-label="Direct link to The Language Backbone" title="Direct link to The Language Backbone" translate="no">â€‹</a></h3>
<p>The core of a VLA model is a <strong>large language model</strong> (LLaMA, PaLM, Gemma, etc.) pre-trained on internet text and image-text pairs. The vision tokens are prepended to the language token sequence, allowing the model to attend jointly to visual and linguistic context.</p>
<p>This is the key insight of VLA models: <strong>vision tokens are treated exactly like language tokens</strong>. The LLM attends across both, allowing visual grounding â€” understanding that &quot;the blue one&quot; refers to the blue object in the image, not some abstract &quot;blue thing.&quot;</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-action-head">The Action Head<a href="#the-action-head" class="hash-link" aria-label="Direct link to The Action Head" title="Direct link to The Action Head" translate="no">â€‹</a></h3>
<p>The action head is a small network attached to the LLM&#x27;s output that maps language model hidden states to robot actions. Two strategies are common:</p>
<p><strong>Tokenised actions</strong>: Actions are discretised into bins (e.g. 256 bins per dimension) and represented as special vocabulary tokens. The LLM&#x27;s output distribution over its vocabulary implicitly encodes action distributions. RT-2 and OpenVLA use this approach.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">LLM output tokens â†’ &quot;ACTION_234 ACTION_156 ACTION_089 ...&quot; â†’ decode bins â†’ [Î”x, Î”y, Î”z, ...]</span><br></span></code></pre></div></div>
<p><strong>Continuous action diffusion</strong>: A diffusion model (like in Ï€0) takes the LLM&#x27;s hidden states as conditioning and denoises random Gaussian noise into a continuous action trajectory. This produces smoother, more dexterous actions.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="14-landmark-vla-systems">1.4 Landmark VLA Systems<a href="#14-landmark-vla-systems" class="hash-link" aria-label="Direct link to 1.4 Landmark VLA Systems" title="Direct link to 1.4 Landmark VLA Systems" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="rt-2-robotic-transformer-2-google-deepmind-2023">RT-2: Robotic Transformer 2 (Google DeepMind, 2023)<a href="#rt-2-robotic-transformer-2-google-deepmind-2023" class="hash-link" aria-label="Direct link to RT-2: Robotic Transformer 2 (Google DeepMind, 2023)" title="Direct link to RT-2: Robotic Transformer 2 (Google DeepMind, 2023)" translate="no">â€‹</a></h3>
<p>RT-2 was the first large-scale demonstration that a web-pretrained VLM could be fine-tuned into a competent robot controller.</p>
<p><strong>Architecture</strong>: Built on PaLI-X (55B parameters), a VLM pretrained on billions of image-text pairs from the web.</p>
<p><strong>Training</strong>: Co-trained on both the original VLM data (web text + images) and robot demonstration data from a fleet of 13 robots collecting data continuously in Google&#x27;s offices. Crucially, the robot actions were serialised as text strings and included directly in the LLM&#x27;s training vocabulary.</p>
<p><strong>Key capabilities</strong>:</p>
<ul>
<li class=""><strong>Emergent semantic reasoning</strong>: When asked &quot;pick up the object that could be used to extinguish a fire,&quot; RT-2 correctly identifies and picks up a toy water bottle â€” a capability never explicitly demonstrated in training.</li>
<li class=""><strong>Chain-of-thought reasoning</strong>: With a special prompt, RT-2 can generate intermediate reasoning steps (&quot;the water bottle is on the left, I should move the arm to the left...&quot;) before outputting actions.</li>
<li class=""><strong>Cross-embodiment</strong>: Skills learned on one robot arm partially transfer to other arm configurations.</li>
</ul>
<p><strong>Limitations</strong>: 55B parameters requires a server-side GPU; cannot run on-robot. 3 Hz inference rate is too slow for fast manipulation.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="openvla-berkeley-2024">OpenVLA (Berkeley, 2024)<a href="#openvla-berkeley-2024" class="hash-link" aria-label="Direct link to OpenVLA (Berkeley, 2024)" title="Direct link to OpenVLA (Berkeley, 2024)" translate="no">â€‹</a></h3>
<p>OpenVLA is the open-source community&#x27;s answer to RT-2 â€” a 7.5B parameter VLA built on the Prismatic VLM and trained on the Open X-Embodiment dataset.</p>
<p><strong>Architecture</strong>: Prismatic (SigLIP vision encoder + LLaMA-2-7B backbone) + tokenised action head.</p>
<p><strong>Training data</strong>: The full Open X-Embodiment dataset â€” 970,000 robot demonstrations across 22 embodiments (robot arms, mobile manipulators) and 50+ environments.</p>
<p><strong>Why OpenVLA matters</strong>:</p>
<ul>
<li class="">Fully open weights (Apache 2.0 license)</li>
<li class="">Can be fine-tuned on custom robot data with 4Ã— A100 GPUs in ~24 hours</li>
<li class="">Reproducible training pipeline with documented hyperparameters</li>
<li class="">7.5B parameters fits on a single A100 80GB GPU for inference</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">OpenVLA performance vs RT-2-E (55B):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Google robot manipulation tasks:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    RT-2-E:  62% success</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    OpenVLA: 56% success  (with 8Ã— fewer parameters)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="Ï€0-physical-intelligence-2024">Ï€0 (Physical Intelligence, 2024)<a href="#Ï€0-physical-intelligence-2024" class="hash-link" aria-label="Direct link to Ï€0 (Physical Intelligence, 2024)" title="Direct link to Ï€0 (Physical Intelligence, 2024)" translate="no">â€‹</a></h3>
<p>Ï€0 (&quot;pi-zero&quot;) represents the next generation: a VLA designed specifically for <strong>dexterous manipulation</strong> at the pace required for real-world deployment.</p>
<p><strong>Key innovation</strong>: Ï€0 uses a <strong>flow matching</strong> action decoder (a form of diffusion) instead of tokenised actions. This produces smooth, 50 Hz action trajectories suitable for tasks like folding laundry, assembling objects, and packaging items.</p>
<p><strong>Training scale</strong>: Trained on data from 7 different robot types covering 68 tasks â€” the largest cross-embodiment dataset for manipulation at the time of release.</p>
<p><strong>Capabilities</strong>: Ï€0 can fold T-shirts, assemble a circuit board, pack a grocery bag, and clean a kitchen â€” tasks that previous robot systems could not approach.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="octo-uc-berkeley-2023">Octo (UC Berkeley, 2023)<a href="#octo-uc-berkeley-2023" class="hash-link" aria-label="Direct link to Octo (UC Berkeley, 2023)" title="Direct link to Octo (UC Berkeley, 2023)" translate="no">â€‹</a></h3>
<p>Octo is a smaller (93M parameters), faster open-source robot transformer designed for real-time on-robot deployment.</p>
<p><strong>Architecture</strong>: Sequence-to-action transformer trained on Open X-Embodiment. Inputs: images + language goals. Output: action chunks (multiple steps at once, for smoother motion).</p>
<p><strong>Key design choices</strong>:</p>
<ul>
<li class=""><strong>Action chunking</strong>: predicts 4 future actions at once, reducing the effective inference frequency requirement to 12 Hz from 50 Hz</li>
<li class="">Small enough to run on a laptop GPU (RTX 3080) at 10+ Hz</li>
<li class="">Easy fine-tuning on custom data in ~2 hours on a single GPU</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="15-training-data-the-open-x-embodiment-dataset">1.5 Training Data: The Open X-Embodiment Dataset<a href="#15-training-data-the-open-x-embodiment-dataset" class="hash-link" aria-label="Direct link to 1.5 Training Data: The Open X-Embodiment Dataset" title="Direct link to 1.5 Training Data: The Open X-Embodiment Dataset" translate="no">â€‹</a></h2>
<p>VLA models are data-hungry. The <strong>Open X-Embodiment (OXE)</strong> dataset, published by a consortium of 33 research labs, is the most important public training corpus:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Open X-Embodiment Dataset (2024):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Total demonstrations:  970,000+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Robot types:           22 embodiments</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Robot arms: Franka, UR5, Kuka, xArm, Jaco</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Mobile manipulators: RT-1 robot, Stretch, HSR</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Dexterous hands: Allegro, Shadow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Tasks:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Pick and place (most common)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Drawer/cabinet opening</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Folding, wiping, pouring</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Navigation + manipulation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Environments:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Kitchen counters</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Office desks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Factory bins</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â€¢ Lab benches</span><br></span></code></pre></div></div>
<p><strong>Why cross-embodiment training matters</strong>: A model trained only on Franka Panda demonstrations learns Franka-specific biases â€” specific workspace geometry, gripper size, torque profiles. Training across 22 robot types forces the model to learn embodiment-invariant representations of tasks (what a &quot;pick&quot; motion looks like conceptually) separate from embodiment-specific representations (how this specific robot executes it). This generalises better to new robots.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="16-the-inference-pipeline">1.6 The Inference Pipeline<a href="#16-the-inference-pipeline" class="hash-link" aria-label="Direct link to 1.6 The Inference Pipeline" title="Direct link to 1.6 The Inference Pipeline" translate="no">â€‹</a></h2>
<p>At inference time (on a deployed robot), a VLA model runs the following pipeline every control step:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Conceptual VLA inference loop (pseudocode)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">vla_control_loop</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">vla_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> instruction</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">str</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> camera_topic</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">str</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Runs the VLA model at control frequency.</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    In practice this runs inside a ROS 2 node.</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">while</span><span class="token plain"> robot_is_running</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 1. Capture current image</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        image </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> capture_camera_frame</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">camera_topic</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">   </span><span class="token comment" style="color:#999988;font-style:italic"># (H, W, 3) numpy array</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 2. Encode inputs</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        vision_tokens </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> vla_model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">encode_image</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">image</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">         </span><span class="token comment" style="color:#999988;font-style:italic"># (N_vis, D)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        language_tokens </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> vla_model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">tokenize</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">instruction</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">     </span><span class="token comment" style="color:#999988;font-style:italic"># (N_lang,)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 3. Forward pass through LLM backbone</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Vision and language tokens are concatenated and processed jointly</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        joint_tokens </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cat</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">vision_tokens</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> language_tokens</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dim</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        hidden_states </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> vla_model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backbone</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">joint_tokens</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic"># (N, D)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 4. Decode action</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        action </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> vla_model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">action_head</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">hidden_states</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># action shape: (7,) = [Î”x, Î”y, Î”z, Î”roll, Î”pitch, Î”yaw, gripper]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 5. Send to robot</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        robot</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">apply_action</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">action</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 6. Check termination</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> task_complete</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">image</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> instruction</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">break</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="latency-breakdown">Latency Breakdown<a href="#latency-breakdown" class="hash-link" aria-label="Direct link to Latency Breakdown" title="Direct link to Latency Breakdown" translate="no">â€‹</a></h3>
<p>The bottleneck in VLA inference is the LLM backbone:</p>
<table><thead><tr><th>Component</th><th>Time (A100 GPU)</th><th>Time (Jetson Orin)</th></tr></thead><tbody><tr><td>Image capture + preprocessing</td><td>~1 ms</td><td>~5 ms</td></tr><tr><td>Vision encoder (SigLIP)</td><td>~15 ms</td><td>~80 ms</td></tr><tr><td>LLM backbone (7B params, int4)</td><td>~50 ms</td><td>~500 ms</td></tr><tr><td>Action head</td><td>~1 ms</td><td>~2 ms</td></tr><tr><td><strong>Total (effective Hz)</strong></td><td><strong>~15 Hz</strong></td><td><strong>~1.6 Hz</strong></td></tr></tbody></table>
<p>This latency gap is why production humanoid deployments (Figure, Agility, Boston Dynamics) run the VLA on a server or powerful onboard GPU and use a faster local controller for high-frequency actuation.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="17-principal-research-challenges">1.7 Principal Research Challenges<a href="#17-principal-research-challenges" class="hash-link" aria-label="Direct link to 1.7 Principal Research Challenges" title="Direct link to 1.7 Principal Research Challenges" translate="no">â€‹</a></h2>
<p>VLA models are impressive but far from solved. Understanding the open problems is essential context for any practitioner:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-inference-speed">1. Inference Speed<a href="#1-inference-speed" class="hash-link" aria-label="Direct link to 1. Inference Speed" title="Direct link to 1. Inference Speed" translate="no">â€‹</a></h3>
<p>Current 7B-parameter VLA models run at 10â€“15 Hz on an A100 GPU. Robot manipulation often requires 50â€“100 Hz. Solutions under active research:</p>
<ul>
<li class=""><strong>Speculative decoding</strong>: draft small model, verify with large model</li>
<li class=""><strong>Action chunking</strong>: predict K future steps, execute between inference calls</li>
<li class=""><strong>Distillation</strong>: train a small fast student model from a large slow teacher</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-generalisation-vs-overfitting">2. Generalisation vs Overfitting<a href="#2-generalisation-vs-overfitting" class="hash-link" aria-label="Direct link to 2. Generalisation vs Overfitting" title="Direct link to 2. Generalisation vs Overfitting" translate="no">â€‹</a></h3>
<p>VLA models trained on narrow datasets overfit to specific object appearances, lighting conditions, and environments. Solutions:</p>
<ul>
<li class="">Larger, more diverse training datasets</li>
<li class="">Synthetic data augmentation (Isaac Sim Replicator â€” Module 3, Chapter 2)</li>
<li class="">In-context learning: feed demonstrations at inference time</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-safety-and-predictability">3. Safety and Predictability<a href="#3-safety-and-predictability" class="hash-link" aria-label="Direct link to 3. Safety and Predictability" title="Direct link to 3. Safety and Predictability" translate="no">â€‹</a></h3>
<p>An LLM backbone is inherently stochastic. For a language chatbot, an unexpected output is annoying. For a robot arm near a human, it can be dangerous. Active research areas:</p>
<ul>
<li class="">Formal verification of action bounds</li>
<li class="">Uncertainty estimation and safe fallback to classical controllers</li>
<li class="">Constrained decoding (never output actions outside joint limits)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-long-horizon-planning">4. Long-Horizon Planning<a href="#4-long-horizon-planning" class="hash-link" aria-label="Direct link to 4. Long-Horizon Planning" title="Direct link to 4. Long-Horizon Planning" translate="no">â€‹</a></h3>
<p>Current VLA models typically execute 10â€“30 second tasks. For long-horizon tasks (&quot;prepare breakfast&quot;), they struggle with:</p>
<ul>
<li class="">Maintaining task context over many steps</li>
<li class="">Error recovery when a sub-task fails</li>
<li class="">Hierarchical decomposition into sub-goals</li>
</ul>
<p>The hybrid approach (VLA for primitive skills + LLM planner for task decomposition) explored in Chapter 3 of this module addresses this limitation.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="18-the-vla-landscape-a-taxonomy">1.8 The VLA Landscape: A Taxonomy<a href="#18-the-vla-landscape-a-taxonomy" class="hash-link" aria-label="Direct link to 1.8 The VLA Landscape: A Taxonomy" title="Direct link to 1.8 The VLA Landscape: A Taxonomy" translate="no">â€‹</a></h2>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Robot Control Approaches</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”œâ”€â”€ Classical (explicit programming)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚   â”œâ”€â”€ Pros: predictable, fast, safe</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚   â””â”€â”€ Cons: brittle, requires exact world model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”œâ”€â”€ Modular Learning (separate perception + planning + control)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚   â”œâ”€â”€ Pros: each component improvable independently</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚   â””â”€â”€ Cons: compounding errors, no emergent generalisation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">â””â”€â”€ End-to-End (VLA â€” this module)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â”œâ”€â”€ Behaviour Cloning VLA (RT-2, OpenVLA, Octo)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â”‚   â”œâ”€â”€ Train: imitation learning from demonstrations</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â”‚   â””â”€â”€ Strong on tasks similar to training data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â”œâ”€â”€ RL-augmented VLA (Ï€0 + RL fine-tuning)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â”‚   â”œâ”€â”€ Train: BC pretraining + RL on target tasks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â”‚   â””â”€â”€ Better generalisation, harder to train</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â”‚</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    â””â”€â”€ Hierarchical (LLM planner + VLA primitive executor)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        â”œâ”€â”€ LLM: high-level task decomposition</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        â””â”€â”€ VLA: low-level primitive execution</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            â†’ This is the architecture we build in Chapter 4</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="19-chapter-summary">1.9 Chapter Summary<a href="#19-chapter-summary" class="hash-link" aria-label="Direct link to 1.9 Chapter Summary" title="Direct link to 1.9 Chapter Summary" translate="no">â€‹</a></h2>
<p>Vision-Language-Action models represent the frontier of robot intelligence, integrating three previously separate fields â€” computer vision, natural language processing, and robot control â€” into unified learned systems.</p>
<ol>
<li class="">
<p><strong>VLA models</strong> map camera images and language instructions directly to robot actions via a joint vision-language model backbone.</p>
</li>
<li class="">
<p><strong>The architecture</strong> consists of a vision encoder (ViT/SigLIP), a language model backbone (LLaMA, PaLM), and an action head (tokenised or diffusion-based).</p>
</li>
<li class="">
<p><strong>RT-2</strong> demonstrated that web-pretrained VLMs can be fine-tuned into competent robot controllers with emergent semantic reasoning. <strong>OpenVLA</strong> made this accessible with open weights. <strong>Ï€0</strong> demonstrated dexterous manipulation via flow matching actions.</p>
</li>
<li class="">
<p><strong>Open X-Embodiment</strong> (970,000 demonstrations across 22 robot types) is the key training dataset enabling cross-embodiment generalisation.</p>
</li>
<li class="">
<p><strong>Open challenges</strong>: inference speed, generalisation beyond training distribution, safety guarantees, and long-horizon planning.</p>
</li>
<li class="">
<p>In this module, we build a <strong>hierarchical VLA system</strong>: voice commands processed by Whisper â†’ high-level planning by an LLM â†’ low-level execution via ROS 2 Nav2 and manipulation primitives.</p>
</li>
</ol>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="review-questions">Review Questions<a href="#review-questions" class="hash-link" aria-label="Direct link to Review Questions" title="Direct link to Review Questions" translate="no">â€‹</a></h2>
<ol>
<li class="">
<p>Explain in your own words why treating vision tokens and language tokens identically within a transformer backbone is a meaningful design choice for robot control. What does it allow the model to do that separate vision and language encoders cannot?</p>
</li>
<li class="">
<p>Compare OpenVLA and Ï€0 on three dimensions: model size, action representation, and target task complexity. Which would you choose for: (a) a pick-and-place task on a Franka arm; (b) folding a shirt; (c) navigating a mobile robot through a building?</p>
</li>
<li class="">
<p>RT-2&#x27;s paper reports that the model can correctly respond to &quot;pick up the object you would use to clean the table&quot; without ever having seen this exact instruction in training. What capability of the pretrained LLM backbone makes this possible, and what does it demonstrate about emergent generalisation?</p>
</li>
<li class="">
<p>The action chunking technique (predicting K future steps at once) is used in both Octo and Diffusion Policy. Explain what problem it solves and what trade-off it introduces.</p>
</li>
<li class="">
<p>A robotics startup proposes deploying a 7B-parameter VLA model on a Jetson Orin AGX for real-time manipulation at 30 Hz. Based on the latency breakdown in Section 1.6, is this feasible? What architectural choices or hardware changes could make it feasible?</p>
</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla/chapter1.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module3-isaac/chapter4"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 4: Nav2 and Path Planning for Humanoids</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chapter 2: Voice to Action with Whisper</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#11-the-grand-challenge-of-robot-intelligence" class="table-of-contents__link toc-highlight">1.1 The Grand Challenge of Robot Intelligence</a></li><li><a href="#12-from-language-models-to-robot-foundation-models" class="table-of-contents__link toc-highlight">1.2 From Language Models to Robot Foundation Models</a><ul><li><a href="#the-foundation-model-revolution-20172022" class="table-of-contents__link toc-highlight">The Foundation Model Revolution (2017â€“2022)</a></li><li><a href="#why-language-matters-for-robots" class="table-of-contents__link toc-highlight">Why Language Matters for Robots</a></li></ul></li><li><a href="#13-the-vla-architecture" class="table-of-contents__link toc-highlight">1.3 The VLA Architecture</a><ul><li><a href="#the-vision-encoder" class="table-of-contents__link toc-highlight">The Vision Encoder</a></li><li><a href="#the-language-backbone" class="table-of-contents__link toc-highlight">The Language Backbone</a></li><li><a href="#the-action-head" class="table-of-contents__link toc-highlight">The Action Head</a></li></ul></li><li><a href="#14-landmark-vla-systems" class="table-of-contents__link toc-highlight">1.4 Landmark VLA Systems</a><ul><li><a href="#rt-2-robotic-transformer-2-google-deepmind-2023" class="table-of-contents__link toc-highlight">RT-2: Robotic Transformer 2 (Google DeepMind, 2023)</a></li><li><a href="#openvla-berkeley-2024" class="table-of-contents__link toc-highlight">OpenVLA (Berkeley, 2024)</a></li><li><a href="#Ï€0-physical-intelligence-2024" class="table-of-contents__link toc-highlight">Ï€0 (Physical Intelligence, 2024)</a></li><li><a href="#octo-uc-berkeley-2023" class="table-of-contents__link toc-highlight">Octo (UC Berkeley, 2023)</a></li></ul></li><li><a href="#15-training-data-the-open-x-embodiment-dataset" class="table-of-contents__link toc-highlight">1.5 Training Data: The Open X-Embodiment Dataset</a></li><li><a href="#16-the-inference-pipeline" class="table-of-contents__link toc-highlight">1.6 The Inference Pipeline</a><ul><li><a href="#latency-breakdown" class="table-of-contents__link toc-highlight">Latency Breakdown</a></li></ul></li><li><a href="#17-principal-research-challenges" class="table-of-contents__link toc-highlight">1.7 Principal Research Challenges</a><ul><li><a href="#1-inference-speed" class="table-of-contents__link toc-highlight">1. Inference Speed</a></li><li><a href="#2-generalisation-vs-overfitting" class="table-of-contents__link toc-highlight">2. Generalisation vs Overfitting</a></li><li><a href="#3-safety-and-predictability" class="table-of-contents__link toc-highlight">3. Safety and Predictability</a></li><li><a href="#4-long-horizon-planning" class="table-of-contents__link toc-highlight">4. Long-Horizon Planning</a></li></ul></li><li><a href="#18-the-vla-landscape-a-taxonomy" class="table-of-contents__link toc-highlight">1.8 The VLA Landscape: A Taxonomy</a></li><li><a href="#19-chapter-summary" class="table-of-contents__link toc-highlight">1.9 Chapter Summary</a></li><li><a href="#review-questions" class="table-of-contents__link toc-highlight">Review Questions</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/intro">Tutorial</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2026 My Project, Inc. Built with Docusaurus.</div></div></div></footer><div class="container_aXVz"><button class="toggleBtn_57VZ" aria-label="Open chat">ğŸ’¬</button></div></div>
</body>
</html>