"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[7205],{3593(e,n,s){s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module3-isaac/chapter3","title":"Chapter 3: Isaac ROS and Visual SLAM","description":"Learning Objectives","source":"@site/docs/module3-isaac/chapter3.mdx","sourceDirName":"module3-isaac","slug":"/module3-isaac/chapter3","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module3-isaac/chapter3","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module3-isaac/chapter3.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Chapter 3: Isaac ROS and Visual SLAM","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Synthetic Data Generation","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module3-isaac/chapter2"},"next":{"title":"Chapter 4: Nav2 and Path Planning for Humanoids","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module3-isaac/chapter4"}}');var r=s(4848),i=s(8453),t=s(7132);const o={title:"Chapter 3: Isaac ROS and Visual SLAM",sidebar_position:3},l="Chapter 3: Isaac ROS and Visual SLAM",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"3.1 From Simulation to Deployment: The Role of Isaac ROS",id:"31-from-simulation-to-deployment-the-role-of-isaac-ros",level:2},{value:"3.2 The Isaac ROS GEM Library",id:"32-the-isaac-ros-gem-library",level:2},{value:"Acceleration Factors",id:"acceleration-factors",level:3},{value:"3.3 Visual SLAM: Conceptual Foundation",id:"33-visual-slam-conceptual-foundation",level:2},{value:"The Core V-SLAM Pipeline",id:"the-core-v-slam-pipeline",level:3},{value:"Key Mathematical Concepts",id:"key-mathematical-concepts",level:3},{value:"Stereo vs Monocular vs RGB-D",id:"stereo-vs-monocular-vs-rgb-d",level:3},{value:"3.4 NVIDIA Jetson Orin: The Target Platform",id:"34-nvidia-jetson-orin-the-target-platform",level:2},{value:"3.5 Setting Up Isaac ROS on Jetson Orin",id:"35-setting-up-isaac-ros-on-jetson-orin",level:2},{value:"Step 1: Flash JetPack",id:"step-1-flash-jetpack",level:3},{value:"Step 2: Install Isaac ROS using Docker",id:"step-2-install-isaac-ros-using-docker",level:3},{value:"Step 3: Build the Isaac ROS VSLAM Package",id:"step-3-build-the-isaac-ros-vslam-package",level:3},{value:"3.6 Running cuVSLAM",id:"36-running-cuvslam",level:2},{value:"Hardware Configuration for Stereo + IMU",id:"hardware-configuration-for-stereo--imu",level:3},{value:"Launch cuVSLAM",id:"launch-cuvslam",level:3},{value:"cuVSLAM Output Topics",id:"cuvslam-output-topics",level:3},{value:"3.7 A Full ROS 2 Node that Consumes VSLAM Output",id:"37-a-full-ros-2-node-that-consumes-vslam-output",level:2},{value:"3.8 Nvblox: Real-Time 3D Mapping",id:"38-nvblox-real-time-3d-mapping",level:2},{value:"Launching Nvblox",id:"launching-nvblox",level:3},{value:"3.9 Camera Calibration: The Prerequisite",id:"39-camera-calibration-the-prerequisite",level:2},{value:"Intrinsic Calibration (per camera)",id:"intrinsic-calibration-per-camera",level:3},{value:"Extrinsic Calibration (stereo baseline)",id:"extrinsic-calibration-stereo-baseline",level:3},{value:"3.10 Performance Tuning on Jetson Orin",id:"310-performance-tuning-on-jetson-orin",level:2},{value:"Set Maximum Performance Mode",id:"set-maximum-performance-mode",level:3},{value:"Monitor GPU and CPU Utilisation",id:"monitor-gpu-and-cpu-utilisation",level:3},{value:"Tuning cuVSLAM for Latency vs Accuracy",id:"tuning-cuvslam-for-latency-vs-accuracy",level:3},{value:"3.11 Chapter Summary",id:"311-chapter-summary",level:2},{value:"Review Questions",id:"review-questions",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.A,{}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-3-isaac-ros-and-visual-slam",children:"Chapter 3: Isaac ROS and Visual SLAM"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explain what Isaac ROS is and how it differs from Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Identify the key hardware-accelerated packages in the Isaac ROS GEM library and their use cases"}),"\n",(0,r.jsx)(n.li,{children:"Describe the core mathematical principles behind Visual SLAM (V-SLAM)"}),"\n",(0,r.jsx)(n.li,{children:"Explain how cuVSLAM achieves real-time SLAM performance on the NVIDIA Jetson Orin"}),"\n",(0,r.jsx)(n.li,{children:"Set up a Jetson Orin development environment with Isaac ROS installed"}),"\n",(0,r.jsx)(n.li,{children:"Build and run a cuVSLAM pipeline in a ROS 2 workspace"}),"\n",(0,r.jsx)(n.li,{children:"Interpret SLAM output (pose, map cloud, odometry) and feed it into downstream navigation"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"31-from-simulation-to-deployment-the-role-of-isaac-ros",children:"3.1 From Simulation to Deployment: The Role of Isaac ROS"}),"\n",(0,r.jsxs)(n.p,{children:["Isaac Sim is where you develop and test. Isaac ROS is where you ",(0,r.jsx)(n.strong,{children:"deploy"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Once a perception or navigation algorithm has been validated in simulation, it needs to run on a real robot. For humanoid and mobile robots, that typically means running on an ",(0,r.jsx)(n.strong,{children:"NVIDIA Jetson"})," \u2014 a compact, power-efficient AI compute module designed for edge inference."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Development Pipeline:\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Isaac Sim  \u2502      \u2502  Isaac Lab  \u2502      \u2502   Isaac ROS      \u2502\n  \u2502  (Workstation\u2502\u2500\u2500\u25b6  \u2502  (GPU Cluster\u2502\u2500\u2500\u25b6  \u2502  (Jetson Orin)   \u2502\n  \u2502  RTX 4090)  \u2502      \u2502  Train policy\u2502      \u2502  Deploy + serve  \u2502\n  \u2502  Test stack \u2502      \u2502             \u2502      \u2502  perception +    \u2502\n  \u2502  Gen synth  \u2502      \u2502             \u2502      \u2502  navigation      \u2502\n  \u2502  data       \u2502      \u2502             \u2502      \u2502  at 30+ Hz       \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Isaac ROS packages are ",(0,r.jsx)(n.strong,{children:"hardware-accelerated"})," versions of common robotics algorithms, implemented using NVIDIA's CUDA, TensorRT, and cuDNN libraries. They are packaged as standard ",(0,r.jsx)(n.strong,{children:"ROS 2 nodes"})," that slot directly into an existing ROS 2 workspace \u2014 the same topics and services from Module 1 apply."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"32-the-isaac-ros-gem-library",children:"3.2 The Isaac ROS GEM Library"}),"\n",(0,r.jsxs)(n.p,{children:["GEM stands for ",(0,r.jsx)(n.strong,{children:"GPU-Efficient Module"}),". Each GEM is a ROS 2 package that accelerates a specific robotics task. As of 2024, the Isaac ROS GEM library includes:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Package"}),(0,r.jsx)(n.th,{children:"Function"}),(0,r.jsx)(n.th,{children:"Acceleration"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_visual_slam"})}),(0,r.jsx)(n.td,{children:"6-DoF pose estimation and mapping (cuVSLAM)"}),(0,r.jsx)(n.td,{children:"CUDA, GPU"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_nvblox"})}),(0,r.jsx)(n.td,{children:"Real-time 3D occupancy + ESDF mapping"}),(0,r.jsx)(n.td,{children:"CUDA"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_object_detection"})}),(0,r.jsx)(n.td,{children:"Detectnet / YOLOv8 inference"}),(0,r.jsx)(n.td,{children:"TensorRT"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_image_segmentation"})}),(0,r.jsx)(n.td,{children:"Semantic segmentation"}),(0,r.jsx)(n.td,{children:"TensorRT"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_pose_estimation"})}),(0,r.jsx)(n.td,{children:"6-DoF object pose (FoundationPose, DOPE)"}),(0,r.jsx)(n.td,{children:"TensorRT"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_apriltag"})}),(0,r.jsx)(n.td,{children:"AprilTag fiducial detection"}),(0,r.jsx)(n.td,{children:"CUDA"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_depth_segmentation"})}),(0,r.jsx)(n.td,{children:"Freespace segmentation from depth"}),(0,r.jsx)(n.td,{children:"CUDA"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_stereo_image_proc"})}),(0,r.jsx)(n.td,{children:"Stereo disparity and point cloud"}),(0,r.jsx)(n.td,{children:"CUDA"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_image_proc"})}),(0,r.jsx)(n.td,{children:"Colour conversion, rectification"}),(0,r.jsx)(n.td,{children:"CUDA"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_argus_camera"})}),(0,r.jsx)(n.td,{children:"Direct CSI camera driver"}),(0,r.jsx)(n.td,{children:"ISP hardware"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"acceleration-factors",children:"Acceleration Factors"}),"\n",(0,r.jsx)(n.p,{children:"For context on what hardware acceleration means in practice:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Algorithm             CPU (x86 laptop)    Jetson Orin (GPU)    Speedup\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500         \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAprilTag detection    12 Hz               200+ Hz              17\xd7\ncuVSLAM               Not real-time       300 Hz               \u221e\nStereo disparity      5 Hz                60+ Hz               12\xd7\nObject detection (TensorRT YOLO) 2 Hz     60 Hz                30\xd7\n"})}),"\n",(0,r.jsx)(n.p,{children:"These speedups make the difference between an algorithm that is theoretically correct and one that actually works on a moving robot."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"33-visual-slam-conceptual-foundation",children:"3.3 Visual SLAM: Conceptual Foundation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SLAM"})," (Simultaneous Localisation and Mapping) is the problem of answering two questions simultaneously:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Where am I?"})," (Localisation: estimating the robot's 6-DoF pose in a coordinate frame)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"What does the world look like?"})," (Mapping: building a map of the environment)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These problems are coupled: you need a map to localise, and you need localisation to build a map. SLAM algorithms solve both simultaneously."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"})," uses cameras as the primary sensor, instead of LiDAR (which is heavier, more expensive, and consumes more power). This makes it ideal for humanoid robots that carry cameras for other purposes."]}),"\n",(0,r.jsx)(n.h3,{id:"the-core-v-slam-pipeline",children:"The Core V-SLAM Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Camera frames (stereo or RGB-D)\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Feature Extraction           \u2502\n\u2502  Find keypoints in the image  \u2502\n\u2502  (corners, edges, blobs)      \u2502\n\u2502  Compute descriptors for each \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Feature Matching & Tracking  \u2502\n\u2502  Match features across frames \u2502\n\u2502  Reject outliers (RANSAC)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Motion Estimation            \u2502\n\u2502  Compute camera motion from   \u2502\n\u2502  matched feature positions    \u2502\n\u2502  (PnP solver, Essential Matrix\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Pose Graph Optimisation      \u2502\n\u2502  Bundle adjustment: refine    \u2502\n\u2502  poses + 3D point positions   \u2502\n\u2502  simultaneously               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Loop Closure Detection       \u2502\n\u2502  Recognise previously-visited \u2502\n\u2502  places; correct drift        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u25bc             \u25bc\n  Robot Pose     3D Point Cloud Map\n  (6-DoF)        (landmarks)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"key-mathematical-concepts",children:"Key Mathematical Concepts"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Feature detection"})," finds distinctive points in an image. In cuVSLAM, NVIDIA uses a GPU-accelerated version of Harris corner detection combined with ORB (Oriented FAST and Rotated BRIEF) descriptors."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"The Fundamental/Essential Matrix"})," relates corresponding points in stereo or consecutive frames via the equation:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"p2^T \xb7 E \xb7 p1 = 0\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Where ",(0,r.jsx)(n.code,{children:"p1"}),", ",(0,r.jsx)(n.code,{children:"p2"})," are normalised image coordinates of a matched feature, and ",(0,r.jsx)(n.code,{children:"E"})," is the 3\xd73 Essential Matrix encoding relative rotation R and translation t."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Bundle Adjustment"})," minimises the reprojection error across all camera poses and 3D points simultaneously \u2014 a large, sparse least-squares problem solved efficiently with the Ceres or g2o libraries."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Loop closure"})," is the algorithm's mechanism for correcting accumulated drift. When the robot returns to a previously visited location, the recogniser identifies it from a bag-of-words or global feature descriptor match, and a pose graph edge is added to correct the accumulated error."]}),"\n",(0,r.jsx)(n.h3,{id:"stereo-vs-monocular-vs-rgb-d",children:"Stereo vs Monocular vs RGB-D"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Setup"}),(0,r.jsx)(n.th,{children:"Pros"}),(0,r.jsx)(n.th,{children:"Cons"}),(0,r.jsx)(n.th,{children:"Best for"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Stereo"})}),(0,r.jsx)(n.td,{children:"True scale, good depth"}),(0,r.jsx)(n.td,{children:"Heavier, needs calibration"}),(0,r.jsx)(n.td,{children:"Outdoor, large spaces"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Monocular"})}),(0,r.jsx)(n.td,{children:"Lightest, single camera"}),(0,r.jsx)(n.td,{children:"No true scale, scale drift"}),(0,r.jsx)(n.td,{children:"Constrained indoor"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"RGB-D"})}),(0,r.jsx)(n.td,{children:"Dense depth per-frame"}),(0,r.jsx)(n.td,{children:"Short range (\u22645m), IR interference"}),(0,r.jsx)(n.td,{children:"Indoor manipulation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Stereo + IMU"})}),(0,r.jsx)(n.td,{children:"Scale-correct, robust motion"}),(0,r.jsx)(n.td,{children:"Complex calibration"}),(0,r.jsx)(n.td,{children:"Humanoid robots (fast motion)"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:["cuVSLAM supports all four configurations. For humanoid robots that move quickly and have access to stereo cameras (most modern humanoids do), ",(0,r.jsx)(n.strong,{children:"stereo + IMU"})," is the recommended configuration."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"34-nvidia-jetson-orin-the-target-platform",children:"3.4 NVIDIA Jetson Orin: The Target Platform"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA Jetson Orin"})," is the compute module designed for deploying Isaac ROS on real robots. It is a system-on-module (SoM) with:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Jetson AGX Orin (flagship)                                  \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502  CPU:    12-core Arm Cortex-A78AE (2.2 GHz)                 \u2502\n\u2502  GPU:    2048-core Ampere GPU                                \u2502\n\u2502  DLA:    2\xd7 Deep Learning Accelerators (50 TOPS each)       \u2502\n\u2502  PVA:    2\xd7 Programmable Vision Accelerators                \u2502\n\u2502  Memory: 32 GB LPDDR5 (shared CPU+GPU)                      \u2502\n\u2502  Storage:64 GB eMMC + NVMe M.2 slot                        \u2502\n\u2502  I/O:    PCIe Gen4, USB 3.2, CSI (camera), 10GbE           \u2502\n\u2502  Power:  15\u201360 W (configurable power modes)                 \u2502\n\u2502  Size:   100 \xd7 87 mm (developer kit carrier board)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"DLA (Deep Learning Accelerator)"})," is a fixed-function neural network engine that runs inference at very low power. TensorRT can compile models to run on the DLA, freeing the GPU for other tasks. This is important for humanoid robots where power budget is tight."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"35-setting-up-isaac-ros-on-jetson-orin",children:"3.5 Setting Up Isaac ROS on Jetson Orin"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-flash-jetpack",children:"Step 1: Flash JetPack"}),"\n",(0,r.jsx)(n.p,{children:"Isaac ROS requires JetPack 6.0+ (which includes Ubuntu 22.04 + CUDA 12.x)."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# On your host PC, use NVIDIA SDK Manager (GUI tool):\n# 1. Connect Jetson via USB-C in recovery mode\n# 2. Open SDK Manager\n# 3. Select "Jetson AGX Orin" + "JetPack 6.0"\n# 4. Install OS + all SDK components\n# 5. Reflash takes ~30 minutes\n\n# Alternatively, flash from command line:\nsudo ./flash.sh jetson-agx-orin-devkit mmcblk0p1\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-install-isaac-ros-using-docker",children:"Step 2: Install Isaac ROS using Docker"}),"\n",(0,r.jsx)(n.p,{children:"NVIDIA provides pre-built Docker containers with all Isaac ROS dependencies. This is the recommended installation method."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# On Jetson Orin:\n\n# 1. Install Docker (if not present)\nsudo apt update && sudo apt install -y docker.io\nsudo usermod -aG docker $USER\nnewgrp docker\n\n# 2. Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release; echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey \\\n    | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list \\\n    | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\nsudo apt update && sudo apt install -y nvidia-container-toolkit\n\n# 3. Pull the Isaac ROS container (choose ROS 2 Humble)\ndocker pull nvcr.io/nvidia/isaac/ros:aarch64-ros2_humble_b906f96e-2024.02.26\n\n# 4. Create a workspace inside the container\ndocker run -it \\\n    --runtime=nvidia \\\n    --privileged \\\n    --network=host \\\n    -v /dev:/dev \\\n    -v ~/ros2_ws:/workspaces/isaac_ros-dev \\\n    nvcr.io/nvidia/isaac/ros:aarch64-ros2_humble_b906f96e-2024.02.26\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-build-the-isaac-ros-vslam-package",children:"Step 3: Build the Isaac ROS VSLAM Package"}),"\n",(0,r.jsx)(n.p,{children:"Inside the container:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Source ROS 2\nsource /opt/ros/humble/setup.bash\n\n# Clone Isaac ROS VSLAM\ncd /workspaces/isaac_ros-dev\ngit clone --recurse-submodules \\\n    https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git \\\n    src/isaac_ros_visual_slam\n\n# Install dependencies\nsudo apt update\nrosdep update\nrosdep install -i -r --from-paths src --rosdistro humble -y\n\n# Build\ncolcon build \\\n    --symlink-install \\\n    --packages-up-to isaac_ros_visual_slam\n\nsource install/setup.bash\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"36-running-cuvslam",children:"3.6 Running cuVSLAM"}),"\n",(0,r.jsx)(n.h3,{id:"hardware-configuration-for-stereo--imu",children:"Hardware Configuration for Stereo + IMU"}),"\n",(0,r.jsx)(n.p,{children:"cuVSLAM expects the following ROS 2 topics:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Topic"}),(0,r.jsx)(n.th,{children:"Message Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/stereo_camera/left/image_raw"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})}),(0,r.jsx)(n.td,{children:"Left camera image"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/stereo_camera/right/image_raw"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})}),(0,r.jsx)(n.td,{children:"Right camera image"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/stereo_camera/left/camera_info"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})}),(0,r.jsx)(n.td,{children:"Left camera intrinsics"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/stereo_camera/right/camera_info"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})}),(0,r.jsx)(n.td,{children:"Right camera intrinsics"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"/imu/data"})," (optional)"]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/Imu"})}),(0,r.jsx)(n.td,{children:"IMU acceleration + gyroscope"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"launch-cuvslam",children:"Launch cuVSLAM"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch isaac_ros_visual_slam isaac_ros_visual_slam_realsense.launch.py\n"})}),"\n",(0,r.jsx)(n.p,{children:"For a custom camera setup:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# launch/my_vslam.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    return LaunchDescription([\n\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='visual_slam_node',\n            name='visual_slam',\n            output='screen',\n            parameters=[{\n                # Camera configuration\n                'num_cameras': 2,                    # Stereo\n                'enable_imu_fusion': True,\n                'imu_params_yaml': '',               # Auto-detect\n                'map_frame': 'map',\n                'odom_frame': 'odom',\n                'base_frame': 'base_link',\n\n                # Algorithm tuning\n                'img_jitter_threshold_ms': 34.0,    # Max acceptable time jitter\n                'enable_slam_constraints': True,     # Enable loop closure\n                'max_landmark_count': 1000,          # Memory/accuracy trade-off\n\n                # Visualisation\n                'path_max_size': 1024,               # Max poses in path visualisation\n                'enable_localization_n_mapping': True,\n            }],\n            remappings=[\n                ('stereo_camera/left/image',  '/camera/infra1/image_rect_raw'),\n                ('stereo_camera/right/image', '/camera/infra2/image_rect_raw'),\n                ('stereo_camera/left/camera_info',  '/camera/infra1/camera_info'),\n                ('stereo_camera/right/camera_info', '/camera/infra2/camera_info'),\n                ('/imu', '/camera/imu'),\n            ],\n        ),\n    ])\n"})}),"\n",(0,r.jsx)(n.h3,{id:"cuvslam-output-topics",children:"cuVSLAM Output Topics"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Topic"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/visual_slam/tracking/odometry"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"nav_msgs/Odometry"})}),(0,r.jsx)(n.td,{children:"Current pose + velocity estimate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/visual_slam/tracking/slam_path"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"nav_msgs/Path"})}),(0,r.jsx)(n.td,{children:"Full trajectory history"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/visual_slam/tracking/vo_path"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"nav_msgs/Path"})}),(0,r.jsx)(n.td,{children:"Visual odometry-only path"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/visual_slam/map/landmark_points"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sensor_msgs/PointCloud2"})}),(0,r.jsx)(n.td,{children:"3D map of tracked features"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/visual_slam/status"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isaac_ros_visual_slam_interfaces/VisualSlamStatus"})}),(0,r.jsx)(n.td,{children:"Tracking confidence"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"37-a-full-ros-2-node-that-consumes-vslam-output",children:"3.7 A Full ROS 2 Node that Consumes VSLAM Output"}),"\n",(0,r.jsx)(n.p,{children:"Here is a Python node that subscribes to VSLAM odometry and uses it to track the robot's position history and trigger alerts:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# vslam_monitor_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nfrom isaac_ros_visual_slam_interfaces.msg import VisualSlamStatus\nimport numpy as np\nimport math\n\nclass VSLAMMonitorNode(Node):\n    \"\"\"\n    Subscribes to cuVSLAM output and provides:\n    - Pose logging\n    - Displacement calculation\n    - Tracking quality monitoring\n    - Alert publishing when tracking is lost\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('vslam_monitor')\n\n        # Odometry subscriber\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            '/visual_slam/tracking/odometry',\n            self.odom_callback,\n            10,\n        )\n\n        # VSLAM status subscriber\n        self.status_sub = self.create_subscription(\n            VisualSlamStatus,\n            '/visual_slam/status',\n            self.status_callback,\n            10,\n        )\n\n        # Alert publisher\n        self.alert_pub = self.create_publisher(String, '/vslam/alert', 10)\n\n        # State\n        self.start_position = None\n        self.current_position = None\n        self.total_distance = 0.0\n        self.tracking_ok = True\n        self.pose_history = []     # List of (x, y, z) tuples\n\n        self.get_logger().info('VSLAM monitor started.')\n\n    def odom_callback(self, msg: Odometry):\n        \"\"\"Process incoming odometry from cuVSLAM.\"\"\"\n        pos = msg.pose.pose.position\n        current = np.array([pos.x, pos.y, pos.z])\n\n        # Initialise on first message\n        if self.start_position is None:\n            self.start_position = current.copy()\n            self.get_logger().info(\n                f'VSLAM origin set: ({pos.x:.3f}, {pos.y:.3f}, {pos.z:.3f})'\n            )\n\n        # Accumulate distance\n        if self.current_position is not None:\n            step = np.linalg.norm(current - self.current_position)\n            self.total_distance += step\n\n        self.current_position = current\n        self.pose_history.append(tuple(current))\n\n        # Calculate displacement from start\n        displacement = np.linalg.norm(current - self.start_position)\n\n        # Extract yaw from quaternion\n        q = msg.pose.pose.orientation\n        yaw = self._quat_to_yaw(q.x, q.y, q.z, q.w)\n\n        if len(self.pose_history) % 50 == 0:\n            self.get_logger().info(\n                f'Position: ({pos.x:.2f}, {pos.y:.2f}, {pos.z:.2f}) m  '\n                f'Yaw: {math.degrees(yaw):.1f}\xb0  '\n                f'Displacement: {displacement:.2f} m  '\n                f'Total path: {self.total_distance:.2f} m'\n            )\n\n    def status_callback(self, msg: VisualSlamStatus):\n        \"\"\"\n        Monitor tracking quality.\n        VisualSlamStatus.TRACKING_STATE values:\n          0 = UNKNOWN\n          1 = NOT_INITIALISED\n          2 = OK\n          3 = LOST\n        \"\"\"\n        state_names = {0: 'UNKNOWN', 1: 'NOT_INITIALISED',\n                       2: 'OK', 3: 'LOST'}\n        state_name = state_names.get(msg.vo_state, 'UNKNOWN')\n\n        was_ok = self.tracking_ok\n        self.tracking_ok = (msg.vo_state == 2)  # 2 = OK\n\n        if was_ok and not self.tracking_ok:\n            self.get_logger().error(f'VSLAM TRACKING LOST! State: {state_name}')\n            alert = String()\n            alert.data = f'VSLAM_LOST:{state_name}'\n            self.alert_pub.publish(alert)\n        elif not was_ok and self.tracking_ok:\n            self.get_logger().info('VSLAM tracking recovered.')\n            alert = String()\n            alert.data = 'VSLAM_RECOVERED'\n            self.alert_pub.publish(alert)\n\n    def _quat_to_yaw(self, x, y, z, w) -> float:\n        \"\"\"Extract yaw (rotation around Z) from a quaternion.\"\"\"\n        siny_cosp = 2.0 * (w * z + x * y)\n        cosy_cosp = 1.0 - 2.0 * (y * y + z * z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\n    def get_pose_history_as_array(self) -> np.ndarray:\n        \"\"\"Return the full pose history as an Nx3 numpy array.\"\"\"\n        return np.array(self.pose_history)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VSLAMMonitorNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        arr = node.get_pose_history_as_array()\n        node.get_logger().info(\n            f'Session ended. Poses recorded: {len(arr)}. '\n            f'Total distance: {node.total_distance:.2f} m'\n        )\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"38-nvblox-real-time-3d-mapping",children:"3.8 Nvblox: Real-Time 3D Mapping"}),"\n",(0,r.jsxs)(n.p,{children:["cuVSLAM produces a ",(0,r.jsx)(n.strong,{children:"sparse"})," feature point map. For navigation, you need a ",(0,r.jsx)(n.strong,{children:"dense"})," occupancy map. ",(0,r.jsx)(n.strong,{children:"Nvblox"})," (part of Isaac ROS) converts depth images into a real-time 3D voxel map with a GPU-accelerated Euclidean Signed Distance Function (ESDF):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Depth images + VSLAM poses\n          \u2502\n          \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Nvblox    \u2502     GPU-accelerated TSDF integration\n    \u2502  (GPU voxel \u2502\u2500\u2500\u25b6  Occupancy grid (2D slice for Nav2)\n    \u2502   mapping)  \u2502\u2500\u2500\u25b6  ESDF (3D distance field for planning)\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u25b6  Mesh (for visualisation)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"launching-nvblox",children:"Launching Nvblox"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch nvblox_examples_bringup isaac_sim_example.launch.py\n"})}),"\n",(0,r.jsx)(n.p,{children:"Nvblox publishes:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/nvblox_node/occupancy_layer"})," \u2014 2D occupancy grid for Nav2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/nvblox_node/esdf_pointcloud"})," \u2014 3D signed distance field"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/nvblox_node/mesh"})," \u2014 triangulated mesh for RViz2 visualisation"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"39-camera-calibration-the-prerequisite",children:"3.9 Camera Calibration: The Prerequisite"}),"\n",(0,r.jsx)(n.p,{children:"cuVSLAM requires accurate camera intrinsic and extrinsic calibration. Poor calibration is the most common cause of VSLAM failure."}),"\n",(0,r.jsx)(n.h3,{id:"intrinsic-calibration-per-camera",children:"Intrinsic Calibration (per camera)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Use the ROS 2 camera calibration tool with a chessboard\nros2 run camera_calibration cameracalibrator \\\n    --size 8x6 \\\n    --square 0.025 \\\n    image:=/camera/left/image_raw \\\n    camera:=/camera/left\n\n# Output (publish to /camera_info topic):\n# - fx, fy: focal length in pixels\n# - cx, cy: principal point\n# - k1, k2, p1, p2, k3: distortion coefficients\n"})}),"\n",(0,r.jsx)(n.h3,{id:"extrinsic-calibration-stereo-baseline",children:"Extrinsic Calibration (stereo baseline)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Use Kalibr for stereo + IMU calibration\n# (run on collected rosbag data)\npip install kalibr\n\nkalibr_calibrate_cameras \\\n    --bag stereo_calibration.bag \\\n    --topics /camera/left/image_raw /camera/right/image_raw \\\n    --models pinhole-radtan pinhole-radtan \\\n    --target aprilgrid_6x6.yaml\n\n# Output: stereo_calibration_results.yaml\n# - T_left_right: 4x4 transform from right to left camera\n# - baseline: stereo baseline in metres (typically 50-120mm)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"310-performance-tuning-on-jetson-orin",children:"3.10 Performance Tuning on Jetson Orin"}),"\n",(0,r.jsx)(n.h3,{id:"set-maximum-performance-mode",children:"Set Maximum Performance Mode"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Enable maximum performance (high power, max clocks)\nsudo nvpmodel -m 0        # Maximum performance mode\nsudo jetson_clocks        # Fix all clocks to maximum\n\n# Verify\nsudo jetson_clocks --show\n"})}),"\n",(0,r.jsx)(n.h3,{id:"monitor-gpu-and-cpu-utilisation",children:"Monitor GPU and CPU Utilisation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Real-time system monitor (Jetson-specific)\nsudo jtop\n\n# Check VSLAM latency\nros2 topic delay /visual_slam/tracking/odometry\nros2 topic hz /visual_slam/tracking/odometry\n"})}),"\n",(0,r.jsx)(n.h3,{id:"tuning-cuvslam-for-latency-vs-accuracy",children:"Tuning cuVSLAM for Latency vs Accuracy"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Low-latency configuration (sacrifice some accuracy)\nparameters=[{\n    'max_landmark_count': 500,       # Fewer landmarks = faster\n    'enable_slam_constraints': False, # Disable loop closure (saves compute)\n    'img_jitter_threshold_ms': 100,  # More tolerant of timing jitter\n}]\n\n# High-accuracy configuration (for mapping tasks)\nparameters=[{\n    'max_landmark_count': 2000,\n    'enable_slam_constraints': True,  # Enable loop closure\n    'img_jitter_threshold_ms': 34,    # Strict timing (30 Hz cameras)\n}]\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"311-chapter-summary",children:"3.11 Chapter Summary"}),"\n",(0,r.jsx)(n.p,{children:"This chapter bridged the gap between simulation-based development and real hardware deployment:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS GEM packages"})," are hardware-accelerated ROS 2 nodes that run algorithms like SLAM, object detection, and depth processing 10\u201330\xd7 faster than CPU implementations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"})," solves localisation and mapping simultaneously using camera images. The pipeline is: feature extraction \u2192 matching \u2192 motion estimation \u2192 pose graph optimisation \u2192 loop closure."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"cuVSLAM"})," is NVIDIA's GPU-accelerated V-SLAM implementation. It supports stereo, monocular, RGB-D, and IMU-fused configurations. For humanoids, stereo + IMU is recommended."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Jetson Orin"})," is the target deployment hardware. It combines a powerful GPU, DLA accelerators, and hardware ISP in a compact, power-efficient module."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS is installed via Docker"})," containers. The ",(0,r.jsx)(n.code,{children:"colcon build --packages-up-to isaac_ros_visual_slam"})," workflow integrates seamlessly with standard ROS 2 development."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Camera calibration"})," is a non-negotiable prerequisite. Use the ROS 2 camera calibration tool for intrinsics and Kalibr for stereo + IMU extrinsics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Nvblox"})," converts VSLAM pose estimates + depth images into a real-time 3D occupancy map suitable for Nav2 path planning."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Explain the difference between Visual Odometry (VO) and full SLAM. What is the role of loop closure, and what failure mode does it prevent?"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"A teammate proposes replacing the stereo + IMU cuVSLAM setup with a single monocular camera to reduce cost and weight on a humanoid robot. What specific problem would arise with monocular-only VSLAM on a robot that occasionally needs to navigate metric distances of 10+ metres?"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["You run ",(0,r.jsx)(n.code,{children:"ros2 topic hz /visual_slam/tracking/odometry"})," on your Jetson Orin and see it outputting at 8 Hz instead of the expected 30 Hz. List four potential causes and how you would diagnose each."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Write a Python ROS 2 node that subscribes to ",(0,r.jsx)(n.code,{children:"/visual_slam/tracking/odometry"})," and publishes a ",(0,r.jsx)(n.code,{children:"PoseStamped"})," message to ",(0,r.jsx)(n.code,{children:"/robot/current_pose"})," only when the robot has moved more than 5 cm since the last published pose (to avoid flooding the topic during stationary periods). Include the full class code."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Describe the full data path from light entering the stereo camera lenses to the ",(0,r.jsx)(n.code,{children:"/visual_slam/tracking/odometry"})," topic containing the robot's 6-DoF pose. Name every processing step, the hardware that executes it, and the data format at each stage."]}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},7132(e,n,s){s.d(n,{A:()=>h});var a=s(6540);const r="container_sRRF",i="row_Wfea",t="translateBtn_zwOG",o="showOriginalBtn__IG_",l="translatedContent_uGS7",c="error_H8Lp";var d=s(4848);function h(){const[e,n]=(0,a.useState)("idle"),[s,h]=(0,a.useState)(""),[u,m]=(0,a.useState)(""),p=(0,a.useCallback)(async()=>{n("loading");const e=document.querySelector("article"),s=e?e.innerText:document.body.innerText,a="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${a}/translate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:s,target_language:"urdu"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const r=await e.json();h(r.translated_text),n("translated")}catch(r){m(r.message),n("error")}},[]),x=(0,a.useCallback)(()=>{n("idle"),h(""),m("")},[]);return(0,d.jsxs)("div",{className:r,children:[("idle"===e||"error"===e)&&(0,d.jsxs)("div",{className:i,children:[(0,d.jsx)("button",{className:t,onClick:p,children:"\ud83c\udf10 Translate to Urdu"}),"error"===e&&(0,d.jsxs)("span",{className:c,children:["Translation failed: ",u]})]}),"loading"===e&&(0,d.jsx)("button",{className:t,disabled:!0,children:"\u23f3 Translating\u2026"}),"translated"===e&&(0,d.jsxs)("div",{children:[(0,d.jsx)("button",{className:o,onClick:x,children:"\ud83d\udcd6 Show Original English"}),(0,d.jsx)("div",{className:l,dir:"rtl",lang:"ur",children:s})]})]})}},8453(e,n,s){s.d(n,{R:()=>t,x:()=>o});var a=s(6540);const r={},i=a.createContext(r);function t(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);