"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[3171],{4155(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module3-isaac/chapter2","title":"Chapter 2: Synthetic Data Generation","description":"Learning Objectives","source":"@site/docs/module3-isaac/chapter2.mdx","sourceDirName":"module3-isaac","slug":"/module3-isaac/chapter2","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module3-isaac/chapter2","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module3-isaac/chapter2.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Chapter 2: Synthetic Data Generation","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: NVIDIA Isaac Sim Overview","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module3-isaac/chapter1"},"next":{"title":"Chapter 3: Isaac ROS and Visual SLAM","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module3-isaac/chapter3"}}');var i=t(4848),r=t(8453),o=t(7132);const s={title:"Chapter 2: Synthetic Data Generation",sidebar_position:2},d="Chapter 2: Synthetic Data Generation",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"2.1 The Data Problem in Robot Perception",id:"21-the-data-problem-in-robot-perception",level:2},{value:"2.2 The Sim-to-Real Gap: Root Causes",id:"22-the-sim-to-real-gap-root-causes",level:2},{value:"Sources of the Gap",id:"sources-of-the-gap",level:3},{value:"Techniques to Close the Gap",id:"techniques-to-close-the-gap",level:3},{value:"2.3 Isaac Sim Replicator",id:"23-isaac-sim-replicator",level:2},{value:"Replicator Architecture",id:"replicator-architecture",level:3},{value:"2.4 A Complete SDG Pipeline",id:"24-a-complete-sdg-pipeline",level:2},{value:"Step 1: Scene and Camera Setup",id:"step-1-scene-and-camera-setup",level:3},{value:"Step 2: Define Randomisers",id:"step-2-define-randomisers",level:3},{value:"Step 3: Attach the Camera and Annotators",id:"step-3-attach-the-camera-and-annotators",level:3},{value:"Step 4: Configure the Writer and Trigger",id:"step-4-configure-the-writer-and-trigger",level:3},{value:"Step 5: Run the Generation Loop",id:"step-5-run-the-generation-loop",level:3},{value:"2.5 Understanding Annotator Outputs",id:"25-understanding-annotator-outputs",level:2},{value:"Reading Bounding Box Annotations in Python",id:"reading-bounding-box-annotations-in-python",level:3},{value:"Reading Instance Segmentation",id:"reading-instance-segmentation",level:3},{value:"2.6 Converting to COCO Format for PyTorch",id:"26-converting-to-coco-format-for-pytorch",level:2},{value:"2.7 Training a Detector on the Synthetic Dataset",id:"27-training-a-detector-on-the-synthetic-dataset",level:2},{value:"2.8 Domain Randomisation Strategies",id:"28-domain-randomisation-strategies",level:2},{value:"Lighting Randomisation",id:"lighting-randomisation",level:3},{value:"Object Distractors",id:"object-distractors",level:3},{value:"Camera Noise Models",id:"camera-noise-models",level:3},{value:"2.9 Evaluating Synthetic-to-Real Transfer",id:"29-evaluating-synthetic-to-real-transfer",level:2},{value:"2.10 Chapter Summary",id:"210-chapter-summary",level:2},{value:"Review Questions",id:"review-questions",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.A,{}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-2-synthetic-data-generation",children:"Chapter 2: Synthetic Data Generation"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Articulate why synthetic data is essential for training robot perception models and how it reduces labelling costs"}),"\n",(0,i.jsx)(n.li,{children:"Explain the sim-to-real gap and identify the specific techniques used to close it"}),"\n",(0,i.jsx)(n.li,{children:"Implement domain randomisation across lighting, textures, materials, and object placement"}),"\n",(0,i.jsx)(n.li,{children:"Use the Isaac Sim Replicator API to write a programmatic synthetic data generation pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Configure annotators to produce ground-truth labels for object detection, instance segmentation, depth estimation, and keypoint detection"}),"\n",(0,i.jsx)(n.li,{children:"Export a dataset in COCO and custom JSON formats suitable for training PyTorch models"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"21-the-data-problem-in-robot-perception",children:"2.1 The Data Problem in Robot Perception"}),"\n",(0,i.jsx)(n.p,{children:'Training a deep neural network to perceive the world requires enormous amounts of annotated data. For an object detection model, "annotated" means every image needs bounding boxes drawn around every object of interest, labelled with the correct class. For a pose estimation model, every image needs 3D keypoint annotations. For a depth model, every pixel needs a ground-truth depth value.'}),"\n",(0,i.jsx)(n.p,{children:"Collecting and annotating this data in the real world is extraordinarily expensive:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Real-world data collection costs (approximate):\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Photographer / operator day rate:     $300\u2013800/day  \u2502\n  \u2502 Object annotation (1 image):          $0.50\u20135.00    \u2502\n  \u2502 Semantic segmentation (1 image):      $5\u201350         \u2502\n  \u2502 3D keypoint annotation (1 image):     $10\u2013100       \u2502\n  \u2502                                                     \u2502\n  \u2502 To train a robust detector you need:  50,000+ images\u2502\n  \u2502 Estimated labelling cost:             $25,000\u2013250,000\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Synthetic data generation produces ",(0,i.jsx)(n.strong,{children:"unlimited, perfectly annotated images at zero marginal cost"}),". The simulator knows the exact position, orientation, and identity of every object \u2014 ground-truth labels are generated automatically."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"22-the-sim-to-real-gap-root-causes",children:"2.2 The Sim-to-Real Gap: Root Causes"}),"\n",(0,i.jsxs)(n.p,{children:["Simply rendering images in a simulator is not enough. A model trained only on synthetic data often fails on real images because of systematic differences between synthetic and real distributions. These differences are called the ",(0,i.jsx)(n.strong,{children:"sim-to-real gap"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"sources-of-the-gap",children:"Sources of the Gap"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Synthetic World                          Real World\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPerfect, uniform lighting                Variable, directional, coloured light\nSmooth, clean surfaces                   Scratches, dust, wear, weathering\nKnown, fixed object textures             Object appearance varies with age\nIdealized camera model                   Lens distortion, chromatic aberration\nNo motion blur                           Motion blur at high speeds\nExact object positions known             Object positions estimated / approximate\nNo occlusion overlap errors              Heavy inter-object occlusion\n"})}),"\n",(0,i.jsx)(n.h3,{id:"techniques-to-close-the-gap",children:"Techniques to Close the Gap"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Technique"}),(0,i.jsx)(n.th,{children:"What It Does"}),(0,i.jsx)(n.th,{children:"Gap Source Targeted"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Domain randomisation"})}),(0,i.jsx)(n.td,{children:"Randomise lighting, textures, poses during generation"}),(0,i.jsx)(n.td,{children:"Lighting, texture, pose variance"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Photorealistic rendering"})}),(0,i.jsx)(n.td,{children:"Use ray tracing to simulate real light physics"}),(0,i.jsx)(n.td,{children:"Lighting, shadows, reflections"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"PBR materials"})}),(0,i.jsx)(n.td,{children:"Physically-Based Rendering materials with correct roughness/metalness"}),(0,i.jsx)(n.td,{children:"Material appearance"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Real background composition"})}),(0,i.jsx)(n.td,{children:"Composite rendered objects onto real background photos"}),(0,i.jsx)(n.td,{children:"Background distribution"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Sensor noise models"})}),(0,i.jsx)(n.td,{children:"Add simulated camera noise, blur, distortion"}),(0,i.jsx)(n.td,{children:"Camera model"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Real + synthetic mixing"})}),(0,i.jsx)(n.td,{children:"Train on 70% synthetic + 30% real images"}),(0,i.jsx)(n.td,{children:"All residual gaps"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["The most powerful single technique is ",(0,i.jsx)(n.strong,{children:"domain randomisation"}),": if you train on images with widely varying lighting, textures, and poses, the model is forced to learn appearance-invariant features, which transfer well to the real world."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"23-isaac-sim-replicator",children:"2.3 Isaac Sim Replicator"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Isaac Sim Replicator"})," is the programmatic synthetic data generation (SDG) framework built into Isaac Sim. It provides:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A Python API for ",(0,i.jsx)(n.strong,{children:"randomising"})," scene properties (lights, materials, transforms, cameras)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Annotators"})," that attach to cameras to automatically output ground-truth labels"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Writers"})," that save data to disk in standard formats (COCO, KITTI, custom)"]}),"\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.strong,{children:"trigger"})," system for advancing the simulation deterministically"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"replicator-architecture",children:"Replicator Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Replicator Pipeline                          \u2502\n\u2502                                                                 \u2502\n\u2502  Scene Setup           Randomisation         Data Collection    \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500         \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n\u2502  Load objects          RandomizerGroup       Annotators:        \u2502\n\u2502  Place cameras    \u2500\u2500\u25b6  \u2022 lights          \u2500\u2500\u25b6 \u2022 BoundingBox2D    \u2502\n\u2502  Add backgrounds       \u2022 textures            \u2022 BoundingBox3D    \u2502\n\u2502  Configure annotators  \u2022 transforms          \u2022 Segmentation     \u2502\n\u2502                        \u2022 visibility          \u2022 Depth            \u2502\n\u2502                        \u2022 camera FOV          \u2022 Normals          \u2502\n\u2502                                              \u2022 Keypoints        \u2502\n\u2502                                                    \u2502            \u2502\n\u2502                                            Writers:             \u2502\n\u2502                                            \u2022 COCO JSON          \u2502\n\u2502                                            \u2022 KITTI              \u2502\n\u2502                                            \u2022 Custom             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"24-a-complete-sdg-pipeline",children:"2.4 A Complete SDG Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"The following script generates a synthetic dataset of a humanoid robot arm picking up objects, with bounding box and segmentation annotations."}),"\n",(0,i.jsx)(n.h3,{id:"step-1-scene-and-camera-setup",children:"Step 1: Scene and Camera Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# sdg_pipeline.py\n# Run with: ./python.sh sdg_pipeline.py\n\nimport carb\nimport omni.replicator.core as rep\nfrom omni.isaac.kit import SimulationApp\n\nCONFIG = {\n    "headless": True,           # Run without display for batch generation\n    "width": 1280,\n    "height": 720,\n    "renderer": "RayTracedLighting",\n}\n\nsimulation_app = SimulationApp(CONFIG)\n\nimport omni.kit.commands\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nimport numpy as np\n\n# \u2500\u2500 Build the Scene \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nworld = World(stage_units_in_meters=1.0)\nworld.scene.add_default_ground_plane()\n\nassets_root = get_assets_root_path()\n\n# Load a robot arm (e.g. Franka Panda)\nadd_reference_to_stage(\n    usd_path=assets_root + "/Isaac/Robots/Franka/franka_alt_fingers.usd",\n    prim_path="/World/Franka"\n)\n\n# Load some target objects onto the table\nOBJECTS = [\n    ("/Isaac/Props/YCB/Axis_Aligned/003_cracker_box.usd", "/World/CrackerBox"),\n    ("/Isaac/Props/YCB/Axis_Aligned/005_tomato_soup_can.usd", "/World/SoupCan"),\n    ("/Isaac/Props/YCB/Axis_Aligned/021_bleach_cleanser.usd", "/World/Bleach"),\n]\n\nfor usd_suffix, prim_path in OBJECTS:\n    add_reference_to_stage(\n        usd_path=assets_root + usd_suffix,\n        prim_path=prim_path\n    )\n\nworld.reset()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-define-randomisers",children:"Step 2: Define Randomisers"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# \u2500\u2500 Randomisation Groups \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nwith rep.new_layer():\n\n    # \u2500\u2500 Light Randomisation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # Creates a new dome light with randomised colour temperature and intensity\n    def randomise_lighting():\n        lights = rep.create.light(\n            light_type="Sphere",\n            temperature=rep.distribution.uniform(2800, 6500),    # Kelvin\n            intensity=rep.distribution.uniform(500, 5000),\n            position=rep.distribution.uniform(\n                (-3, -3, 2), (3, 3, 5)                          # Random position\n            ),\n            scale=rep.distribution.uniform(0.5, 3.0),\n            count=1,                                             # One light per frame\n        )\n        return lights.node\n\n    rep.randomizer.register(randomise_lighting)\n\n    # \u2500\u2500 Object Transform Randomisation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def randomise_object_poses():\n        """Randomise position and orientation of all YCB objects."""\n        objects = rep.get.prims(path_pattern="/World/CrackerBox|/World/SoupCan|/World/Bleach")\n        with objects:\n            rep.randomizer.scatter_2d(\n                surface_prims=[rep.get.prims(path_pattern="/World/defaultGroundPlane")],\n                check_for_collisions=True,\n            )\n            rep.modify.pose(\n                rotation=rep.distribution.uniform(\n                    (0, 0, 0), (0, 360, 0)                       # Random yaw rotation\n                ),\n            )\n        return objects.node\n\n    rep.randomizer.register(randomise_object_poses)\n\n    # \u2500\u2500 Material / Texture Randomisation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def randomise_ground_texture():\n        """Randomise the ground plane material."""\n        ground = rep.get.prims(path_pattern="/World/defaultGroundPlane/.*Mesh")\n        with ground:\n            rep.randomizer.materials(\n                rep.utils.get_usd_files(\n                    assets_root + "/Isaac/Materials/Textures/Patterns",\n                    recursive=True\n                )\n            )\n        return ground.node\n\n    rep.randomizer.register(randomise_ground_texture)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-attach-the-camera-and-annotators",children:"Step 3: Attach the Camera and Annotators"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'    # \u2500\u2500 Camera Setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    camera = rep.create.camera(\n        position=rep.distribution.uniform((1.0, -1.5, 1.2), (1.5, 1.5, 2.0)),\n        look_at="/World/Franka",          # Always point at the robot\n        focal_length=rep.distribution.uniform(18.0, 35.0),  # Zoom randomisation\n    )\n\n    # Render product: ties the camera to a resolution\n    render_product = rep.create.render_product(camera, resolution=(1280, 720))\n\n    # \u2500\u2500 Annotators \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # Each annotator attaches to the render product and auto-generates labels\n\n    # 1. RGB image (the actual training image)\n    rgb_annotator = rep.AnnotatorRegistry.get_annotator("rgb")\n    rgb_annotator.attach([render_product])\n\n    # 2. 2D bounding boxes\n    bbox2d_annotator = rep.AnnotatorRegistry.get_annotator(\n        "bounding_box_2d_tight"   # Tight boxes (no occluded area)\n    )\n    bbox2d_annotator.attach([render_product])\n\n    # 3. 3D bounding boxes (world-space corners)\n    bbox3d_annotator = rep.AnnotatorRegistry.get_annotator("bounding_box_3d")\n    bbox3d_annotator.attach([render_product])\n\n    # 4. Instance segmentation (each object gets a unique colour)\n    seg_annotator = rep.AnnotatorRegistry.get_annotator("instance_segmentation")\n    seg_annotator.attach([render_product])\n\n    # 5. Depth (metres from camera)\n    depth_annotator = rep.AnnotatorRegistry.get_annotator("distance_to_image_plane")\n    depth_annotator.attach([render_product])\n\n    # 6. Occlusion ratio (fraction of each object that is visible)\n    occlusion_annotator = rep.AnnotatorRegistry.get_annotator("occlusion")\n    occlusion_annotator.attach([render_product])\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-configure-the-writer-and-trigger",children:"Step 4: Configure the Writer and Trigger"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'    # \u2500\u2500 Writer: Save to COCO format \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    writer = rep.WriterRegistry.get("BasicWriter")\n    writer.initialize(\n        output_dir="/output/sdg_dataset",\n        rgb=True,\n        bounding_box_2d_tight=True,\n        bounding_box_3d=True,\n        instance_segmentation=True,\n        distance_to_image_plane=True,\n        occlusion=True,\n        image_format="png",\n    )\n    writer.attach([render_product])\n\n    # \u2500\u2500 Trigger: Controls when randomisation fires \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # on_frame: runs once per render frame\n    with rep.trigger.on_frame(num_frames=1000):    # Generate 1000 images\n        rep.randomizer.randomise_lighting()\n        rep.randomizer.randomise_object_poses()\n        rep.randomizer.randomise_ground_texture()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-5-run-the-generation-loop",children:"Step 5: Run the Generation Loop"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# \u2500\u2500 Run the Pipeline \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint("Starting synthetic data generation...")\n\n# Orchestrator runs all registered randomisers and writers\nrep.orchestrator.run()\n\n# For headless batch generation, use step-and-wait:\nfor i in range(1000):\n    rep.orchestrator.step(rt_subframes=1)   # 1 ray-trace subframe per sample\n    if i % 100 == 0:\n        print(f"Generated {i + 1} / 1000 images")\n\nprint("Dataset generation complete. Saved to /output/sdg_dataset/")\nsimulation_app.close()\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"25-understanding-annotator-outputs",children:"2.5 Understanding Annotator Outputs"}),"\n",(0,i.jsx)(n.p,{children:"After running the pipeline, the output directory contains:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"/output/sdg_dataset/\n\u251c\u2500\u2500 rgb/\n\u2502   \u251c\u2500\u2500 rgb_0000.png          \u2190 Training image (1280\xd7720, RGB)\n\u2502   \u251c\u2500\u2500 rgb_0001.png\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 bounding_box_2d_tight/\n\u2502   \u251c\u2500\u2500 bounding_box_2d_tight_0000.npy    \u2190 NumPy structured array\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 instance_segmentation/\n\u2502   \u251c\u2500\u2500 instance_segmentation_0000.png    \u2190 Colour-coded instance map\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 distance_to_image_plane/\n\u2502   \u251c\u2500\u2500 distance_to_image_plane_0000.npy  \u2190 Float32 depth in metres\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 metadata.json             \u2190 Camera intrinsics, class-to-ID mapping\n"})}),"\n",(0,i.jsx)(n.h3,{id:"reading-bounding-box-annotations-in-python",children:"Reading Bounding Box Annotations in Python"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport json\nfrom pathlib import Path\n\ndef load_bbox_sample(sample_idx: int, dataset_dir: str):\n    """Load one sample: image path + bounding box annotations."""\n\n    # Bounding box NumPy structured array\n    bbox_path = (\n        Path(dataset_dir) / "bounding_box_2d_tight" /\n        f"bounding_box_2d_tight_{sample_idx:04d}.npy"\n    )\n    bbox_data = np.load(str(bbox_path))\n\n    # Each row: (semanticId, x_min, y_min, x_max, y_max, occlusion_ratio)\n    print(f"Sample {sample_idx}: {len(bbox_data)} objects detected")\n    for row in bbox_data:\n        print(f"  Class {row[\'semanticId\']:3d}  "\n              f"bbox=({row[\'x_min\']:.0f}, {row[\'y_min\']:.0f}, "\n              f"{row[\'x_max\']:.0f}, {row[\'y_max\']:.0f})  "\n              f"occlusion={row[\'occlusionRatio\']:.2f}")\n\n    rgb_path = (\n        Path(dataset_dir) / "rgb" / f"rgb_{sample_idx:04d}.png"\n    )\n    return str(rgb_path), bbox_data\n\n# Load sample 0\nimg_path, boxes = load_bbox_sample(0, "/output/sdg_dataset")\nprint(f"Image: {img_path}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"reading-instance-segmentation",children:"Reading Instance Segmentation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from PIL import Image\nimport numpy as np\n\ndef load_segmentation(sample_idx: int, dataset_dir: str):\n    """Load instance segmentation mask."""\n    seg_path = (\n        Path(dataset_dir) / "instance_segmentation" /\n        f"instance_segmentation_{sample_idx:04d}.png"\n    )\n    seg_img = np.array(Image.open(seg_path))\n\n    # Each unique colour corresponds to one object instance\n    # Get unique instance IDs\n    unique_ids = np.unique(seg_img.reshape(-1, 3), axis=0)\n    print(f"Instance count: {len(unique_ids) - 1}")  # -1 for background\n    return seg_img\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"26-converting-to-coco-format-for-pytorch",children:"2.6 Converting to COCO Format for PyTorch"}),"\n",(0,i.jsxs)(n.p,{children:["The COCO (Common Objects in Context) JSON format is the standard for object detection training with libraries like PyTorch's ",(0,i.jsx)(n.code,{children:"torchvision"}),", Detectron2, and YOLO variants."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# convert_to_coco.py\nimport json\nimport numpy as np\nfrom pathlib import Path\nfrom PIL import Image\n\ndef sdg_to_coco(dataset_dir: str, output_path: str, class_map: dict):\n    """\n    Convert an Isaac Sim Replicator dataset to COCO JSON format.\n\n    class_map: {semantic_id: {"id": coco_id, "name": "class_name"}}\n    Example: {1: {"id": 1, "name": "cracker_box"},\n              2: {"id": 2, "name": "soup_can"}}\n    """\n    dataset_dir = Path(dataset_dir)\n\n    coco = {\n        "info": {\n            "description": "Isaac Sim Synthetic Dataset",\n            "version": "1.0",\n            "year": 2024,\n        },\n        "licenses": [],\n        "categories": [\n            {"id": v["id"], "name": v["name"], "supercategory": "object"}\n            for v in class_map.values()\n        ],\n        "images": [],\n        "annotations": [],\n    }\n\n    annotation_id = 0\n\n    rgb_files = sorted((dataset_dir / "rgb").glob("rgb_*.png"))\n\n    for image_id, rgb_file in enumerate(rgb_files):\n        sample_idx = int(rgb_file.stem.split("_")[-1])\n\n        # Get image dimensions\n        img = Image.open(rgb_file)\n        width, height = img.size\n\n        coco["images"].append({\n            "id": image_id,\n            "file_name": rgb_file.name,\n            "width": width,\n            "height": height,\n        })\n\n        # Load bounding boxes\n        bbox_file = (\n            dataset_dir / "bounding_box_2d_tight" /\n            f"bounding_box_2d_tight_{sample_idx:04d}.npy"\n        )\n        if not bbox_file.exists():\n            continue\n\n        bbox_data = np.load(str(bbox_file))\n\n        for row in bbox_data:\n            sem_id = int(row[\'semanticId\'])\n            if sem_id not in class_map:\n                continue  # Skip background or unlabelled objects\n\n            x_min = float(row[\'x_min\'])\n            y_min = float(row[\'y_min\'])\n            x_max = float(row[\'x_max\'])\n            y_max = float(row[\'y_max\'])\n            w = x_max - x_min\n            h = y_max - y_min\n\n            if w <= 0 or h <= 0:\n                continue  # Skip degenerate boxes\n\n            occlusion = float(row.get(\'occlusionRatio\', 0.0))\n            if occlusion > 0.9:\n                continue  # Skip >90% occluded objects\n\n            coco["annotations"].append({\n                "id": annotation_id,\n                "image_id": image_id,\n                "category_id": class_map[sem_id]["id"],\n                "bbox": [x_min, y_min, w, h],  # COCO: [x, y, w, h]\n                "area": w * h,\n                "iscrowd": 0,\n                "attributes": {"occlusion": occlusion},\n            })\n            annotation_id += 1\n\n    with open(output_path, \'w\') as f:\n        json.dump(coco, f, indent=2)\n\n    print(f"COCO dataset saved: {output_path}")\n    print(f"  Images: {len(coco[\'images\'])}")\n    print(f"  Annotations: {len(coco[\'annotations\'])}")\n    return coco\n\n\n# Example usage\nclass_map = {\n    1: {"id": 1, "name": "cracker_box"},\n    2: {"id": 2, "name": "soup_can"},\n    3: {"id": 3, "name": "bleach_cleanser"},\n}\n\nsdg_to_coco(\n    dataset_dir="/output/sdg_dataset",\n    output_path="/output/coco_annotations.json",\n    class_map=class_map\n)\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"27-training-a-detector-on-the-synthetic-dataset",children:"2.7 Training a Detector on the Synthetic Dataset"}),"\n",(0,i.jsx)(n.p,{children:"Once you have the COCO JSON file, training with PyTorch is straightforward:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# train_detector.py\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.datasets import CocoDetection\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\nimport torch\n\n# \u2500\u2500 Dataset \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntransform = T.Compose([T.ToTensor()])\n\ntrain_dataset = CocoDetection(\n    root="/output/sdg_dataset/rgb",\n    annFile="/output/coco_annotations.json",\n    transforms=transform,\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    collate_fn=lambda batch: tuple(zip(*batch)),\n)\n\n# \u2500\u2500 Model: Faster R-CNN with ResNet-50 FPN backbone \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nNUM_CLASSES = 4  # 3 objects + 1 background\n\nmodel = fasterrcnn_resnet50_fpn(\n    weights="DEFAULT",       # Start from ImageNet pre-trained weights\n    num_classes=NUM_CLASSES,\n)\nmodel.train()\n\n# \u2500\u2500 Optimiser \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# \u2500\u2500 Training Loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndevice = torch.device(\'cuda\') if torch.cuda.is_available() else torch.device(\'cpu\')\nmodel.to(device)\n\nNUM_EPOCHS = 10\nfor epoch in range(NUM_EPOCHS):\n    epoch_loss = 0.0\n    for images, targets in train_loader:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        epoch_loss += losses.item()\n\n    print(f"Epoch {epoch+1}/{NUM_EPOCHS}  Loss: {epoch_loss/len(train_loader):.4f}")\n\ntorch.save(model.state_dict(), "detector_synthetic.pth")\nprint("Model saved.")\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"28-domain-randomisation-strategies",children:"2.8 Domain Randomisation Strategies"}),"\n",(0,i.jsx)(n.p,{children:"The quality of your synthetic dataset depends heavily on the diversity of your randomisation. Here are strategies used in production systems:"}),"\n",(0,i.jsx)(n.h3,{id:"lighting-randomisation",children:"Lighting Randomisation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def randomise_lighting_advanced():\n    """\n    Production-grade lighting randomisation:\n    - Variable number of lights (1\u20134)\n    - Mixed light types (sphere, disk, distant)\n    - Randomised HDR environment maps\n    """\n    # Random number of point lights\n    num_lights = np.random.randint(1, 5)\n    lights = rep.create.light(\n        light_type=rep.distribution.choice(["Sphere", "Disk", "Distant"]),\n        temperature=rep.distribution.uniform(2200, 7500),\n        intensity=rep.distribution.uniform(200, 10000),\n        position=rep.distribution.uniform((-5, -5, 1), (5, 5, 6)),\n        count=num_lights,\n    )\n\n    # Add a randomised dome light (HDR environment map)\n    dome = rep.create.light(\n        light_type="Dome",\n        texture=rep.distribution.choice([\n            "omniverse://localhost/NVIDIA/Assets/Skies/Clear/clear_sky.hdr",\n            "omniverse://localhost/NVIDIA/Assets/Skies/Overcast/overcast.hdr",\n            "omniverse://localhost/NVIDIA/Assets/Skies/Night/night_sky.hdr",\n        ]),\n        intensity=rep.distribution.uniform(100, 2000),\n    )\n    return lights.node\n'})}),"\n",(0,i.jsx)(n.h3,{id:"object-distractors",children:"Object Distractors"}),"\n",(0,i.jsx)(n.p,{children:'Add random "distractor" objects that do not belong to any training class. These force the model to learn what objects look like (not just where objects typically appear):'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def add_distractor_objects():\n    """Add random household objects as distractors."""\n    DISTRACTOR_PATHS = [\n        "/Isaac/Props/Blocks/red_block.usd",\n        "/Isaac/Props/Blocks/blue_block.usd",\n        "/Isaac/Props/KLT_Bin/KLT_bin.usd",\n    ]\n    for i, path in enumerate(np.random.choice(DISTRACTOR_PATHS, size=5)):\n        add_reference_to_stage(\n            usd_path=assets_root + path,\n            prim_path=f"/World/Distractor_{i}"\n        )\n'})}),"\n",(0,i.jsx)(n.h3,{id:"camera-noise-models",children:"Camera Noise Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def add_camera_noise(render_product):\n    """Simulate real camera imperfections."""\n    # Chromatic aberration (colour channel offset)\n    rep.modify.render_settings(\n        chromatic_aberration_strength=rep.distribution.uniform(0.0, 0.003),\n    )\n    # Motion blur\n    rep.modify.render_settings(\n        motion_blur_fraction=rep.distribution.uniform(0.0, 0.05),\n    )\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"29-evaluating-synthetic-to-real-transfer",children:"2.9 Evaluating Synthetic-to-Real Transfer"}),"\n",(0,i.jsxs)(n.p,{children:["After training on synthetic data, evaluate on ",(0,i.jsx)(n.strong,{children:"real images"})," to measure how well the model generalises. A standard metric is the mean Average Precision at IoU threshold 0.5 (mAP@0.5):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# evaluate_transfer.py\nfrom torchvision.datasets import CocoDetection\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nimport torchvision.transforms as T\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nimport torch\n\n# Load your real-image test set (you need to collect and annotate ~200 real images)\ntest_dataset = CocoDetection(\n    root="/real_images/test",\n    annFile="/real_images/test_annotations.json",\n    transforms=T.Compose([T.ToTensor()]),\n)\n\nmodel = fasterrcnn_resnet50_fpn(num_classes=4)\nmodel.load_state_dict(torch.load("detector_synthetic.pth"))\nmodel.eval()\n\nmetric = MeanAveragePrecision(iou_type="bbox")\n\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\nmodel.to(device)\n\nwith torch.no_grad():\n    for images, targets in DataLoader(test_dataset, batch_size=1):\n        images = [img.to(device) for img in images]\n        predictions = model(images)\n        metric.update(predictions, targets)\n\nresults = metric.compute()\nprint(f"mAP@0.5: {results[\'map_50\']:.4f}")\nprint(f"mAP@0.5:0.95: {results[\'map\']:.4f}")\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Benchmarking rule of thumb"}),": A well-randomised Isaac Sim synthetic dataset typically achieves 60\u201380% of the mAP that an equivalently sized real-image dataset achieves. Adding a small number (500\u20131000) of real annotated images to fine-tune the model usually closes most of the remaining gap."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"210-chapter-summary",children:"2.10 Chapter Summary"}),"\n",(0,i.jsx)(n.p,{children:"Synthetic data generation is one of the most powerful tools in the robot AI developer's arsenal:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"The data problem"}),": Real-world data is expensive to collect and annotate. Synthetic data is unlimited and auto-labelled."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"The sim-to-real gap"})," arises from mismatches in lighting, texture, material appearance, and camera models. Domain randomisation, photorealistic rendering, and noise models close this gap."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Isaac Sim Replicator"})," provides a Python API to define randomisers, annotators, and writers as a composable pipeline. The ",(0,i.jsx)(n.code,{children:"with rep.new_layer()"})," + ",(0,i.jsx)(n.code,{children:"rep.trigger.on_frame()"})," pattern drives deterministic data generation."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Annotators"})," (bounding box, segmentation, depth, occlusion) generate ground-truth labels automatically without any human effort."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"COCO format"})," is the standard for object detection datasets. The ",(0,i.jsx)(n.code,{children:"sdg_to_coco"})," converter transforms Replicator output into a format accepted by PyTorch's ",(0,i.jsx)(n.code,{children:"torchvision"}),", Detectron2, and YOLO."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Synthetic-to-real transfer"})," is measured by evaluating on real images. A small real-image fine-tuning step closes most residual gaps."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A colleague argues that using 1,000,000 synthetic images is always better than 10,000 real images for training an object detector. Critique this argument \u2014 under what specific conditions might the real images outperform the synthetic ones?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:'Explain why "distractor objects" improve model performance. What failure mode would you expect in a model trained without distractors, and how would you demonstrate this failure experimentally?'}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"You are generating a synthetic dataset for training a robot to pick up coffee mugs. List five specific scene properties you would randomise and justify each choice with a concrete real-world scenario your robot might encounter."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"load_bbox_sample"})," function above loads bounding boxes from a NumPy structured array. Write a function ",(0,i.jsx)(n.code,{children:"visualise_sample(sample_idx, dataset_dir)"})," that loads the RGB image with Pillow, draws the bounding boxes on it using ",(0,i.jsx)(n.code,{children:"PIL.ImageDraw"}),", colours each box by class, and saves the annotated image."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A team trains on 50,000 synthetic images and achieves mAP@0.5 = 0.42 on their real-image test set. They argue the synthetic data is useless and want to collect 5,000 real images instead. What counter-proposal would you make? Describe a mixed-data training strategy and explain why it is likely to outperform either approach alone."}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},7132(e,n,t){t.d(n,{A:()=>h});var a=t(6540);const i="container_sRRF",r="row_Wfea",o="translateBtn_zwOG",s="showOriginalBtn__IG_",d="translatedContent_uGS7",c="error_H8Lp";var l=t(4848);function h(){const[e,n]=(0,a.useState)("idle"),[t,h]=(0,a.useState)(""),[m,p]=(0,a.useState)(""),u=(0,a.useCallback)(async()=>{n("loading");const e=document.querySelector("article"),t=e?e.innerText:document.body.innerText,a="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${a}/translate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:t,target_language:"urdu"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const i=await e.json();h(i.translated_text),n("translated")}catch(i){p(i.message),n("error")}},[]),g=(0,a.useCallback)(()=>{n("idle"),h(""),p("")},[]);return(0,l.jsxs)("div",{className:i,children:[("idle"===e||"error"===e)&&(0,l.jsxs)("div",{className:r,children:[(0,l.jsx)("button",{className:o,onClick:u,children:"\ud83c\udf10 Translate to Urdu"}),"error"===e&&(0,l.jsxs)("span",{className:c,children:["Translation failed: ",m]})]}),"loading"===e&&(0,l.jsx)("button",{className:o,disabled:!0,children:"\u23f3 Translating\u2026"}),"translated"===e&&(0,l.jsxs)("div",{children:[(0,l.jsx)("button",{className:s,onClick:g,children:"\ud83d\udcd6 Show Original English"}),(0,l.jsx)("div",{className:d,dir:"rtl",lang:"ur",children:t})]})]})}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>s});var a=t(6540);const i={},r=a.createContext(i);function o(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);