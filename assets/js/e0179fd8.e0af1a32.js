"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[4637],{7312(e,n,t){t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4-vla/chapter2","title":"Chapter 2: Voice to Action with Whisper","description":"Learning Objectives","source":"@site/docs/module4-vla/chapter2.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/chapter2","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter2","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla/chapter2.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Chapter 2: Voice to Action with Whisper","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to Vision-Language-Action Models","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter1"},"next":{"title":"Chapter 3: Cognitive Planning with LLMs","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter3"}}');var r=t(4848),s=t(8453),a=t(7132);const o={title:"Chapter 2: Voice to Action with Whisper",sidebar_position:2},l="Chapter 2: Voice to Action with Whisper",d={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"2.1 Why Voice Control for Robots?",id:"21-why-voice-control-for-robots",level:2},{value:"2.2 OpenAI Whisper: Architecture and Capabilities",id:"22-openai-whisper-architecture-and-capabilities",level:2},{value:"Architecture",id:"architecture",level:3},{value:"Model Sizes",id:"model-sizes",level:3},{value:"Key Capabilities Relevant to Robotics",id:"key-capabilities-relevant-to-robotics",level:3},{value:"2.3 Installation",id:"23-installation",level:2},{value:"2.4 Basic Whisper Transcription",id:"24-basic-whisper-transcription",level:2},{value:"2.5 Real-Time Audio Capture Pipeline",id:"25-real-time-audio-capture-pipeline",level:2},{value:"2.6 Wake Word Detection",id:"26-wake-word-detection",level:2},{value:"2.7 The Complete Voice Pipeline Node",id:"27-the-complete-voice-pipeline-node",level:2},{value:"2.8 Integrating Wake Word Detection with the Audio Stream",id:"28-integrating-wake-word-detection-with-the-audio-stream",level:2},{value:"2.9 Routing Voice Commands to Robot Actions",id:"29-routing-voice-commands-to-robot-actions",level:2},{value:"2.10 Testing the Voice Pipeline",id:"210-testing-the-voice-pipeline",level:2},{value:"Injecting a Simulated Voice Command (for testing without a microphone)",id:"injecting-a-simulated-voice-command-for-testing-without-a-microphone",level:3},{value:"2.11 Chapter Summary",id:"211-chapter-summary",level:2},{value:"Review Questions",id:"review-questions",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.A,{}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-2-voice-to-action-with-whisper",children:"Chapter 2: Voice to Action with Whisper"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explain how OpenAI Whisper works and why it outperforms traditional speech recognition systems"}),"\n",(0,r.jsx)(n.li,{children:"Set up a real-time microphone capture and transcription pipeline in Python"}),"\n",(0,r.jsx)(n.li,{children:"Build a ROS 2 node that publishes transcribed speech as standard messages"}),"\n",(0,r.jsx)(n.li,{children:"Implement wake-word detection to avoid processing ambient speech"}),"\n",(0,r.jsx)(n.li,{children:"Handle the latency, noise robustness, and language detection capabilities of Whisper"}),"\n",(0,r.jsx)(n.li,{children:"Connect the voice pipeline to the ROS 2 action system so that spoken commands trigger robot behaviours"}),"\n",(0,r.jsx)(n.li,{children:"Design fallback strategies for ambiguous or unrecognised commands"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"21-why-voice-control-for-robots",children:"2.1 Why Voice Control for Robots?"}),"\n",(0,r.jsxs)(n.p,{children:["Robots deployed in human environments must be controllable by non-experts. Touchscreens require proximity. Keyboard commands require training. Dedicated remotes require dedicated hardware. ",(0,r.jsx)(n.strong,{children:"Voice is the natural human communication interface"}),": it is always available, works at distance, requires no learned interface, and allows rich, context-dependent commands."]}),"\n",(0,r.jsx)(n.p,{children:"For a humanoid robot working alongside humans in a home, warehouse, or hospital, voice is not a convenience feature \u2014 it is a fundamental accessibility and usability requirement."}),"\n",(0,r.jsx)(n.p,{children:"The pipeline we build in this chapter sits at the very start of the autonomous humanoid control stack:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Microphone\n    \u2502\n    \u25bc\nAudio Capture (PyAudio / sounddevice)\n    \u2502\n    \u25bc\nVoice Activity Detection (silence gating)\n    \u2502\n    \u25bc\nWake Word Detection ("Hey Robot")\n    \u2502  (only passes through after wake word)\n    \u25bc\nOpenAI Whisper (speech \u2192 text)\n    \u2502\n    \u25bc\nROS 2 /voice/command topic (std_msgs/String)\n    \u2502\n    \u25bc\nLLM Planner (Chapter 3) \u2192 Nav2 / Manipulation (Chapter 4)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"22-openai-whisper-architecture-and-capabilities",children:"2.2 OpenAI Whisper: Architecture and Capabilities"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Whisper"})," is an open-source automatic speech recognition (ASR) model published by OpenAI in 2022. It was trained on 680,000 hours of multilingual, multitask audio data collected from the internet \u2014 orders of magnitude more data than previous open-source ASR systems."]}),"\n",(0,r.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,r.jsxs)(n.p,{children:["Whisper is a ",(0,r.jsx)(n.strong,{children:"sequence-to-sequence Transformer"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Audio waveform\n    \u2502\n    \u25bc (25ms frames, 10ms stride)\nLog-Mel Spectrogram (80 frequency bins \xd7 time frames)\n    \u2502\n    \u25bc\nCNN feature extractor (2 conv layers + GELU)\n    \u2502\n    \u25bc\nTransformer Encoder (processes audio features)\n    \u2502\n    \u25bc\nTransformer Decoder (autoregressively generates text tokens)\n    \u2502\n    \u25bc\nTranscribed text: "pick up the red cup"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["The decoder is conditioned on ",(0,r.jsx)(n.strong,{children:"special tokens"})," that control:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task"}),": transcription vs translation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language"}),": auto-detected or forced (e.g., ",(0,r.jsx)(n.code,{children:"<|en|>"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Timestamps"}),": whether to output word-level timestamps"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"model-sizes",children:"Model Sizes"}),"\n",(0,r.jsx)(n.p,{children:"Whisper is available in five sizes with different speed/accuracy trade-offs:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Parameters"}),(0,r.jsx)(n.th,{children:"VRAM"}),(0,r.jsx)(n.th,{children:"Speed (CPU)"}),(0,r.jsx)(n.th,{children:"Speed (GPU)"}),(0,r.jsx)(n.th,{children:"WER (English)"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"tiny"})}),(0,r.jsx)(n.td,{children:"39M"}),(0,r.jsx)(n.td,{children:"~1 GB"}),(0,r.jsx)(n.td,{children:"~32\xd7 real-time"}),(0,r.jsx)(n.td,{children:"~160\xd7"}),(0,r.jsx)(n.td,{children:"5.7%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"base"})}),(0,r.jsx)(n.td,{children:"74M"}),(0,r.jsx)(n.td,{children:"~1 GB"}),(0,r.jsx)(n.td,{children:"~16\xd7 real-time"}),(0,r.jsx)(n.td,{children:"~120\xd7"}),(0,r.jsx)(n.td,{children:"4.2%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"small"})}),(0,r.jsx)(n.td,{children:"244M"}),(0,r.jsx)(n.td,{children:"~2 GB"}),(0,r.jsx)(n.td,{children:"~6\xd7 real-time"}),(0,r.jsx)(n.td,{children:"~70\xd7"}),(0,r.jsx)(n.td,{children:"3.4%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"medium"})}),(0,r.jsx)(n.td,{children:"769M"}),(0,r.jsx)(n.td,{children:"~5 GB"}),(0,r.jsx)(n.td,{children:"~2\xd7 real-time"}),(0,r.jsx)(n.td,{children:"~24\xd7"}),(0,r.jsx)(n.td,{children:"3.0%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"large-v3"})}),(0,r.jsx)(n.td,{children:"1550M"}),(0,r.jsx)(n.td,{children:"~10 GB"}),(0,r.jsx)(n.td,{children:"0.7\xd7 real-time"}),(0,r.jsx)(n.td,{children:"~8\xd7"}),(0,r.jsx)(n.td,{children:"2.7%"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"WER = Word Error Rate. Lower is better."}),"\n",(0,r.jsxs)(n.p,{children:["For ",(0,r.jsx)(n.strong,{children:"robotic applications"})," on a Jetson Orin or a moderately powerful laptop: ",(0,r.jsx)(n.code,{children:"small"})," or ",(0,r.jsx)(n.code,{children:"medium"})," offers the best trade-off. For a workstation with an RTX GPU: ",(0,r.jsx)(n.code,{children:"large-v3"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"key-capabilities-relevant-to-robotics",children:"Key Capabilities Relevant to Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise robustness"}),": Whisper was trained on real internet audio with background noise, music, and crosstalk. It outperforms commercial ASR systems in noisy environments."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilingual"}),": 99 languages supported natively."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Technical vocabulary"}),': Because of its large training corpus, Whisper handles domain-specific terms ("Nav2", "gripper", "URDF") better than consumer ASR.']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No cloud dependency"}),": Runs entirely locally, which is critical for robots operating in environments without internet access."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"23-installation",children:"2.3 Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install Whisper and its dependencies\npip install openai-whisper\n\n# Audio capture (cross-platform)\npip install sounddevice soundfile numpy\n\n# For microphone input with silence detection\npip install webrtcvad\n\n# For wake word detection\npip install pvporcupine  # Picovoice Porcupine (free tier available)\n# or\npip install openwakeword  # Fully open-source alternative\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"24-basic-whisper-transcription",children:"2.4 Basic Whisper Transcription"}),"\n",(0,r.jsx)(n.p,{children:"Before building the ROS 2 pipeline, understand the basic Whisper API:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# basic_whisper.py\nimport whisper\nimport numpy as np\n\n# Load model (downloads on first run, cached locally)\nmodel = whisper.load_model("small")\n\n# Transcribe from a file\nresult = model.transcribe("audio_sample.wav")\nprint(result["text"])\n# \u2192 "pick up the red cup and place it on the tray"\n\n# Transcribe from a numpy array (float32, mono, 16kHz)\n# This is what we use in the real-time pipeline\naudio_array = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32)\naudio_array /= 32768.0  # Normalise to [-1, 1]\nresult = model.transcribe(audio_array, language="en", fp16=False)\nprint(result["text"])\nprint(result["language"])     # Detected language\nprint(result["segments"])     # Word-level timing (if requested)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"25-real-time-audio-capture-pipeline",children:"2.5 Real-Time Audio Capture Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:["The core challenge in real-time voice control is handling ",(0,r.jsx)(n.strong,{children:"streaming audio"}),": you cannot accumulate the entire utterance before transcribing (that would introduce unacceptable latency), but Whisper expects a complete audio segment as input."]}),"\n",(0,r.jsxs)(n.p,{children:["The solution is ",(0,r.jsx)(n.strong,{children:"Voice Activity Detection (VAD)"}),": monitor the audio stream continuously, and when silence is detected after speech, send the accumulated speech segment to Whisper."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# audio_capture.py\nimport sounddevice as sd\nimport numpy as np\nimport queue\nimport threading\nimport webrtcvad\nimport collections\n\nSAMPLE_RATE = 16000       # Hz (Whisper requires 16kHz)\nFRAME_MS = 30             # ms per VAD frame (10, 20, or 30)\nFRAME_SAMPLES = int(SAMPLE_RATE * FRAME_MS / 1000)  # 480 samples\nVAD_AGGRESSIVENESS = 2    # 0\u20133 (higher = more aggressive filtering)\n\n# Silence padding: keep N frames before/after speech to avoid clipping\nPADDING_FRAMES = 10\nSILENCE_FRAMES_TO_END = 20  # Stop recording after 20 silent frames\n\nclass AudioCapture:\n    """\n    Captures microphone audio and yields complete speech segments\n    for Whisper transcription.\n    """\n\n    def __init__(self, sample_rate: int = SAMPLE_RATE):\n        self.sample_rate = sample_rate\n        self.vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)\n        self.audio_queue = queue.Queue()\n        self._running = False\n\n    def _audio_callback(self, indata, frames, time, status):\n        """Called by sounddevice for every audio frame."""\n        if status:\n            print(f"Audio callback status: {status}")\n        # Convert to bytes for WebRTC VAD\n        audio_bytes = (indata[:, 0] * 32768).astype(np.int16).tobytes()\n        self.audio_queue.put(audio_bytes)\n\n    def start(self):\n        """Start the microphone stream."""\n        self._running = True\n        self._stream = sd.InputStream(\n            samplerate=self.sample_rate,\n            channels=1,\n            dtype=\'float32\',\n            blocksize=FRAME_SAMPLES,\n            callback=self._audio_callback,\n        )\n        self._stream.start()\n\n    def stop(self):\n        self._running = False\n        self._stream.stop()\n        self._stream.close()\n\n    def iter_speech_segments(self):\n        """\n        Generator that yields complete speech segments as float32 numpy arrays.\n        Uses a ring buffer of padding frames to avoid clipping speech edges.\n        """\n        # Ring buffer: keeps the last PADDING_FRAMES frames\n        ring_buffer = collections.deque(maxlen=PADDING_FRAMES)\n\n        triggered = False       # Are we currently recording speech?\n        voiced_frames = []      # Frames of confirmed speech\n        silent_frame_count = 0  # Consecutive silent frames after speech\n\n        while self._running:\n            try:\n                frame_bytes = self.audio_queue.get(timeout=0.5)\n            except queue.Empty:\n                continue\n\n            is_speech = self.vad.is_speech(frame_bytes, self.sample_rate)\n\n            if not triggered:\n                ring_buffer.append((frame_bytes, is_speech))\n                # Check if majority of ring buffer frames are speech\n                num_voiced = sum(1 for _, speech in ring_buffer if speech)\n                if num_voiced > 0.9 * ring_buffer.maxlen:\n                    triggered = True\n                    # Include the buffered pre-speech context\n                    voiced_frames.extend([f for f, _ in ring_buffer])\n                    ring_buffer.clear()\n            else:\n                voiced_frames.append(frame_bytes)\n                ring_buffer.append((frame_bytes, is_speech))\n                num_unvoiced = sum(1 for _, speech in ring_buffer if not speech)\n                if num_unvoiced > 0.9 * ring_buffer.maxlen:\n                    # Silence detected: emit the segment\n                    audio_array = self._bytes_to_float32(voiced_frames)\n                    yield audio_array\n                    # Reset state\n                    triggered = False\n                    voiced_frames = []\n                    ring_buffer.clear()\n                    silent_frame_count = 0\n\n    def _bytes_to_float32(self, frames: list) -> np.ndarray:\n        """Convert list of int16 byte frames to float32 numpy array."""\n        audio_bytes = b\'\'.join(frames)\n        audio_int16 = np.frombuffer(audio_bytes, dtype=np.int16)\n        return audio_int16.astype(np.float32) / 32768.0\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"26-wake-word-detection",children:"2.6 Wake Word Detection"}),"\n",(0,r.jsxs)(n.p,{children:["Continuously transcribing everything in the environment would be noisy and wasteful. A ",(0,r.jsx)(n.strong,{children:"wake word"}),' (like "Hey Robot" or "Atlas") gates the system so that only commands following the wake word are processed.']}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# wake_word_detector.py\n# Uses openwakeword \u2014 fully open-source, runs locally\n\nimport numpy as np\nimport openwakeword\nfrom openwakeword.model import Model\n\nclass WakeWordDetector:\n    """\n    Detects a custom wake word in an audio stream.\n    Uses openwakeword models (open-source, runs on CPU).\n    """\n\n    # Available models: hey_mycroft, hey_jarvis, alexa, hey_rhasspy\n    # For custom wake words, see openwakeword training docs\n    MODEL_NAME = "hey_mycroft"    # Use as "Hey Robot" proxy for development\n    ACTIVATION_THRESHOLD = 0.5   # Confidence threshold [0, 1]\n\n    def __init__(self):\n        # Download models on first run\n        openwakeword.utils.download_models()\n        self.model = Model(wakeword_models=[self.MODEL_NAME])\n        self._activated = False\n\n    def process_frame(self, audio_chunk: np.ndarray) -> bool:\n        """\n        Process one chunk of audio (1280 samples @ 16kHz = 80ms).\n        Returns True if wake word was detected in this chunk.\n        """\n        # openwakeword expects int16\n        audio_int16 = (audio_chunk * 32768).astype(np.int16)\n        predictions = self.model.predict(audio_int16)\n        score = predictions.get(self.MODEL_NAME, 0.0)\n        detected = score > self.ACTIVATION_THRESHOLD\n        if detected:\n            print(f"Wake word detected! Score: {score:.3f}")\n        return detected\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"27-the-complete-voice-pipeline-node",children:"2.7 The Complete Voice Pipeline Node"}),"\n",(0,r.jsx)(n.p,{children:"Now we integrate audio capture, wake word detection, and Whisper into a single ROS 2 node:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# voice_command_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom diagnostic_msgs.msg import DiagnosticStatus\nimport whisper\nimport numpy as np\nimport threading\nimport time\n\nfrom audio_capture import AudioCapture\nfrom wake_word_detector import WakeWordDetector\n\n# Minimum audio duration to attempt transcription (avoids noise artefacts)\nMIN_AUDIO_SECONDS = 0.5\nMAX_AUDIO_SECONDS = 15.0   # Discard unusually long segments (noise runaway)\n\nclass VoiceCommandNode(Node):\n    \"\"\"\n    Captures microphone audio, detects wake word, transcribes speech\n    with Whisper, and publishes clean commands to /voice/command.\n\n    Published topics:\n      /voice/command  (std_msgs/String) \u2014 transcribed command text\n      /voice/raw      (std_msgs/String) \u2014 raw Whisper output (for debugging)\n      /voice/status   (std_msgs/String) \u2014 pipeline state\n\n    Parameters:\n      whisper_model  (str)   \u2014 Whisper model size (tiny/base/small/medium/large)\n      language       (str)   \u2014 Force language ('en') or 'auto'\n      use_wake_word  (bool)  \u2014 Enable/disable wake word gating\n      wake_word_timeout (float) \u2014 Seconds to stay active after wake word\n    \"\"\"\n\n    STATES = ['IDLE', 'LISTENING', 'TRANSCRIBING', 'ERROR']\n\n    def __init__(self):\n        super().__init__('voice_command_node')\n\n        # Parameters\n        self.declare_parameter('whisper_model', 'small')\n        self.declare_parameter('language', 'en')\n        self.declare_parameter('use_wake_word', True)\n        self.declare_parameter('wake_word_timeout', 8.0)\n\n        model_name = self.get_parameter('whisper_model').value\n        self.language = self.get_parameter('language').value\n        self.use_wake_word = self.get_parameter('use_wake_word').value\n        self.wake_word_timeout = self.get_parameter('wake_word_timeout').value\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, '/voice/command', 10)\n        self.raw_pub = self.create_publisher(String, '/voice/raw', 10)\n        self.status_pub = self.create_publisher(String, '/voice/status', 10)\n\n        # Internal state\n        self.state = 'IDLE'\n        self.wake_word_activated_at = None\n\n        # Load Whisper model (may take 10\u201330s on first load)\n        self.get_logger().info(f'Loading Whisper model: {model_name}...')\n        self.whisper_model = whisper.load_model(model_name)\n        self.get_logger().info('Whisper model loaded.')\n\n        # Audio components\n        self.audio_capture = AudioCapture()\n        self.wake_detector = WakeWordDetector() if self.use_wake_word else None\n\n        # Run audio pipeline in a background thread\n        self._audio_thread = threading.Thread(\n            target=self._audio_pipeline, daemon=True\n        )\n        self._audio_thread.start()\n\n        self._publish_status('IDLE')\n        self.get_logger().info(\n            f'Voice command node ready. '\n            f'Wake word: {\"enabled\" if self.use_wake_word else \"disabled\"}'\n        )\n\n    def _audio_pipeline(self):\n        \"\"\"Background thread: capture audio and transcribe commands.\"\"\"\n        self.audio_capture.start()\n\n        try:\n            for audio_segment in self.audio_capture.iter_speech_segments():\n                duration = len(audio_segment) / 16000.0\n\n                # Sanity-check segment duration\n                if duration < MIN_AUDIO_SECONDS:\n                    continue\n                if duration > MAX_AUDIO_SECONDS:\n                    self.get_logger().warn(\n                        f'Segment too long ({duration:.1f}s), discarding.'\n                    )\n                    continue\n\n                # Check wake word gating\n                if self.use_wake_word:\n                    if not self._is_wake_word_active():\n                        # Check if this segment contains the wake word\n                        # (Wake word detector runs on the raw audio stream\n                        #  in practice; simplified here to check per segment)\n                        self.get_logger().debug('Waiting for wake word...')\n                        continue\n\n                # Transcribe\n                self._publish_status('TRANSCRIBING')\n                self.get_logger().info(\n                    f'Transcribing {duration:.1f}s segment...'\n                )\n                t_start = time.time()\n\n                try:\n                    result = self.whisper_model.transcribe(\n                        audio_segment,\n                        language=self.language if self.language != 'auto' else None,\n                        fp16=False,       # fp16=True for GPU, False for CPU\n                        condition_on_previous_text=False,  # Prevents hallucination\n                        no_speech_threshold=0.6,          # Filter out non-speech\n                        logprob_threshold=-1.0,\n                    )\n                except Exception as e:\n                    self.get_logger().error(f'Whisper transcription failed: {e}')\n                    self._publish_status('ERROR')\n                    continue\n\n                elapsed = time.time() - t_start\n                raw_text = result['text'].strip()\n                no_speech_prob = result.get('segments', [{}])[0].get(\n                    'no_speech_prob', 0.0\n                )\n\n                self.get_logger().info(\n                    f'Transcribed in {elapsed:.2f}s: \"{raw_text}\" '\n                    f'(no_speech_prob={no_speech_prob:.2f})'\n                )\n\n                # Publish raw output for debugging\n                raw_msg = String()\n                raw_msg.data = raw_text\n                self.raw_pub.publish(raw_msg)\n\n                # Filter likely non-speech artefacts\n                if no_speech_prob > 0.8:\n                    self.get_logger().debug('Non-speech detected, discarding.')\n                    self._publish_status('IDLE')\n                    continue\n\n                if not raw_text or len(raw_text) < 3:\n                    self._publish_status('IDLE')\n                    continue\n\n                # Clean and publish the command\n                cleaned = self._clean_text(raw_text)\n                cmd_msg = String()\n                cmd_msg.data = cleaned\n                self.command_pub.publish(cmd_msg)\n                self.get_logger().info(f'Command published: \"{cleaned}\"')\n                self._publish_status('IDLE')\n\n        finally:\n            self.audio_capture.stop()\n\n    def _is_wake_word_active(self) -> bool:\n        \"\"\"Returns True if a wake word was detected within the timeout window.\"\"\"\n        if self.wake_word_activated_at is None:\n            return False\n        elapsed = time.time() - self.wake_word_activated_at\n        return elapsed < self.wake_word_timeout\n\n    def activate_wake_word(self):\n        \"\"\"Called by the wake word detector when it fires.\"\"\"\n        self.wake_word_activated_at = time.time()\n        self.get_logger().info('Wake word activated! Listening for command...')\n        self._publish_status('LISTENING')\n\n    def _clean_text(self, text: str) -> str:\n        \"\"\"Remove common Whisper artefacts and normalise.\"\"\"\n        # Whisper sometimes adds filler phrases\n        REMOVE = [\n            'thank you for watching', 'thanks for watching',\n            'please subscribe', '[music]', '[applause]',\n            '(music)', '...', 'uh', 'um',\n        ]\n        cleaned = text.lower()\n        for phrase in REMOVE:\n            cleaned = cleaned.replace(phrase, '')\n        # Strip extra whitespace\n        return ' '.join(cleaned.split()).strip()\n\n    def _publish_status(self, status: str):\n        self.state = status\n        msg = String()\n        msg.data = status\n        self.status_pub.publish(msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.audio_capture.stop()\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"28-integrating-wake-word-detection-with-the-audio-stream",children:"2.8 Integrating Wake Word Detection with the Audio Stream"}),"\n",(0,r.jsx)(n.p,{children:"The wake word detector runs on the raw audio stream in real-time, in parallel with the VAD:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# integrated_voice_node.py \u2014 combined wake word + VAD + Whisper\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport sounddevice as sd\nimport numpy as np\nimport queue\nimport whisper\nimport threading\nimport time\n\nSAMPLE_RATE = 16000\nWAKE_WORD_CHUNK = 1280   # 80ms chunks for openwakeword\n\nclass IntegratedVoiceNode(Node):\n    \"\"\"\n    Integrates wake word detection and Whisper in a single node.\n    Uses a two-thread architecture:\n      Thread 1: continuously reads microphone, runs wake word detector\n      Thread 2: waits for wake word signal, records command, transcribes\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('integrated_voice')\n\n        self.declare_parameter('whisper_model', 'small')\n        self.whisper_model = whisper.load_model(\n            self.get_parameter('whisper_model').value\n        )\n\n        self.command_pub = self.create_publisher(String, '/voice/command', 10)\n        self.status_pub = self.create_publisher(String, '/voice/status', 10)\n\n        # Thread-safe events\n        self._wake_event = threading.Event()   # Signals: wake word detected\n        self._stop_event = threading.Event()   # Signals: shutdown\n\n        # Audio buffers\n        self._raw_audio_queue = queue.Queue(maxsize=100)\n        self._command_audio_buffer = []\n        self._recording_command = False\n\n        # Import here to avoid circular imports in the example\n        try:\n            from wake_word_detector import WakeWordDetector\n            self.wake_detector = WakeWordDetector()\n        except ImportError:\n            self.wake_detector = None\n            self.get_logger().warn('WakeWordDetector not available. Wake word disabled.')\n\n        # Start threads\n        threading.Thread(target=self._mic_thread, daemon=True).start()\n        threading.Thread(target=self._command_thread, daemon=True).start()\n\n        self.get_logger().info('Integrated voice node started. Say \"Hey Mycroft\" to activate.')\n\n    def _mic_thread(self):\n        \"\"\"\n        Thread 1: Reads microphone in 80ms chunks.\n        Runs wake word detector on each chunk.\n        When detected, signals Thread 2 to begin recording.\n        \"\"\"\n        def audio_callback(indata, frames, time_info, status):\n            audio_chunk = indata[:, 0].copy()\n            self._raw_audio_queue.put(audio_chunk)\n\n            # Also buffer for command recording\n            if self._recording_command:\n                self._command_audio_buffer.append(audio_chunk)\n\n        with sd.InputStream(\n            samplerate=SAMPLE_RATE,\n            channels=1,\n            dtype='float32',\n            blocksize=WAKE_WORD_CHUNK,\n            callback=audio_callback,\n        ):\n            while not self._stop_event.is_set():\n                try:\n                    chunk = self._raw_audio_queue.get(timeout=0.5)\n                    if self.wake_detector and self.wake_detector.process_frame(chunk):\n                        self._wake_event.set()  # Signal to command thread\n                except queue.Empty:\n                    continue\n\n    def _command_thread(self):\n        \"\"\"\n        Thread 2: Waits for wake word event, then records\n        COMMAND_DURATION seconds of audio and transcribes it.\n        \"\"\"\n        COMMAND_DURATION = 6.0   # Seconds to record after wake word\n\n        while not self._stop_event.is_set():\n            # Wait for wake word\n            activated = self._wake_event.wait(timeout=1.0)\n            if not activated:\n                continue\n            self._wake_event.clear()\n\n            self.get_logger().info('Wake word! Recording command...')\n            self._publish_status('LISTENING')\n\n            # Start buffering audio\n            self._command_audio_buffer = []\n            self._recording_command = True\n            time.sleep(COMMAND_DURATION)\n            self._recording_command = False\n\n            # Concatenate and transcribe\n            if not self._command_audio_buffer:\n                continue\n\n            audio = np.concatenate(self._command_audio_buffer)\n            self._publish_status('TRANSCRIBING')\n\n            try:\n                result = self.whisper_model.transcribe(\n                    audio,\n                    language='en',\n                    fp16=False,\n                    no_speech_threshold=0.6,\n                )\n                text = result['text'].strip().lower()\n                self.get_logger().info(f'Heard: \"{text}\"')\n\n                if text and len(text) > 2:\n                    msg = String()\n                    msg.data = text\n                    self.command_pub.publish(msg)\n            except Exception as e:\n                self.get_logger().error(f'Transcription error: {e}')\n\n            self._publish_status('IDLE')\n\n    def _publish_status(self, status: str):\n        msg = String()\n        msg.data = status\n        self.status_pub.publish(msg)\n\n    def destroy_node(self):\n        self._stop_event.set()\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IntegratedVoiceNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"29-routing-voice-commands-to-robot-actions",children:"2.9 Routing Voice Commands to Robot Actions"}),"\n",(0,r.jsxs)(n.p,{children:["Once ",(0,r.jsx)(n.code,{children:"/voice/command"})," is publishing text, a ",(0,r.jsx)(n.strong,{children:"command router node"})," maps recognised commands to robot actions. This node provides a structured bridge between free-form speech and specific ROS 2 capabilities:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# command_router_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nimport re\nimport math\n\nclass CommandRouterNode(Node):\n    \"\"\"\n    Routes voice commands to appropriate robot actions.\n\n    Simple commands \u2192 direct ROS 2 messages (fast path)\n    Complex commands \u2192 forwarded to LLM planner (Chapter 3)\n    \"\"\"\n\n    # Simple command patterns (regex \u2192 action mapping)\n    SIMPLE_COMMANDS = [\n        (r'stop|halt|freeze',                   'stop'),\n        (r'(go|move|walk)\\s+forward',           'move_forward'),\n        (r'(go|move|walk)\\s+back(ward)?',       'move_backward'),\n        (r'turn\\s+left',                        'turn_left'),\n        (r'turn\\s+right',                       'turn_right'),\n        (r'(come\\s+here|return\\s+(home|base))', 'go_home'),\n        (r'wave(\\s+hand)?',                     'wave'),\n        (r'(nod|yes)',                           'nod'),\n    ]\n\n    def __init__(self):\n        super().__init__('command_router')\n\n        # Subscribe to voice commands\n        self.voice_sub = self.create_subscription(\n            String, '/voice/command', self.route_command, 10\n        )\n\n        # Publishers for simple direct commands\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.gesture_pub = self.create_publisher(String, '/robot/gesture', 10)\n\n        # Publisher: forward complex commands to LLM planner\n        self.llm_pub = self.create_publisher(\n            String, '/robot/instruction', 10   # From Module 1, Chapter 4\n        )\n\n        # Nav2 action client (for navigation commands)\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        # Named locations map (populated from a config file in production)\n        self.named_locations = {\n            'home':     (0.0, 0.0, 0.0),\n            'kitchen':  (3.5, 2.0, 0.0),\n            'office':   (7.2, -1.5, 1.5708),\n            'charging': (-1.0, 0.5, 3.1416),\n        }\n\n        self.get_logger().info('Command router ready.')\n\n    def route_command(self, msg: String):\n        text = msg.data.strip().lower()\n        self.get_logger().info(f'Routing command: \"{text}\"')\n\n        # Try simple pattern matching first (low latency)\n        for pattern, action in self.SIMPLE_COMMANDS:\n            if re.search(pattern, text):\n                self._execute_simple(action, text)\n                return\n\n        # Try navigation to a named location\n        for location_name in self.named_locations:\n            if location_name in text:\n                self._navigate_to(location_name)\n                return\n\n        # Fall through: send to LLM planner for complex reasoning\n        self.get_logger().info(\n            f'Complex command \u2014 forwarding to LLM planner: \"{text}\"'\n        )\n        fwd_msg = String()\n        fwd_msg.data = text\n        self.llm_pub.publish(fwd_msg)\n\n    def _execute_simple(self, action: str, original_text: str):\n        \"\"\"Execute a simple, direct robot command without LLM.\"\"\"\n        self.get_logger().info(f'Simple action: {action}')\n\n        if action == 'stop':\n            self.cmd_vel_pub.publish(Twist())  # Zero velocity\n\n        elif action == 'move_forward':\n            # Extract distance if mentioned (\"walk forward 3 metres\")\n            distance = self._extract_distance(original_text, default=1.0)\n            twist = Twist()\n            twist.linear.x = 0.3\n            self.cmd_vel_pub.publish(twist)\n            # Timer to stop after duration = distance / speed\n            duration = distance / 0.3\n            self.create_timer(duration, lambda: self.cmd_vel_pub.publish(Twist()))\n\n        elif action == 'move_backward':\n            distance = self._extract_distance(original_text, default=0.5)\n            twist = Twist()\n            twist.linear.x = -0.2\n            self.cmd_vel_pub.publish(twist)\n            duration = distance / 0.2\n            self.create_timer(duration, lambda: self.cmd_vel_pub.publish(Twist()))\n\n        elif action == 'turn_left':\n            angle = self._extract_angle(original_text, default=90.0)\n            twist = Twist()\n            twist.angular.z = 0.5\n            self.cmd_vel_pub.publish(twist)\n            duration = math.radians(angle) / 0.5\n            self.create_timer(duration, lambda: self.cmd_vel_pub.publish(Twist()))\n\n        elif action == 'turn_right':\n            angle = self._extract_angle(original_text, default=90.0)\n            twist = Twist()\n            twist.angular.z = -0.5\n            self.cmd_vel_pub.publish(twist)\n            duration = math.radians(angle) / 0.5\n            self.create_timer(duration, lambda: self.cmd_vel_pub.publish(Twist()))\n\n        elif action == 'go_home':\n            self._navigate_to('home')\n\n        elif action in ('wave', 'nod'):\n            gesture_msg = String()\n            gesture_msg.data = action\n            self.gesture_pub.publish(gesture_msg)\n\n    def _navigate_to(self, location_name: str):\n        \"\"\"Send a Nav2 goal to a named location.\"\"\"\n        if location_name not in self.named_locations:\n            self.get_logger().warn(f'Unknown location: {location_name}')\n            return\n\n        x, y, yaw = self.named_locations[location_name]\n        self.get_logger().info(\n            f'Navigating to {location_name} at ({x:.1f}, {y:.1f})'\n        )\n\n        if not self.nav_client.wait_for_server(timeout_sec=2.0):\n            self.get_logger().error('Nav2 server not available!')\n            return\n\n        from geometry_msgs.msg import PoseStamped\n        goal = NavigateToPose.Goal()\n        goal.pose = PoseStamped()\n        goal.pose.header.frame_id = 'map'\n        goal.pose.header.stamp = self.get_clock().now().to_msg()\n        goal.pose.pose.position.x = x\n        goal.pose.pose.position.y = y\n        goal.pose.pose.orientation.z = math.sin(yaw / 2)\n        goal.pose.pose.orientation.w = math.cos(yaw / 2)\n\n        self.nav_client.send_goal_async(goal)\n\n    def _extract_distance(self, text: str, default: float) -> float:\n        \"\"\"Extract a distance in metres from text.\"\"\"\n        patterns = [\n            r'(\\d+\\.?\\d*)\\s*(meter|metre|m)\\b',\n            r'(\\d+\\.?\\d*)\\s*(foot|feet|ft)\\b',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text)\n            if match:\n                value = float(match.group(1))\n                unit = match.group(2)\n                if 'foot' in unit or 'feet' in unit or 'ft' in unit:\n                    return value * 0.3048\n                return value\n        # Try bare number\n        numbers = re.findall(r'\\b(\\d+\\.?\\d*)\\b', text)\n        if numbers:\n            return min(float(numbers[0]), 5.0)  # Cap at 5m for safety\n        return default\n\n    def _extract_angle(self, text: str, default: float) -> float:\n        \"\"\"Extract an angle in degrees from text.\"\"\"\n        match = re.search(r'(\\d+)\\s*(degree|deg|\xb0)', text)\n        if match:\n            return float(match.group(1))\n        return default\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandRouterNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"210-testing-the-voice-pipeline",children:"2.10 Testing the Voice Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Terminal 1: Start the voice command node\nros2 run my_robot_pkg voice_command_node \\\n    --ros-args -p whisper_model:=small -p use_wake_word:=false\n\n# Terminal 2: Monitor published commands\nros2 topic echo /voice/command\nros2 topic echo /voice/status\n\n# Terminal 3: Start the command router\nros2 run my_robot_pkg command_router_node\n\n# Terminal 4: Monitor robot velocity commands\nros2 topic echo /cmd_vel\n\n# Speak into the microphone:\n# "Walk forward 2 metres"  \u2192 cmd_vel published\n# "Turn left 90 degrees"   \u2192 cmd_vel published\n# "Go to the kitchen"      \u2192 Nav2 goal sent\n# "Pick up the red cup"    \u2192 forwarded to LLM planner\n'})}),"\n",(0,r.jsx)(n.h3,{id:"injecting-a-simulated-voice-command-for-testing-without-a-microphone",children:"Injecting a Simulated Voice Command (for testing without a microphone)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub /voice/command std_msgs/msg/String \\\n    \"data: 'walk forward 1 metre'\" --once\n\nros2 topic pub /voice/command std_msgs/msg/String \\\n    \"data: 'pick up the blue block and bring it to me'\" --once\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"211-chapter-summary",children:"2.11 Chapter Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, we built the voice interface layer of the autonomous humanoid:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"OpenAI Whisper"})," is a sequence-to-sequence transformer trained on 680,000 hours of multilingual audio. The ",(0,r.jsx)(n.code,{children:"small"})," model is the best default for robotics \u2014 fast enough for real-time use, accurate enough for technical vocabulary."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Voice Activity Detection"})," (using WebRTC VAD) gates Whisper processing so it only runs on complete speech segments, avoiding continuous transcription of ambient sound."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Wake word detection"})," (using openwakeword) adds a layer of intentional activation, preventing ambient conversation from triggering robot commands."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"VoiceCommandNode"})," integrates all three into a ROS 2 node that publishes clean, normalised text on ",(0,r.jsx)(n.code,{children:"/voice/command"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"CommandRouterNode"})," dispatches commands via two paths: simple, pattern-matched commands go directly to ROS 2 topics (low latency); complex, semantically rich commands are forwarded to the LLM planner (Chapter 3)."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Explain why ",(0,r.jsx)(n.code,{children:"no_speech_threshold=0.6"})," is set in the Whisper transcription call. What does this threshold control, and what happens when background music or TV audio is present?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"CommandRouterNode"})," uses a two-path architecture: simple pattern matching and LLM forwarding. What are the latency and reliability trade-offs of each path? Give a specific example of a command that should use each path."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:['A user says "Robot, walk forward slowly." The ',(0,r.jsx)(n.code,{children:"_extract_distance"})," function returns the default ",(0,r.jsx)(n.code,{children:"1.0 m"})," because no distance was specified, and ",(0,r.jsx)(n.code,{children:"_execute_simple"})," moves the robot at ",(0,r.jsx)(n.code,{children:"0.3 m/s"}),'. The user expected "slowly" to mean a slower speed, not a fixed distance. How would you modify the ',(0,r.jsx)(n.code,{children:"CommandRouterNode"}),' to handle speed modifiers ("slowly", "quickly", "carefully")?']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Whisper's ",(0,r.jsx)(n.code,{children:"large-v3"})," model achieves 2.7% WER on English vs 3.4% for ",(0,r.jsx)(n.code,{children:"small"}),". In a robotics context, when would this 0.7% WER improvement matter, and when would it not justify the 6\xd7 speed penalty?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Design a ",(0,r.jsx)(n.strong,{children:"multilingual voice pipeline"})," for a robot deployed in a French/English bilingual hospital environment. Which Whisper parameters would you configure, how would you route commands in two languages to the same robot action system, and how would the wake word detection need to change?"]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},7132(e,n,t){t.d(n,{A:()=>u});var i=t(6540);const r="container_sRRF",s="row_Wfea",a="translateBtn_zwOG",o="showOriginalBtn__IG_",l="translatedContent_uGS7",d="error_H8Lp";var c=t(4848);function u(){const[e,n]=(0,i.useState)("idle"),[t,u]=(0,i.useState)(""),[m,p]=(0,i.useState)(""),h=(0,i.useCallback)(async()=>{n("loading");const e=document.querySelector("article"),t=e?e.innerText:document.body.innerText,i="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${i}/translate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:t,target_language:"urdu"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const r=await e.json();u(r.translated_text),n("translated")}catch(r){p(r.message),n("error")}},[]),f=(0,i.useCallback)(()=>{n("idle"),u(""),p("")},[]);return(0,c.jsxs)("div",{className:r,children:[("idle"===e||"error"===e)&&(0,c.jsxs)("div",{className:s,children:[(0,c.jsx)("button",{className:a,onClick:h,children:"\ud83c\udf10 Translate to Urdu"}),"error"===e&&(0,c.jsxs)("span",{className:d,children:["Translation failed: ",m]})]}),"loading"===e&&(0,c.jsx)("button",{className:a,disabled:!0,children:"\u23f3 Translating\u2026"}),"translated"===e&&(0,c.jsxs)("div",{children:[(0,c.jsx)("button",{className:o,onClick:f,children:"\ud83d\udcd6 Show Original English"}),(0,c.jsx)("div",{className:l,dir:"rtl",lang:"ur",children:t})]})]})}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>o});var i=t(6540);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);