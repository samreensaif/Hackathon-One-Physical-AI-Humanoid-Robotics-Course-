"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[1007],{8377(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module4-vla/chapter4","title":"Chapter 4: Capstone Project \u2014 The Autonomous Humanoid","description":"Learning Objectives","source":"@site/docs/module4-vla/chapter4.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/chapter4","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter4","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla/chapter4.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Chapter 4: Capstone Project \u2014 The Autonomous Humanoid","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Cognitive Planning with LLMs","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/docs/module4-vla/chapter3"}}');var i=t(4848),o=t(8453);const a={title:"Chapter 4: Capstone Project \u2014 The Autonomous Humanoid",sidebar_position:4},r="Chapter 4: Capstone Project \u2014 The Autonomous Humanoid",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"4.1 Project Overview",id:"41-project-overview",level:2},{value:"What the System Does",id:"what-the-system-does",level:3},{value:"Full System Architecture",id:"full-system-architecture",level:3},{value:"4.2 Project Setup",id:"42-project-setup",level:2},{value:"Repository Structure",id:"repository-structure",level:3},{value:"Configuration Files",id:"configuration-files",level:3},{value:"4.3 The Text-to-Speech Node",id:"43-the-text-to-speech-node",level:2},{value:"4.4 The Named Locations Loader",id:"44-the-named-locations-loader",level:2},{value:"4.5 The Master Launch File",id:"45-the-master-launch-file",level:2},{value:"Running the Full System",id:"running-the-full-system",level:3},{value:"4.6 Integration Testing: The Verification Ladder",id:"46-integration-testing-the-verification-ladder",level:2},{value:"Rung 1: ROS 2 Sanity Check",id:"rung-1-ros-2-sanity-check",level:3},{value:"Rung 3: Voice \u2192 Planner Integration Test",id:"rung-3-voice--planner-integration-test",level:3},{value:"4.7 Debugging Common Failures",id:"47-debugging-common-failures",level:2},{value:"Failure: Voice command node transcribes nothing",id:"failure-voice-command-node-transcribes-nothing",level:3},{value:"Failure: LLM planner returns invalid JSON",id:"failure-llm-planner-returns-invalid-json",level:3},{value:"Failure: Nav2 does not reach the goal",id:"failure-nav2-does-not-reach-the-goal",level:3},{value:"Failure: System crashes on startup",id:"failure-system-crashes-on-startup",level:3},{value:"4.8 System Monitoring Dashboard",id:"48-system-monitoring-dashboard",level:2},{value:"System Status Publisher",id:"system-status-publisher",level:3},{value:"4.9 Extension Challenges",id:"49-extension-challenges",level:2},{value:"Extension A: Personality and Expressions (Beginner)",id:"extension-a-personality-and-expressions-beginner",level:3},{value:"Extension B: Multi-Room Mapping (Intermediate)",id:"extension-b-multi-room-mapping-intermediate",level:3},{value:"Extension C: Manipulation with MoveIt 2 (Advanced)",id:"extension-c-manipulation-with-moveit-2-advanced",level:3},{value:"Extension D: Multi-User Voice (Advanced)",id:"extension-d-multi-user-voice-advanced",level:3},{value:"Extension E: Continuous Learning (Research Level)",id:"extension-e-continuous-learning-research-level",level:3},{value:"4.10 Project Deliverables and Assessment",id:"410-project-deliverables-and-assessment",level:2},{value:"Required Deliverables",id:"required-deliverables",level:3},{value:"Assessment Rubric",id:"assessment-rubric",level:3},{value:"4.11 Module 4 and Textbook Summary",id:"411-module-4-and-textbook-summary",level:2},{value:"Module 1: The Robotic Nervous System (ROS 2)",id:"module-1-the-robotic-nervous-system-ros-2",level:3},{value:"Module 2: Simulation",id:"module-2-simulation",level:3},{value:"Module 3: The AI-Robot Brain (NVIDIA Isaac)",id:"module-3-the-ai-robot-brain-nvidia-isaac",level:3},{value:"Module 4: Vision-Language-Action",id:"module-4-vision-language-action",level:3},{value:"The Bigger Picture",id:"the-bigger-picture",level:3},{value:"Review Questions",id:"review-questions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-4-capstone-project--the-autonomous-humanoid",children:"Chapter 4: Capstone Project \u2014 The Autonomous Humanoid"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design and implement the complete software stack for a voice-controlled autonomous humanoid robot"}),"\n",(0,i.jsx)(n.li,{children:"Integrate all prior modules: ROS 2 nodes, Isaac Sim simulation, Isaac ROS/Nav2 navigation, Whisper voice recognition, and LLM cognitive planning"}),"\n",(0,i.jsx)(n.li,{children:"Verify each integration layer independently before combining them into the full system"}),"\n",(0,i.jsx)(n.li,{children:"Write a comprehensive launch file that brings up the complete stack in a single command"}),"\n",(0,i.jsx)(n.li,{children:"Apply systematic debugging methodology when components fail to communicate"}),"\n",(0,i.jsx)(n.li,{children:"Extend the base system with additional capabilities: manipulation, multi-room navigation, and personality"}),"\n",(0,i.jsx)(n.li,{children:"Present your project as a university-level engineering deliverable"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"41-project-overview",children:"4.1 Project Overview"}),"\n",(0,i.jsxs)(n.p,{children:["The capstone project assembles everything from Modules 1\u20134 into a single working system: ",(0,i.jsx)(n.strong,{children:"an autonomous humanoid robot that can understand and execute voiced natural language commands in a simulated home environment."})]}),"\n",(0,i.jsx)(n.h3,{id:"what-the-system-does",children:"What the System Does"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'User says: "Hey Robot, go to the kitchen and get me a glass of water"\n    \u2502\n    \u25bc [Chapter 2] Whisper ASR\n"go to the kitchen and get me a glass of water"\n    \u2502\n    \u25bc [Chapter 3] LLM Planner (Claude)\nPlan: [speak confirm] \u2192 [navigate kitchen] \u2192 [scan] \u2192 [identify glass]\n      \u2192 [pick glass] \u2192 [navigate user] \u2192 [place glass] \u2192 [speak done]\n    \u2502\n    \u25bc [Module 3] Nav2 + cuVSLAM (or Isaac Sim)\nRobot navigates to kitchen, builds map, localises continuously\n    \u2502\n    \u25bc [Module 3] Isaac ROS Vision\nObject detection identifies glass, estimates 6-DoF pose\n    \u2502\n    \u25bc [Module 1] Manipulation ROS 2 nodes\nRobot picks up glass, carries it to user, places it\n    \u2502\n    \u25bc\nUser has water. Task complete.\n'})}),"\n",(0,i.jsx)(n.h3,{id:"full-system-architecture",children:"Full System Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    AUTONOMOUS HUMANOID \u2014 FULL STACK                         \u2502\n\u2502                                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502   Hardware   \u2502   \u2502  Simulation  \u2502   \u2502    Cognitive Layer           \u2502   \u2502\n\u2502  \u2502  (or Sim)    \u2502   \u2502  Isaac Sim   \u2502   \u2502                              \u2502   \u2502\n\u2502  \u2502              \u2502   \u2502  (Gazebo)    \u2502   \u2502  voice_command_node          \u2502   \u2502\n\u2502  \u2502  Stereo cam  \u2502   \u2502              \u2502   \u2502    (Whisper ASR)             \u2502   \u2502\n\u2502  \u2502  LiDAR       \u2502   \u2502  Physics:    \u2502   \u2502         \u2502                    \u2502   \u2502\n\u2502  \u2502  IMU         \u2502   \u2502  PhysX       \u2502   \u2502         \u25bc                    \u2502   \u2502\n\u2502  \u2502  Joints      \u2502   \u2502  Rendering:  \u2502   \u2502  command_router_node         \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  RTX         \u2502   \u2502    (pattern matching)        \u2502   \u2502\n\u2502         \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502         \u2502                    \u2502   \u2502\n\u2502         \u2502                  \u2502           \u2502         \u25bc                    \u2502   \u2502\n\u2502         \u25bc                  \u25bc           \u2502  llm_planner_node            \u2502   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502    (Claude)                  \u2502   \u2502\n\u2502  \u2502   Sensor Processing Layer       \u2502   \u2502         \u2502                    \u2502   \u2502\n\u2502  \u2502                                 \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u2502  isaac_ros_visual_slam          \u2502             \u2502                         \u2502\n\u2502  \u2502    \u2192 /visual_slam/odometry      \u2502             \u2502 /robot/instruction       \u2502\n\u2502  \u2502                                 \u2502             \u2502 (String)                 \u2502\n\u2502  \u2502  nvblox                         \u2502             \u2502                         \u2502\n\u2502  \u2502    \u2192 /nvblox/occupancy          \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502                                 \u2502   \u2502   Execution Layer            \u2502   \u2502\n\u2502  \u2502  isaac_ros_object_detection     \u2502   \u2502                              \u2502   \u2502\n\u2502  \u2502    \u2192 /detections                \u2502   \u2502  llm_planner_node            \u2502   \u2502\n\u2502  \u2502                                 \u2502   \u2502  (also does execution)       \u2502   \u2502\n\u2502  \u2502  robot_state_publisher          \u2502   \u2502    \u2502                         \u2502   \u2502\n\u2502  \u2502    \u2192 /tf, /joint_states         \u2502   \u2502    \u251c\u2500 Nav2 Action Client      \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502    \u251c\u2500 Manipulation Client     \u2502   \u2502\n\u2502                                        \u2502    \u2514\u2500 /cmd_vel publisher      \u2502   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502                              \u2502   \u2502\n\u2502  \u2502   Navigation Layer              \u2502   \u2502  twist_to_gait_node          \u2502   \u2502\n\u2502  \u2502                                 \u2502   \u2502    \u2192 /gait/command           \u2502   \u2502\n\u2502  \u2502  Nav2 BT Navigator             \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u2502  Planner (Smac Hybrid A*)       \u2502                                       \u2502\n\u2502  \u2502  Controller (DWB)               \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Recoveries                     \u2502   \u2502   Safety Layer               \u2502   \u2502\n\u2502  \u2502  Costmap (global + local)       \u2502   \u2502                              \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  safety_watchdog_node        \u2502   \u2502\n\u2502                                        \u2502  velocity_limiter_node       \u2502   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  emergency_stop_node         \u2502   \u2502\n\u2502  \u2502   Monitoring (RViz2 + rqt)      \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"42-project-setup",children:"4.2 Project Setup"}),"\n",(0,i.jsx)(n.h3,{id:"repository-structure",children:"Repository Structure"}),"\n",(0,i.jsx)(n.p,{children:"Create a new ROS 2 package for the capstone:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\n\nros2 pkg create --build-type ament_python \\\n    --dependencies rclpy std_msgs geometry_msgs nav2_msgs \\\n                   sensor_msgs tf2_ros tf2_geometry_msgs \\\n    autonomous_humanoid_pkg\n"})}),"\n",(0,i.jsx)(n.p,{children:"Organise the package:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"autonomous_humanoid_pkg/\n\u251c\u2500\u2500 package.xml\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 autonomous_humanoid_pkg/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 voice_command_node.py        \u2190 Chapter 2\n\u2502   \u251c\u2500\u2500 audio_capture.py             \u2190 Chapter 2\n\u2502   \u251c\u2500\u2500 wake_word_detector.py        \u2190 Chapter 2\n\u2502   \u251c\u2500\u2500 command_router_node.py       \u2190 Chapter 2\n\u2502   \u251c\u2500\u2500 llm_planner_node.py          \u2190 Chapter 3\n\u2502   \u251c\u2500\u2500 planner_prompts.py           \u2190 Chapter 3\n\u2502   \u251c\u2500\u2500 conversation_manager.py      \u2190 Chapter 3\n\u2502   \u251c\u2500\u2500 safety_watchdog_node.py      \u2190 Module 1, Chapter 4\n\u2502   \u251c\u2500\u2500 twist_to_gait_node.py        \u2190 Module 3, Chapter 4\n\u2502   \u2514\u2500\u2500 text_to_speech_node.py       \u2190 New: TTS output\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 nav2_params.yaml             \u2190 Module 3, Chapter 4\n\u2502   \u251c\u2500\u2500 planner_config.yaml          \u2190 New: planner settings\n\u2502   \u2514\u2500\u2500 named_locations.yaml         \u2190 New: map location database\n\u251c\u2500\u2500 launch/\n\u2502   \u251c\u2500\u2500 full_system.launch.py        \u2190 Master launch file\n\u2502   \u251c\u2500\u2500 simulation.launch.py         \u2190 Isaac Sim / Gazebo\n\u2502   \u251c\u2500\u2500 navigation.launch.py         \u2190 Nav2 + VSLAM\n\u2502   \u2514\u2500\u2500 cognition.launch.py          \u2190 Voice + Planner\n\u251c\u2500\u2500 maps/\n\u2502   \u2514\u2500\u2500 home_environment.yaml        \u2190 Pre-built map\n\u251c\u2500\u2500 urdf/\n\u2502   \u2514\u2500\u2500 humanoid.urdf               \u2190 Robot description\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 test_voice_pipeline.py\n    \u251c\u2500\u2500 test_planner.py\n    \u2514\u2500\u2500 test_integration.py\n"})}),"\n",(0,i.jsx)(n.h3,{id:"configuration-files",children:"Configuration Files"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# config/planner_config.yaml\nllm_planner:\n  ros__parameters:\n    model: "claude-opus-4-6"\n    max_tokens: 2048\n    max_retries: 2\n    dry_run: false\n    replan_max_attempts: 2\n\nvoice_command_node:\n  ros__parameters:\n    whisper_model: "small"\n    language: "en"\n    use_wake_word: true\n    wake_word_timeout: 8.0\n\ncommand_router:\n  ros__parameters:\n    llm_threshold: true    # Forward ambiguous commands to LLM\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# config/named_locations.yaml\n# Named locations in the home environment (map frame coordinates)\nlocations:\n  home:       {x: 0.0,  y: 0.0,  yaw: 0.0}\n  kitchen:    {x: 3.5,  y: 2.0,  yaw: 0.0}\n  living_room:{x: 1.5,  y: -2.5, yaw: 1.57}\n  bedroom:    {x: 7.0,  y: 1.0,  yaw: 3.14}\n  bathroom:   {x: 6.0,  y: -1.5, yaw: -1.57}\n  front_door: {x: -2.0, y: 0.0,  yaw: 3.14}\n  charging:   {x: -1.0, y: 0.5,  yaw: 0.0}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"43-the-text-to-speech-node",children:"4.3 The Text-to-Speech Node"}),"\n",(0,i.jsxs)(n.p,{children:["The robot should be able to speak its status and confirmations. Here is a simple TTS node using the ",(0,i.jsx)(n.code,{children:"pyttsx3"})," library (offline, no API required):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# text_to_speech_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport threading\nimport queue\n\nclass TextToSpeechNode(Node):\n    \"\"\"\n    Subscribes to /robot/speak and synthesises speech.\n    Uses pyttsx3 for offline, privacy-preserving TTS.\n    Install: pip install pyttsx3\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('text_to_speech')\n\n        self.declare_parameter('voice_rate', 150)   # Words per minute\n        self.declare_parameter('volume', 0.9)       # 0.0\u20131.0\n\n        self._speech_queue = queue.Queue()\n        self._engine = None\n        self._init_tts()\n\n        self.speak_sub = self.create_subscription(\n            String, '/robot/speak', self.speak_callback, 10\n        )\n\n        # Start TTS in background thread (pyttsx3 is not thread-safe)\n        self._tts_thread = threading.Thread(\n            target=self._tts_loop, daemon=True\n        )\n        self._tts_thread.start()\n\n        self.get_logger().info('TTS node started.')\n\n    def _init_tts(self):\n        try:\n            import pyttsx3\n            self._engine = pyttsx3.init()\n            self._engine.setProperty(\n                'rate', self.get_parameter('voice_rate').value\n            )\n            self._engine.setProperty(\n                'volume', self.get_parameter('volume').value\n            )\n            # Set a clear, neutral voice\n            voices = self._engine.getProperty('voices')\n            if voices:\n                self._engine.setProperty('voice', voices[0].id)\n        except Exception as e:\n            self.get_logger().warn(f'TTS init failed: {e}. Speech disabled.')\n\n    def speak_callback(self, msg: String):\n        text = msg.data.strip()\n        if text:\n            self._speech_queue.put(text)\n            self.get_logger().info(f'Queued speech: \"{text}\"')\n\n    def _tts_loop(self):\n        \"\"\"Background thread: process speech queue one item at a time.\"\"\"\n        while True:\n            text = self._speech_queue.get()\n            if text is None:\n                break\n            if self._engine:\n                try:\n                    self._engine.say(text)\n                    self._engine.runAndWait()\n                except Exception as e:\n                    self.get_logger().error(f'TTS error: {e}')\n\n    def destroy_node(self):\n        self._speech_queue.put(None)  # Signal shutdown\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = TextToSpeechNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"44-the-named-locations-loader",children:"4.4 The Named Locations Loader"}),"\n",(0,i.jsx)(n.p,{children:"A utility node that reads the YAML location database and makes it available to other nodes via a service:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# named_locations_service.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_srvs.srv import Trigger\nimport yaml\nimport json\nimport os\nfrom ament_index_python.packages import get_package_share_directory\n\nclass NamedLocationsService(Node):\n    \"\"\"\n    Loads named locations from YAML and serves them via a ROS 2 service.\n    Also publishes the location list on startup.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('named_locations_service')\n\n        pkg_share = get_package_share_directory('autonomous_humanoid_pkg')\n        yaml_path = os.path.join(pkg_share, 'config', 'named_locations.yaml')\n\n        self.locations = self._load_locations(yaml_path)\n\n        # Service: returns all known locations as JSON\n        self.srv = self.create_service(\n            Trigger, '/robot/get_locations', self.handle_get_locations\n        )\n\n        # Publisher: broadcast on startup\n        from std_msgs.msg import String\n        self.pub = self.create_publisher(String, '/robot/known_locations', 1)\n        from std_msgs.msg import String\n        msg = String()\n        msg.data = json.dumps(self.locations)\n        # Publish after a brief delay (allow subscribers to connect)\n        self.create_timer(1.0, lambda: self.pub.publish(msg))\n\n        self.get_logger().info(\n            f'Loaded {len(self.locations)} named locations: '\n            f'{list(self.locations.keys())}'\n        )\n\n    def _load_locations(self, path: str) -> dict:\n        try:\n            with open(path, 'r') as f:\n                data = yaml.safe_load(f)\n            return data.get('locations', {})\n        except FileNotFoundError:\n            self.get_logger().warn(f'Location file not found: {path}')\n            return {}\n\n    def handle_get_locations(self, request, response):\n        response.success = True\n        response.message = json.dumps(self.locations)\n        return response\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NamedLocationsService()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"45-the-master-launch-file",children:"4.5 The Master Launch File"}),"\n",(0,i.jsx)(n.p,{children:"This launch file starts the complete system in the correct order, respecting dependencies:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/full_system.launch.py\nimport os\nfrom launch import LaunchDescription\nfrom launch.actions import (\n    DeclareLaunchArgument,\n    IncludeLaunchDescription,\n    TimerAction,\n    GroupAction,\n    LogInfo,\n)\nfrom launch.conditions import IfCondition, UnlessCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\nPKG = 'autonomous_humanoid_pkg'\nNAV2_PKG = 'nav2_bringup'\n\ndef generate_launch_description():\n    pkg_share = get_package_share_directory(PKG)\n    nav2_share = get_package_share_directory(NAV2_PKG)\n\n    # \u2500\u2500 Launch Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    use_sim_arg = DeclareLaunchArgument(\n        'use_sim', default_value='true',\n        description='true=Isaac Sim/Gazebo, false=real hardware'\n    )\n    use_voice_arg = DeclareLaunchArgument(\n        'use_voice', default_value='true',\n        description='Enable microphone voice input'\n    )\n    whisper_model_arg = DeclareLaunchArgument(\n        'whisper_model', default_value='small',\n        description='Whisper model size: tiny/base/small/medium/large'\n    )\n    dry_run_arg = DeclareLaunchArgument(\n        'dry_run', default_value='false',\n        description='Plan but do not execute (for testing)'\n    )\n\n    use_sim      = LaunchConfiguration('use_sim')\n    use_voice    = LaunchConfiguration('use_voice')\n    whisper_model = LaunchConfiguration('whisper_model')\n    dry_run      = LaunchConfiguration('dry_run')\n\n    params_file = os.path.join(pkg_share, 'config', 'nav2_params.yaml')\n    map_file    = os.path.join(pkg_share, 'maps', 'home_environment.yaml')\n    planner_cfg = os.path.join(pkg_share, 'config', 'planner_config.yaml')\n\n    # \u2500\u2500 Layer 0: Robot Description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        name='robot_state_publisher',\n        output='screen',\n        parameters=[{\n            'robot_description':\n                open(os.path.join(pkg_share, 'urdf', 'humanoid.urdf')).read()\n        }],\n    )\n\n    # \u2500\u2500 Layer 1: Simulation (conditional) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    simulation = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource(\n            os.path.join(pkg_share, 'launch', 'simulation.launch.py')\n        ),\n        condition=IfCondition(use_sim),\n    )\n\n    # \u2500\u2500 Layer 2: Sensing and Mapping \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # Delay 5s to allow simulation to initialise\n    sensing_layer = TimerAction(period=5.0, actions=[\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(pkg_share, 'launch', 'navigation.launch.py')\n            ),\n            launch_arguments={\n                'use_sim': use_sim,\n                'params_file': params_file,\n                'map': map_file,\n            }.items(),\n        ),\n    ])\n\n    # \u2500\u2500 Layer 3: Safety (always starts early) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    safety_watchdog = Node(\n        package=PKG,\n        executable='safety_watchdog_node',\n        name='safety_watchdog',\n        output='screen',\n    )\n\n    twist_to_gait = Node(\n        package=PKG,\n        executable='twist_to_gait_node',\n        name='twist_to_gait',\n        output='screen',\n    )\n\n    # \u2500\u2500 Layer 4: Named Locations Service \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    locations_service = Node(\n        package=PKG,\n        executable='named_locations_service',\n        name='named_locations',\n        output='screen',\n    )\n\n    # \u2500\u2500 Layer 5: Cognition (voice + planner) \u2014 delay 10s \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    cognition_layer = TimerAction(period=10.0, actions=[\n\n        # TTS always on (robot needs to speak even without mic)\n        Node(\n            package=PKG,\n            executable='text_to_speech_node',\n            name='tts',\n            output='screen',\n        ),\n\n        # Voice input (conditional on use_voice)\n        Node(\n            package=PKG,\n            executable='voice_command_node',\n            name='voice_input',\n            output='screen',\n            parameters=[{\n                'whisper_model': whisper_model,\n                'use_wake_word': True,\n            }],\n            condition=IfCondition(use_voice),\n        ),\n\n        # Command router\n        Node(\n            package=PKG,\n            executable='command_router_node',\n            name='command_router',\n            output='screen',\n        ),\n\n        # LLM planner\n        Node(\n            package=PKG,\n            executable='llm_planner_node',\n            name='llm_planner',\n            output='screen',\n            parameters=[\n                planner_cfg,\n                {'dry_run': dry_run},\n            ],\n        ),\n    ])\n\n    # \u2500\u2500 Layer 6: Monitoring \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    rviz = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', os.path.join(pkg_share, 'config', 'full_system.rviz')],\n        output='screen',\n    )\n\n    return LaunchDescription([\n        # Arguments\n        use_sim_arg, use_voice_arg, whisper_model_arg, dry_run_arg,\n\n        # Startup log\n        LogInfo(msg='=== Autonomous Humanoid System Starting ==='),\n\n        # Layers (ordered by dependency)\n        robot_state_publisher,\n        simulation,\n        safety_watchdog,\n        twist_to_gait,\n        locations_service,\n        sensing_layer,     # Delayed 5s\n        cognition_layer,   # Delayed 10s\n        rviz,\n    ])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"running-the-full-system",children:"Running the Full System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Option A: With simulation (no real robot required)\nros2 launch autonomous_humanoid_pkg full_system.launch.py \\\n    use_sim:=true use_voice:=true whisper_model:=small\n\n# Option B: On real hardware\nros2 launch autonomous_humanoid_pkg full_system.launch.py \\\n    use_sim:=false use_voice:=true whisper_model:=medium\n\n# Option C: Dry run (test planning without execution)\nros2 launch autonomous_humanoid_pkg full_system.launch.py \\\n    use_sim:=true use_voice:=false dry_run:=true\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"46-integration-testing-the-verification-ladder",children:"4.6 Integration Testing: The Verification Ladder"}),"\n",(0,i.jsx)(n.p,{children:"Test each integration point systematically before testing the full stack. This approach isolates failures to a specific layer."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Rung 6: Full pipeline end-to-end\n    "Hey Robot, go to the kitchen"\n    \u2191\nRung 5: LLM Planner \u2192 Nav2\n    ros2 topic pub /robot/instruction ... "navigate to kitchen"\n    \u2191\nRung 4: Nav2 navigation\n    ros2 action send_goal /navigate_to_pose ...\n    \u2191\nRung 3: Voice \u2192 Planner\n    ros2 topic pub /voice/command ... "go to the kitchen"\n    \u2191\nRung 2: Whisper transcription\n    python3 -c "import whisper; m=whisper.load_model(\'small\'); print(m.transcribe(\'test.wav\'))"\n    \u2191\nRung 1: ROS 2 system\n    ros2 run demo_nodes_py talker\n    ros2 run demo_nodes_py listener\n'})}),"\n",(0,i.jsx)(n.h3,{id:"rung-1-ros-2-sanity-check",children:"Rung 1: ROS 2 Sanity Check"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Verify the workspace is built and sourced\ncolcon build --packages-select autonomous_humanoid_pkg\nsource ~/ros2_ws/install/setup.bash\n\n# Verify all nodes can be listed\nros2 run autonomous_humanoid_pkg llm_planner_node &\nros2 node list | grep llm_planner\n# Expected: /llm_planner\n"})}),"\n",(0,i.jsx)(n.h3,{id:"rung-3-voice--planner-integration-test",children:"Rung 3: Voice \u2192 Planner Integration Test"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# test/test_integration.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport threading\nimport time\nimport pytest\n\nclass IntegrationTester(Node):\n    def __init__(self):\n        super().__init__('integration_tester')\n        self.received_plans = []\n\n        self.voice_pub = self.create_publisher(\n            String, '/voice/command', 10\n        )\n        self.plan_sub = self.create_subscription(\n            String, '/planner/current_plan', self.plan_callback, 10\n        )\n\n    def plan_callback(self, msg: String):\n        import json\n        plan = json.loads(msg.data)\n        self.received_plans.append(plan)\n\n    def send_voice_command(self, text: str):\n        msg = String()\n        msg.data = text\n        self.voice_pub.publish(msg)\n\n@pytest.fixture(autouse=True)\ndef ros_context():\n    rclpy.init()\n    yield\n    rclpy.shutdown()\n\ndef test_voice_to_plan_pipeline():\n    \"\"\"Test that a voice command produces a valid plan.\"\"\"\n    tester = IntegrationTester()\n\n    # Send command\n    tester.send_voice_command(\"go to the kitchen\")\n\n    # Spin for up to 15 seconds waiting for a plan\n    deadline = time.time() + 15.0\n    while time.time() < deadline and not tester.received_plans:\n        rclpy.spin_once(tester, timeout_sec=0.1)\n\n    assert len(tester.received_plans) > 0, \"No plan received within 15s\"\n\n    plan = tester.received_plans[0]\n    assert 'steps' in plan\n    assert len(plan['steps']) > 0\n\n    # Check that navigation step is present\n    actions = [s['action'] for s in plan['steps']]\n    assert 'navigate_to_named' in actions, (\n        f\"Expected navigate_to_named in plan, got: {actions}\"\n    )\n\n    tester.destroy_node()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"47-debugging-common-failures",children:"4.7 Debugging Common Failures"}),"\n",(0,i.jsx)(n.h3,{id:"failure-voice-command-node-transcribes-nothing",children:"Failure: Voice command node transcribes nothing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check microphone is detected\npython3 -c \"import sounddevice as sd; print(sd.query_devices())\"\n\n# Check Whisper can transcribe a test file\npython3 -c \"\nimport whisper\nm = whisper.load_model('small')\nr = m.transcribe('test_audio.wav', language='en')\nprint(r['text'])\n\"\n\n# Check the ROS 2 topic is publishing\nros2 topic echo /voice/raw     # Should show raw transcription\nros2 topic echo /voice/status  # Should cycle IDLE\u2192TRANSCRIBING\u2192IDLE\n"})}),"\n",(0,i.jsx)(n.h3,{id:"failure-llm-planner-returns-invalid-json",children:"Failure: LLM planner returns invalid JSON"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check API key is set\necho $ANTHROPIC_API_KEY  # Should not be empty\n\n# Enable debug logging to see raw LLM output\nros2 run autonomous_humanoid_pkg llm_planner_node \\\n    --ros-args --log-level DEBUG\n\n# Monitor raw planner output\nros2 topic echo /planner/current_plan\n"})}),"\n",(0,i.jsx)(n.h3,{id:"failure-nav2-does-not-reach-the-goal",children:"Failure: Nav2 does not reach the goal"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check costmap is populated\nros2 topic echo /global_costmap/costmap --once\n\n# Check VSLAM is publishing odometry\nros2 topic hz /visual_slam/tracking/odometry\n# Expected: ~30 Hz\n\n# Check Nav2 status\nros2 action list\n# Expected: /navigate_to_pose listed\n\n# Visualise in RViz2\n# Add: Map, Global Path, Local Path, Robot Model, TF\n"})}),"\n",(0,i.jsx)(n.h3,{id:"failure-system-crashes-on-startup",children:"Failure: System crashes on startup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Check build is clean\ncd ~/ros2_ws\ncolcon build 2>&1 | grep -E "ERROR|error"\n\n# Check all Python imports succeed\npython3 -c "\nimport rclpy\nimport whisper\nfrom anthropic import Anthropic\nimport sounddevice\nprint(\'All imports OK\')\n"\n\n# Start nodes individually to isolate the failing one\nros2 run autonomous_humanoid_pkg text_to_speech_node &\nros2 run autonomous_humanoid_pkg voice_command_node &\nros2 run autonomous_humanoid_pkg command_router_node &\n# Watch which one produces errors\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"48-system-monitoring-dashboard",children:"4.8 System Monitoring Dashboard"}),"\n",(0,i.jsxs)(n.p,{children:["Use ",(0,i.jsx)(n.code,{children:"rqt"})," and RViz2 to build a live monitoring view:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Launch the full monitoring suite\nrqt &\n\n# In rqt, add these plugins:\n# Plugins \u2192 Topics \u2192 Topic Monitor\n#   \u2192 /voice/status\n#   \u2192 /planner/status\n#   \u2192 /planner/current_step\n#   \u2192 /vslam/alert\n#   \u2192 /cmd_vel\n\n# Plugins \u2192 Visualization \u2192 Plot\n#   \u2192 /visual_slam/tracking/odometry/pose/pose/position/x\n#   \u2192 /visual_slam/tracking/odometry/pose/pose/position/y\n#   (draws the robot's path in real-time)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"system-status-publisher",children:"System Status Publisher"}),"\n",(0,i.jsx)(n.p,{children:"A monitoring node that aggregates all component statuses into a single dashboard topic:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# system_monitor_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport time\n\nclass SystemMonitorNode(Node):\n    \"\"\"\n    Aggregates component statuses and publishes a unified system health report.\n    \"\"\"\n\n    COMPONENTS = [\n        '/voice/status',\n        '/planner/status',\n        '/vslam/alert',\n        '/planner/current_step',\n    ]\n\n    def __init__(self):\n        super().__init__('system_monitor')\n\n        self.status = {\n            'voice':    'UNKNOWN',\n            'planner':  'UNKNOWN',\n            'vslam':    'UNKNOWN',\n            'nav2':     'UNKNOWN',\n            'uptime_s': 0.0,\n            'start_time': time.time(),\n        }\n\n        # Subscribe to each component status\n        self.create_subscription(\n            String, '/voice/status',\n            lambda m: self._update('voice', m.data), 10\n        )\n        self.create_subscription(\n            String, '/planner/status',\n            lambda m: self._update('planner', m.data), 10\n        )\n        self.create_subscription(\n            String, '/vslam/alert',\n            lambda m: self._update('vslam', m.data), 10\n        )\n\n        # Publish dashboard at 1 Hz\n        self.dashboard_pub = self.create_publisher(\n            String, '/system/dashboard', 10\n        )\n        self.create_timer(1.0, self.publish_dashboard)\n\n        self.get_logger().info('System monitor started.')\n\n    def _update(self, component: str, value: str):\n        self.status[component] = value\n\n    def publish_dashboard(self):\n        self.status['uptime_s'] = round(\n            time.time() - self.status['start_time'], 1\n        )\n        msg = String()\n        msg.data = json.dumps(self.status, indent=2)\n        self.dashboard_pub.publish(msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SystemMonitorNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"49-extension-challenges",children:"4.9 Extension Challenges"}),"\n",(0,i.jsx)(n.p,{children:"Once the base system is working, extend it with these challenges ordered by difficulty:"}),"\n",(0,i.jsx)(n.h3,{id:"extension-a-personality-and-expressions-beginner",children:"Extension A: Personality and Expressions (Beginner)"}),"\n",(0,i.jsxs)(n.p,{children:["Add a ",(0,i.jsx)(n.strong,{children:"personality layer"})," that gives the robot a consistent character:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# personality_node.py\nROBOT_NAME = "Atlas"\nGREETINGS = [\n    f"Hello! I\'m {ROBOT_NAME}, ready to help.",\n    f"Hi there! {ROBOT_NAME} at your service.",\n    f"Good to see you! What can I do for you?",\n]\n\nACKNOWLEDGEMENTS = [\n    "Sure, I\'ll get right on that.",\n    "Consider it done!",\n    "On my way!",\n    "I\'m on it.",\n]\n\n# The robot randomly selects from these on each interaction\n# Add idle behaviours: look around when not busy, approach when someone\n# enters the room (face detection trigger), wave when someone waves.\n'})}),"\n",(0,i.jsx)(n.h3,{id:"extension-b-multi-room-mapping-intermediate",children:"Extension B: Multi-Room Mapping (Intermediate)"}),"\n",(0,i.jsxs)(n.p,{children:["Extend the named locations to support ",(0,i.jsx)(n.strong,{children:"dynamic discovery"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# The robot is told \"there's a bottle on the kitchen counter\"\n# It stores this as a dynamic object location:\nself.object_locations['bottle'] = {\n    'room': 'kitchen',\n    'position': (3.5, 2.2),\n    'confidence': 0.9,\n    'timestamp': time.time(),\n}\n\n# Later: \"bring me the bottle\" resolves to navigate_to kitchen + pick bottle\n"})}),"\n",(0,i.jsx)(n.h3,{id:"extension-c-manipulation-with-moveit-2-advanced",children:"Extension C: Manipulation with MoveIt 2 (Advanced)"}),"\n",(0,i.jsxs)(n.p,{children:["Replace the placeholder ",(0,i.jsx)(n.code,{children:"pick_object"})," and ",(0,i.jsx)(n.code,{children:"place_object"})," handlers with real MoveIt 2 motion planning:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from moveit_msgs.action import MoveGroup\nfrom rclpy.action import ActionClient\n\nclass ManipulationExecutor(Node):\n    def __init__(self):\n        super().__init__('manipulation_executor')\n        self.moveit_client = ActionClient(self, MoveGroup, '/move_action')\n\n    def pick_object(self, object_pose, approach='top'):\n        \"\"\"\n        Full pick sequence:\n        1. Plan approach trajectory to pre-grasp pose (MoveIt)\n        2. Open gripper\n        3. Plan grasp trajectory (Cartesian move)\n        4. Close gripper\n        5. Plan lift trajectory (straight up)\n        \"\"\"\n        # ... MoveIt 2 planning code ...\n        pass\n"})}),"\n",(0,i.jsx)(n.h3,{id:"extension-d-multi-user-voice-advanced",children:"Extension D: Multi-User Voice (Advanced)"}),"\n",(0,i.jsxs)(n.p,{children:["Use ",(0,i.jsx)(n.strong,{children:"speaker diarisation"})," to track multiple users:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# pip install pyannote.audio\nfrom pyannote.audio import Pipeline as DiarisationPipeline\n\n# Identify WHO is speaking before routing the command\n# \u2192 "User A says go to kitchen" vs "User B says stop"\n# \u2192 robot prioritises based on operator vs observer role\n'})}),"\n",(0,i.jsx)(n.h3,{id:"extension-e-continuous-learning-research-level",children:"Extension E: Continuous Learning (Research Level)"}),"\n",(0,i.jsx)(n.p,{children:"Log every instruction, plan, and success/failure. Fine-tune the LLM prompt or a smaller model on this data to improve performance on your specific environment and users over time."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"410-project-deliverables-and-assessment",children:"4.10 Project Deliverables and Assessment"}),"\n",(0,i.jsx)(n.h3,{id:"required-deliverables",children:"Required Deliverables"}),"\n",(0,i.jsx)(n.p,{children:"For a university course submission, your project should include:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Working System Demonstration (40%)"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Record a 5-minute video of the robot executing at least 5 different voice commands successfully"}),"\n",(0,i.jsx)(n.li,{children:"Include at least one recovery scenario (robot gets stuck, replans)"}),"\n",(0,i.jsx)(n.li,{children:"Include at least one navigation task and one manipulation task"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Technical Report (40%)"})}),"\n",(0,i.jsx)(n.p,{children:"The report should cover:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"System architecture diagram (your version of the diagram in Section 4.1)"}),"\n",(0,i.jsx)(n.li,{children:"Design decisions and alternatives considered"}),"\n",(0,i.jsx)(n.li,{children:"Performance measurements:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Metric                        Target      Your Result\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nVoice transcription latency    < 3s        ___s\nLLM planning latency           < 8s        ___s\nNavigation success rate        > 80%       ___%\nTask completion rate           > 60%       ___%\nEnd-to-end latency (cmd\u2192move)  < 15s       ___s\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Three specific failures observed and their root causes"}),"\n",(0,i.jsx)(n.li,{children:"Limitations of your implementation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. Code Repository (20%)"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Clean, documented Python code"}),"\n",(0,i.jsx)(n.li,{children:"All nodes have docstrings"}),"\n",(0,i.jsx)(n.li,{children:"README with exact setup and run instructions"}),"\n",(0,i.jsxs)(n.li,{children:["At least 5 test cases in ",(0,i.jsx)(n.code,{children:"test/"})]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"assessment-rubric",children:"Assessment Rubric"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Category"}),(0,i.jsx)(n.th,{children:"Weight"}),(0,i.jsx)(n.th,{children:"Excellent"}),(0,i.jsx)(n.th,{children:"Satisfactory"}),(0,i.jsx)(n.th,{children:"Needs Work"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Voice pipeline works"}),(0,i.jsx)(n.td,{children:"15%"}),(0,i.jsx)(n.td,{children:"\u22642s latency, wake word"}),(0,i.jsx)(n.td,{children:"Works, no wake word"}),(0,i.jsx)(n.td,{children:"Requires manual injection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"LLM planning correct"}),(0,i.jsx)(n.td,{children:"20%"}),(0,i.jsx)(n.td,{children:"Handles 5+ task types"}),(0,i.jsx)(n.td,{children:"Handles 3 task types"}),(0,i.jsx)(n.td,{children:"1-2 task types only"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Nav2 navigation"}),(0,i.jsx)(n.td,{children:"20%"}),(0,i.jsx)(n.td,{children:"Reliable, with recovery"}),(0,i.jsx)(n.td,{children:"Mostly reaches goals"}),(0,i.jsx)(n.td,{children:"Frequent failures"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Integration"}),(0,i.jsx)(n.td,{children:"25%"}),(0,i.jsx)(n.td,{children:"Seamless end-to-end"}),(0,i.jsx)(n.td,{children:"Manual restarts needed"}),(0,i.jsx)(n.td,{children:"Parts don't connect"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Code quality"}),(0,i.jsx)(n.td,{children:"10%"}),(0,i.jsx)(n.td,{children:"Documented, tested"}),(0,i.jsx)(n.td,{children:"Runs but messy"}),(0,i.jsx)(n.td,{children:"Cannot reproduce"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Report quality"}),(0,i.jsx)(n.td,{children:"10%"}),(0,i.jsx)(n.td,{children:"Quantified, honest"}),(0,i.jsx)(n.td,{children:"Descriptive"}),(0,i.jsx)(n.td,{children:"Incomplete"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"411-module-4-and-textbook-summary",children:"4.11 Module 4 and Textbook Summary"}),"\n",(0,i.jsx)(n.p,{children:"You have completed the final module of this textbook. Let us review the arc of the entire course:"}),"\n",(0,i.jsx)(n.h3,{id:"module-1-the-robotic-nervous-system-ros-2",children:"Module 1: The Robotic Nervous System (ROS 2)"}),"\n",(0,i.jsx)(n.p,{children:"You learned the middleware that connects all robot software components \u2014 nodes, topics, services, and actions \u2014 and built your first AI-to-robot bridge, connecting a Python LLM agent to a robot executor via structured JSON commands."}),"\n",(0,i.jsx)(n.h3,{id:"module-2-simulation",children:"Module 2: Simulation"}),"\n",(0,i.jsx)(n.p,{children:"You discovered how to develop and test robot software safely in simulation before touching expensive hardware, using tools like Gazebo and PyBullet alongside the ROS 2 bridge."}),"\n",(0,i.jsx)(n.h3,{id:"module-3-the-ai-robot-brain-nvidia-isaac",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac)"}),"\n",(0,i.jsx)(n.p,{children:"You mastered the production-grade toolchain: Isaac Sim's photorealistic rendering, Replicator's synthetic data generation, cuVSLAM's hardware-accelerated localisation, and Nav2's autonomous navigation stack \u2014 all deployed on Jetson Orin for real-world performance."}),"\n",(0,i.jsx)(n.h3,{id:"module-4-vision-language-action",children:"Module 4: Vision-Language-Action"}),"\n",(0,i.jsx)(n.p,{children:"You studied VLA models from first principles (RT-2, OpenVLA, \u03c00), built a complete speech-to-action pipeline with Whisper, implemented a cognitive planning system with Claude, and assembled everything into a working autonomous humanoid."}),"\n",(0,i.jsx)(n.h3,{id:"the-bigger-picture",children:"The Bigger Picture"}),"\n",(0,i.jsx)(n.p,{children:"The system you built in this capstone is a simplified but structurally complete version of what commercial humanoid robot companies are building today. The key differences are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scale"}),": commercial systems use larger models, more powerful hardware, and more training data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reliability"}),": commercial systems have much more extensive safety systems, testing, and validation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation"}),": real dexterous manipulation requires years of additional engineering beyond Nav2"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["But the ",(0,i.jsx)(n.strong,{children:"architecture"})," is the same. Voice \u2192 understanding \u2192 planning \u2192 navigation \u2192 manipulation \u2192 feedback. You now have the vocabulary, the tools, and the working code to continue building on this foundation."]}),"\n",(0,i.jsx)(n.p,{children:"The robots of 2030 will be built by engineers who understand \u2014 at the code level \u2014 how language models, sensor fusion, path planning, and physical control integrate into a unified system. You are now one of those engineers."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The master launch file uses ",(0,i.jsx)(n.code,{children:"TimerAction(period=5.0)"})," for the sensing layer and ",(0,i.jsx)(n.code,{children:"TimerAction(period=10.0)"})," for the cognition layer. Why are these delays necessary? What failure mode would occur if all nodes were started simultaneously?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"During integration testing, you find that the LLM planner receives voice commands correctly but the Nav2 goal is never reached. Walk through the verification ladder (Section 4.6) and describe exactly which diagnostic commands you would run at each rung, and what output would confirm or eliminate each rung as the source of the failure."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:'The assessment rubric weights "integration" at 25% \u2014 the highest single weight. Justify this weighting from a software engineering perspective. Why is the integration of existing components often harder than building them individually?'}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Extension C requires replacing placeholder manipulation handlers with real MoveIt 2 calls. Write the precondition check that must pass before the ",(0,i.jsx)(n.code,{children:"pick_object"})," action is sent to MoveIt 2, including: gripper state, object pose confidence threshold, reachability check against the robot's arm reach specification, and object weight estimation."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"You have built a system that converts voice commands to robot actions via an LLM. A safety engineer reviewing the system identifies two concerns: (a) the LLM could plan an action that is kinematically infeasible, and (b) the TTS node confirming the plan adds latency but also creates an opportunity for human intervention. For each concern, describe a concrete technical mitigation you would implement in the codebase."}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);