"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[938],{1922(e,n,t){t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"module1-ros2/chapter4","title":"Chapter 4: Bridging Python AI Agents to ROS 2","description":"Learning Objectives","source":"@site/docs/module1-ros2/chapter4.mdx","sourceDirName":"module1-ros2","slug":"/module1-ros2/chapter4","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module1-ros2/chapter4","draft":false,"unlisted":false,"editUrl":"https://github.com/samreensaif/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/tree/main/physical-ai-textbook/docs/module1-ros2/chapter4.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Chapter 4: Bridging Python AI Agents to ROS 2","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Building ROS 2 Packages","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module1-ros2/chapter3"},"next":{"title":"Simulation Basics","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module2-simulation/chapter1"}}');var i=t(4848),r=t(8453),o=t(7132),a=t(20);const l={title:"Chapter 4: Bridging Python AI Agents to ROS 2",sidebar_position:4},c="Chapter 4: Bridging Python AI Agents to ROS 2",d={},h=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"4.1 The Core Challenge: Two Different Worlds",id:"41-the-core-challenge-two-different-worlds",level:2},{value:"4.2 URDF: Describing the Robot Body",id:"42-urdf-describing-the-robot-body",level:2},{value:"Core URDF Concepts",id:"core-urdf-concepts",level:3},{value:"A Minimal Humanoid URDF",id:"a-minimal-humanoid-urdf",level:3},{value:"Joint Types",id:"joint-types",level:3},{value:"Viewing URDF in RViz2",id:"viewing-urdf-in-rviz2",level:3},{value:"The Robot State Publisher",id:"the-robot-state-publisher",level:3},{value:"4.3 Architecture Patterns for AI-Robot Integration",id:"43-architecture-patterns-for-ai-robot-integration",level:2},{value:"Pattern A: The Reactive Agent (Topic-Based)",id:"pattern-a-the-reactive-agent-topic-based",level:3},{value:"Pattern B: The Goal-Directed Agent (Action-Based)",id:"pattern-b-the-goal-directed-agent-action-based",level:3},{value:"Pattern C: The Hybrid Agent (Hierarchical)",id:"pattern-c-the-hybrid-agent-hierarchical",level:3},{value:"4.4 Building an LLM-Based Robot Controller",id:"44-building-an-llm-based-robot-controller",level:2},{value:"The Structured Command Interface",id:"the-structured-command-interface",level:3},{value:"Node 1: The LLM Agent Node",id:"node-1-the-llm-agent-node",level:3},{value:"Node 2: The Robot Executor Node",id:"node-2-the-robot-executor-node",level:3},{value:"4.5 Testing the System End-to-End",id:"45-testing-the-system-end-to-end",level:2},{value:"4.6 Connecting a Real Vision-Language-Action Model",id:"46-connecting-a-real-vision-language-action-model",level:2},{value:"Using Vision Input",id:"using-vision-input",level:3},{value:"4.7 Safety Design for AI-Controlled Robots",id:"47-safety-design-for-ai-controlled-robots",level:2},{value:"Safety Layer Architecture",id:"safety-layer-architecture",level:3},{value:"Implementing a Velocity Safety Filter",id:"implementing-a-velocity-safety-filter",level:3},{value:"The Watchdog Pattern",id:"the-watchdog-pattern",level:3},{value:"4.8 Complete Launch File for the AI Robot System",id:"48-complete-launch-file-for-the-ai-robot-system",level:2},{value:"4.9 Design Patterns and Best Practices",id:"49-design-patterns-and-best-practices",level:2},{value:"Principle 1: Separate Reasoning from Control",id:"principle-1-separate-reasoning-from-control",level:3},{value:"Principle 2: Structured Output Over Free-Form Text",id:"principle-2-structured-output-over-free-form-text",level:3},{value:"Principle 3: The Stopwatch Rule",id:"principle-3-the-stopwatch-rule",level:3},{value:"Principle 4: Log Everything",id:"principle-4-log-everything",level:3},{value:"Principle 5: Test in Simulation First",id:"principle-5-test-in-simulation-first",level:3},{value:"4.10 Chapter Summary",id:"410-chapter-summary",level:2},{value:"Module Summary",id:"module-summary",level:2},{value:"Review Questions",id:"review-questions",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.A,{}),"\n",(0,i.jsx)(a.A,{}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-4-bridging-python-ai-agents-to-ros-2",children:"Chapter 4: Bridging Python AI Agents to ROS 2"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design the software architecture that connects an AI reasoning agent to a physical or simulated robot via ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Build a ROS 2 node that wraps an AI model (LLM or vision model) and translates its outputs into robot commands"}),"\n",(0,i.jsx)(n.li,{children:"Understand the URDF robot description format and read a simplified humanoid URDF"}),"\n",(0,i.jsx)(n.li,{children:"Implement an action-based command interface suitable for high-level AI control"}),"\n",(0,i.jsx)(n.li,{children:"Apply safety patterns \u2014 watchdogs, velocity limits, state machines \u2014 to prevent runaway AI control loops"}),"\n",(0,i.jsx)(n.li,{children:"Reason about the latency and timing constraints of AI-in-the-loop robot control"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"41-the-core-challenge-two-different-worlds",children:"4.1 The Core Challenge: Two Different Worlds"}),"\n",(0,i.jsx)(n.p,{children:"An AI agent and a robot live in fundamentally different time domains:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"AI Agent World                    Robot World\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nOperates at 1\u201310 Hz               Operates at 100\u20131000 Hz\nHandles natural language          Handles sensor streams\nThinks in high-level goals        Thinks in joint torques\nHas uncertain, slow inference     Has hard real-time deadlines\nPython / CUDA / NumPy             C++, ROS 2, safety systems\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The job of the ",(0,i.jsx)(n.strong,{children:"AI-ROS 2 bridge"})," is to translate between these two worlds gracefully:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        AI Agent Layer                        \u2502\n\u2502   (LLM, Vision Model, Planning Algorithm, Reward Function)   \u2502\n\u2502                    Python / PyTorch / Numpy                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502  high-level commands (JSON, natural language,\n                    \u2502  goal poses, skill names)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   AI-ROS 2 Bridge Node                       \u2502\n\u2502   - Translates AI intent \u2192 ROS 2 messages                   \u2502\n\u2502   - Buffers and rate-limits commands                         \u2502\n\u2502   - Monitors safety constraints                              \u2502\n\u2502   - Reports robot state back to the AI                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502  ROS 2 topics, services, actions\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Robot Control Stack                       \u2502\n\u2502   (nav2, MoveIt2, hardware drivers, joint controllers)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"42-urdf-describing-the-robot-body",children:"4.2 URDF: Describing the Robot Body"}),"\n",(0,i.jsxs)(n.p,{children:["Before commanding a robot, you need to know what it looks like. The ",(0,i.jsx)(n.strong,{children:"Unified Robot Description Format (URDF)"})," is an XML-based language that describes a robot's physical structure."]}),"\n",(0,i.jsx)(n.h3,{id:"core-urdf-concepts",children:"Core URDF Concepts"}),"\n",(0,i.jsxs)(n.p,{children:["A URDF consists of ",(0,i.jsx)(n.strong,{children:"links"})," (rigid bodies \u2014 limbs, torso, head) and ",(0,i.jsx)(n.strong,{children:"joints"})," (the connections between links that define allowable motion)."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"link: right_hand\njoint: right_wrist  (connects right_forearm \u2192 right_hand)\nlink: right_forearm\njoint: right_elbow  (connects right_upper_arm \u2192 right_forearm)\nlink: right_upper_arm\njoint: right_shoulder  (connects torso \u2192 right_upper_arm)\nlink: torso\n   \u2191 root link (base_link)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"a-minimal-humanoid-urdf",children:"A Minimal Humanoid URDF"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="simple_humanoid">\n\n  \x3c!-- \u2550\u2550\u2550\u2550\u2550\u2550\u2550 BASE (Torso) \u2550\u2550\u2550\u2550\u2550\u2550\u2550 --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.3 0.2 0.5"/>  \x3c!-- width depth height in metres --\x3e\n      </geometry>\n      <material name="gray">\n        <color rgba="0.5 0.5 0.5 1.0"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.3 0.2 0.5"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="20.0"/>  \x3c!-- kg --\x3e\n      <inertia ixx="0.5" ixy="0" ixz="0" iyy="0.5" iyz="0" izz="0.3"/>\n    </inertial>\n  </link>\n\n  \x3c!-- \u2550\u2550\u2550\u2550\u2550\u2550\u2550 HEAD \u2550\u2550\u2550\u2550\u2550\u2550\u2550 --\x3e\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.12"/>\n      </geometry>\n    </visual>\n    <inertial>\n      <mass value="3.0"/>\n      <inertia ixx="0.02" ixy="0" ixz="0" iyy="0.02" iyz="0" izz="0.02"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Neck joint: connects torso to head --\x3e\n  <joint name="neck_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="head"/>\n    \x3c!-- Origin of joint relative to parent link --\x3e\n    <origin xyz="0.0 0.0 0.37" rpy="0 0 0"/>\n    \x3c!-- Rotation axis (z = yaw, y = pitch, x = roll) --\x3e\n    <axis xyz="0 0 1"/>\n    \x3c!-- Joint limits: lower=-1.57 to upper=1.57 radians (\xb190\xb0) --\x3e\n    <limit lower="-1.57" upper="1.57" effort="10.0" velocity="1.0"/>\n    \x3c!-- Dynamics: damping and friction --\x3e\n    <dynamics damping="0.1" friction="0.01"/>\n  </joint>\n\n  \x3c!-- \u2550\u2550\u2550\u2550\u2550\u2550\u2550 LEFT ARM \u2550\u2550\u2550\u2550\u2550\u2550\u2550 --\x3e\n  <link name="left_upper_arm">\n    <visual>\n      <geometry>\n        <cylinder radius="0.04" length="0.30"/>\n      </geometry>\n    </visual>\n    <inertial>\n      <mass value="2.5"/>\n      <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.005"/>\n    </inertial>\n  </link>\n\n  <joint name="left_shoulder_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="left_upper_arm"/>\n    <origin xyz="0.20 0.0 0.15" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>  \x3c!-- Pitches forward/backward --\x3e\n    <limit lower="-3.14" upper="3.14" effort="50.0" velocity="2.0"/>\n    <dynamics damping="0.3" friction="0.02"/>\n  </joint>\n\n  <link name="left_forearm">\n    <visual>\n      <geometry>\n        <cylinder radius="0.035" length="0.28"/>\n      </geometry>\n    </visual>\n    <inertial>\n      <mass value="1.5"/>\n      <inertia ixx="0.005" ixy="0" ixz="0" iyy="0.005" iyz="0" izz="0.003"/>\n    </inertial>\n  </link>\n\n  <joint name="left_elbow_joint" type="revolute">\n    <parent link="left_upper_arm"/>\n    <child link="left_forearm"/>\n    <origin xyz="0.0 0.0 -0.30" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="0.0" upper="2.617" effort="30.0" velocity="2.0"/>\n    <dynamics damping="0.2" friction="0.01"/>\n  </joint>\n\n  <link name="left_hand">\n    <visual>\n      <geometry>\n        <box size="0.08 0.12 0.04"/>\n      </geometry>\n    </visual>\n    <inertial>\n      <mass value="0.5"/>\n      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  <joint name="left_wrist_joint" type="revolute">\n    <parent link="left_forearm"/>\n    <child link="left_hand"/>\n    <origin xyz="0.0 0.0 -0.28" rpy="0 0 0"/>\n    <axis xyz="1 0 0"/>\n    <limit lower="-1.57" upper="1.57" effort="10.0" velocity="2.0"/>\n    <dynamics damping="0.1" friction="0.005"/>\n  </joint>\n\n</robot>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"joint-types",children:"Joint Types"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Description"}),(0,i.jsx)(n.th,{children:"Degrees of Freedom"}),(0,i.jsx)(n.th,{children:"Example"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"fixed"})}),(0,i.jsx)(n.td,{children:"No movement"}),(0,i.jsx)(n.td,{children:"0"}),(0,i.jsx)(n.td,{children:"Sensor mounted on chassis"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"revolute"})}),(0,i.jsx)(n.td,{children:"Rotation within limits"}),(0,i.jsx)(n.td,{children:"1"}),(0,i.jsx)(n.td,{children:"Elbow, knee"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"continuous"})}),(0,i.jsx)(n.td,{children:"Unlimited rotation"}),(0,i.jsx)(n.td,{children:"1"}),(0,i.jsx)(n.td,{children:"Wheel"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"prismatic"})}),(0,i.jsx)(n.td,{children:"Linear motion within limits"}),(0,i.jsx)(n.td,{children:"1"}),(0,i.jsx)(n.td,{children:"Gripper finger"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"floating"})}),(0,i.jsx)(n.td,{children:"6 DOF free motion"}),(0,i.jsx)(n.td,{children:"6"}),(0,i.jsx)(n.td,{children:"Floating base"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"viewing-urdf-in-rviz2",children:"Viewing URDF in RViz2"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Publish the URDF and launch RViz2 to visualise the robot\nros2 launch urdf_tutorial display.launch.py \\\n    model:=$(pwd)/simple_humanoid.urdf\n"})}),"\n",(0,i.jsx)(n.h3,{id:"the-robot-state-publisher",children:"The Robot State Publisher"}),"\n",(0,i.jsxs)(n.p,{children:["In a running system, a node called ",(0,i.jsx)(n.code,{children:"robot_state_publisher"})," reads the URDF and publishes the transforms between links as ",(0,i.jsx)(n.code,{children:"tf2"})," transforms, based on the current joint states:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'ros2 run robot_state_publisher robot_state_publisher \\\n    --ros-args -p robot_description:="$(cat simple_humanoid.urdf)"\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"43-architecture-patterns-for-ai-robot-integration",children:"4.3 Architecture Patterns for AI-Robot Integration"}),"\n",(0,i.jsx)(n.p,{children:"There are three main architectural patterns for connecting AI to ROS 2, each with different trade-offs."}),"\n",(0,i.jsx)(n.h3,{id:"pattern-a-the-reactive-agent-topic-based",children:"Pattern A: The Reactive Agent (Topic-Based)"}),"\n",(0,i.jsx)(n.p,{children:"The AI continuously reads sensor topics and publishes command topics. This is the simplest pattern and works well for high-frequency, reactive control (e.g., visual servoing)."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Sensor Topics \u2192 AI Node \u2192 Command Topics \u2192 Robot\n(high frequency, low latency, no feedback)\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Best for"}),": Continuous, reactive tasks. Following a ball. Maintaining balance. Reactive collision avoidance."]}),"\n",(0,i.jsx)(n.h3,{id:"pattern-b-the-goal-directed-agent-action-based",children:"Pattern B: The Goal-Directed Agent (Action-Based)"}),"\n",(0,i.jsx)(n.p,{children:"The AI sets high-level goals via ROS 2 actions and waits for feedback and results. The robot's own controllers handle low-level execution."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"AI Agent \u2192 Action Client \u2192 ROS 2 Action Server \u2192 Robot\n                         \u2190 Feedback (progress updates) \u2190\n                         \u2190 Result (success/failure)    \u2190\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Best for"}),': Task-level planning. "Pick up the red cup." "Navigate to room 5." This is the pattern used by navigation (Nav2) and manipulation (MoveIt 2) stacks.']}),"\n",(0,i.jsx)(n.h3,{id:"pattern-c-the-hybrid-agent-hierarchical",children:"Pattern C: The Hybrid Agent (Hierarchical)"}),"\n",(0,i.jsx)(n.p,{children:"The AI operates at multiple time scales simultaneously. A slow LLM generates high-level plans; a fast reactive controller executes them."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"LLM (1 Hz)  \u2192 Task Planner \u2192 Action Goals\n                                    \u2193\nPerception (30 Hz) \u2192 Skill Controller \u2192 Velocity Commands\n                                    \u2193\n                               Hardware (1000 Hz)\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Best for"}),": General-purpose humanoid robots that must reason and act simultaneously. This is the architecture used in systems like RT-2, OpenVLA, and similar VLA (Vision-Language-Action) models."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"44-building-an-llm-based-robot-controller",children:"4.4 Building an LLM-Based Robot Controller"}),"\n",(0,i.jsx)(n.p,{children:"Let's build Pattern B: an LLM agent that interprets text commands and translates them to robot motion via ROS 2 topics."}),"\n",(0,i.jsx)(n.p,{children:"The system has two nodes:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"llm_agent_node"})})," \u2014 wraps an LLM, receives text commands, outputs structured goals"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"robot_executor_node"})})," \u2014 receives structured goals from the LLM, executes them as velocity commands"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"the-structured-command-interface",children:"The Structured Command Interface"}),"\n",(0,i.jsx)(n.p,{children:"First, define the interface between the LLM and the executor. We use a ROS 2 topic carrying a JSON-structured command. This is far more robust than parsing raw LLM text in the executor:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# robot_command.py \u2014 shared data class\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport json\n\n@dataclass\nclass RobotCommand:\n    """\n    Structured command output from the AI agent.\n    The LLM is prompted to generate valid JSON matching this schema.\n    """\n    action: str                      # "move", "turn", "stop", "wave_hand"\n    parameters: dict = field(default_factory=dict)\n    confidence: float = 1.0          # LLM confidence [0.0, 1.0]\n    explanation: str = ""            # LLM reasoning (for logging)\n\n    def to_json(self) -> str:\n        return json.dumps({\n            \'action\': self.action,\n            \'parameters\': self.parameters,\n            \'confidence\': self.confidence,\n            \'explanation\': self.explanation,\n        })\n\n    @classmethod\n    def from_json(cls, json_str: str) -> \'RobotCommand\':\n        data = json.loads(json_str)\n        return cls(**data)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"node-1-the-llm-agent-node",children:"Node 1: The LLM Agent Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# llm_agent_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport re\n\n# In a real system, replace this with your actual LLM client:\n# from anthropic import Anthropic\n# from openai import OpenAI\n\nclass LLMAgentNode(Node):\n    \"\"\"\n    Wraps an LLM (e.g. Claude, GPT-4) and translates\n    natural-language commands into structured robot commands.\n    \"\"\"\n\n    SYSTEM_PROMPT = \"\"\"You are a robot controller AI.\nYou receive natural language instructions and output ONLY valid JSON\ndescribing the robot action to take.\n\nValid actions and their parameters:\n  - move:      {\"distance\": float (metres), \"speed\": float (m/s, max 0.5)}\n  - turn:      {\"angle\": float (radians, positive=left), \"speed\": float (rad/s, max 1.0)}\n  - stop:      {}\n  - wave_hand: {\"hand\": \"left\"|\"right\", \"repetitions\": int}\n  - nod:       {\"repetitions\": int}\n\nOutput format (JSON only, no prose):\n{\n  \"action\": \"<action_name>\",\n  \"parameters\": {<action params>},\n  \"confidence\": <0.0-1.0>,\n  \"explanation\": \"<brief reasoning>\"\n}\n\nExamples:\nUser: \"Walk forward 2 metres\"\n{\"action\": \"move\", \"parameters\": {\"distance\": 2.0, \"speed\": 0.3},\n \"confidence\": 0.98, \"explanation\": \"Direct forward movement command\"}\n\nUser: \"Turn left 90 degrees\"\n{\"action\": \"turn\", \"parameters\": {\"angle\": 1.5708, \"speed\": 0.5},\n \"confidence\": 0.95, \"explanation\": \"90 degrees = pi/2 radians, left = positive\"}\n\nUser: \"Stop immediately\"\n{\"action\": \"stop\", \"parameters\": {},\n \"confidence\": 1.0, \"explanation\": \"Emergency stop requested\"}\n\"\"\"\n\n    def __init__(self):\n        super().__init__('llm_agent')\n\n        # Publisher: sends structured commands to the executor\n        self.command_pub = self.create_publisher(\n            String, '/robot/llm_command', 10\n        )\n\n        # Subscriber: receives raw text commands (from a UI, voice, etc.)\n        self.instruction_sub = self.create_subscription(\n            String,\n            '/robot/instruction',\n            self.instruction_callback,\n            10\n        )\n\n        # Subscriber: receives robot state for context\n        self.state_sub = self.create_subscription(\n            String,\n            '/robot/state',\n            self.state_callback,\n            10\n        )\n\n        self.current_state = {}\n\n        self.get_logger().info('LLM Agent node started. Waiting for instructions...')\n\n    def state_callback(self, msg: String):\n        \"\"\"Update our model of the current robot state.\"\"\"\n        try:\n            self.current_state = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().warn('Received invalid state JSON.')\n\n    def instruction_callback(self, msg: String):\n        \"\"\"\n        Called when a new text instruction arrives.\n        Queries the LLM and publishes the structured command.\n        \"\"\"\n        instruction = msg.data\n        self.get_logger().info(f'Received instruction: \"{instruction}\"')\n\n        try:\n            command_json = self.query_llm(instruction)\n            self.validate_and_publish(command_json)\n        except Exception as e:\n            self.get_logger().error(f'LLM query failed: {e}')\n            # Publish a stop command as a safety fallback\n            stop_cmd = json.dumps({\n                'action': 'stop',\n                'parameters': {},\n                'confidence': 1.0,\n                'explanation': 'LLM error fallback'\n            })\n            out_msg = String()\n            out_msg.data = stop_cmd\n            self.command_pub.publish(out_msg)\n\n    def query_llm(self, instruction: str) -> str:\n        \"\"\"\n        Sends the instruction to the LLM and returns the JSON response.\n\n        Replace this stub with a real API call:\n\n        client = Anthropic()\n        message = client.messages.create(\n            model=\"claude-opus-4-6\",\n            max_tokens=256,\n            system=self.SYSTEM_PROMPT,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Robot state: {json.dumps(self.current_state)}\\n\"\n                           f\"Instruction: {instruction}\"\n            }]\n        )\n        return message.content[0].text\n        \"\"\"\n        # --- STUB for illustration (deterministic fake LLM) ---\n        instruction_lower = instruction.lower()\n\n        if 'forward' in instruction_lower or 'walk' in instruction_lower:\n            dist = self._extract_number(instruction, default=1.0)\n            return json.dumps({\n                'action': 'move',\n                'parameters': {'distance': dist, 'speed': 0.3},\n                'confidence': 0.92,\n                'explanation': f'Move forward {dist}m'\n            })\n        elif 'turn left' in instruction_lower:\n            angle = self._extract_number(instruction, default=1.5708)\n            return json.dumps({\n                'action': 'turn',\n                'parameters': {'angle': angle, 'speed': 0.5},\n                'confidence': 0.90,\n                'explanation': 'Turn left'\n            })\n        elif 'turn right' in instruction_lower:\n            angle = self._extract_number(instruction, default=1.5708)\n            return json.dumps({\n                'action': 'turn',\n                'parameters': {'angle': -angle, 'speed': 0.5},\n                'confidence': 0.90,\n                'explanation': 'Turn right (negative angle)'\n            })\n        elif 'stop' in instruction_lower:\n            return json.dumps({\n                'action': 'stop',\n                'parameters': {},\n                'confidence': 1.0,\n                'explanation': 'Stop command'\n            })\n        elif 'wave' in instruction_lower:\n            hand = 'left' if 'left' in instruction_lower else 'right'\n            return json.dumps({\n                'action': 'wave_hand',\n                'parameters': {'hand': hand, 'repetitions': 3},\n                'confidence': 0.85,\n                'explanation': f'Wave {hand} hand'\n            })\n        else:\n            return json.dumps({\n                'action': 'stop',\n                'parameters': {},\n                'confidence': 0.3,\n                'explanation': 'Unclear instruction, defaulting to stop'\n            })\n\n    def _extract_number(self, text: str, default: float) -> float:\n        \"\"\"Extract the first number found in a string.\"\"\"\n        numbers = re.findall(r'\\d+\\.?\\d*', text)\n        return float(numbers[0]) if numbers else default\n\n    def validate_and_publish(self, command_json: str):\n        \"\"\"Validate the LLM output before publishing.\"\"\"\n        try:\n            cmd = json.loads(command_json)\n        except json.JSONDecodeError as e:\n            raise ValueError(f'LLM returned invalid JSON: {e}')\n\n        # Safety check: reject commands with low confidence\n        if cmd.get('confidence', 0) < 0.5:\n            self.get_logger().warn(\n                f'Low confidence command ({cmd[\"confidence\"]:.2f}), '\n                f'sending stop instead.'\n            )\n            cmd = {'action': 'stop', 'parameters': {},\n                   'confidence': 1.0, 'explanation': 'Low confidence fallback'}\n\n        # Safety check: clip any speed parameters to safe limits\n        if 'parameters' in cmd:\n            if 'speed' in cmd['parameters']:\n                cmd['parameters']['speed'] = min(\n                    cmd['parameters']['speed'], 0.5  # Hard limit: 0.5 m/s\n                )\n\n        out_msg = String()\n        out_msg.data = json.dumps(cmd)\n        self.command_pub.publish(out_msg)\n        self.get_logger().info(\n            f'Published command: {cmd[\"action\"]} '\n            f'(confidence: {cmd.get(\"confidence\", \"?\"):.2f})'\n        )\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMAgentNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"node-2-the-robot-executor-node",children:"Node 2: The Robot Executor Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# robot_executor_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport json\nimport math\n\nclass RobotExecutorNode(Node):\n    \"\"\"\n    Receives structured commands from the LLM agent and\n    executes them as ROS 2 velocity commands.\n\n    Implements a simple state machine:\n    IDLE \u2192 EXECUTING \u2192 IDLE\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('robot_executor')\n\n        # State\n        self.state = 'IDLE'\n        self.current_command = None\n        self.ticks_remaining = 0\n        self.LOOP_HZ = 10.0\n\n        # Publisher: velocity commands to the robot base\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Publisher: state feedback back to the AI agent\n        self.state_pub = self.create_publisher(String, '/robot/state', 10)\n\n        # Subscriber: receives commands from the LLM agent\n        self.command_sub = self.create_subscription(\n            String,\n            '/robot/llm_command',\n            self.command_callback,\n            10\n        )\n\n        # Control loop at 10 Hz\n        self.timer = self.create_timer(1.0 / self.LOOP_HZ, self.control_loop)\n\n        # Watchdog: if no command for 5 seconds, stop the robot\n        self.watchdog_timer = self.create_timer(5.0, self.watchdog_callback)\n        self.last_command_time = self.get_clock().now()\n\n        self.get_logger().info('Robot executor node started.')\n\n    def command_callback(self, msg: String):\n        \"\"\"Receive and queue a new command from the LLM agent.\"\"\"\n        try:\n            cmd = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error('Received invalid command JSON.')\n            return\n\n        action = cmd.get('action', 'stop')\n        params = cmd.get('parameters', {})\n\n        self.get_logger().info(f'Executing action: {action} params: {params}')\n\n        self.current_command = cmd\n        self.last_command_time = self.get_clock().now()\n\n        if action == 'stop':\n            self._enter_idle()\n        elif action == 'move':\n            distance = params.get('distance', 0.0)\n            speed = min(params.get('speed', 0.3), 0.5)\n            duration_ticks = int((distance / speed) * self.LOOP_HZ)\n            self._enter_executing(\n                linear_x=speed, angular_z=0.0,\n                ticks=duration_ticks\n            )\n        elif action == 'turn':\n            angle = params.get('angle', 0.0)    # radians\n            speed = min(abs(params.get('speed', 0.5)), 1.0)\n            direction = 1.0 if angle >= 0 else -1.0\n            duration_ticks = int((abs(angle) / speed) * self.LOOP_HZ)\n            self._enter_executing(\n                linear_x=0.0, angular_z=direction * speed,\n                ticks=duration_ticks\n            )\n        elif action == 'wave_hand':\n            # For arm actions, log and send a joint command (simplified here)\n            repetitions = params.get('repetitions', 3)\n            hand = params.get('hand', 'right')\n            self.get_logger().info(\n                f'Wave action: {hand} hand, {repetitions} times '\n                f'(joint commands omitted in this example)'\n            )\n            self._enter_idle()\n        else:\n            self.get_logger().warn(f'Unknown action: {action}. Stopping.')\n            self._enter_idle()\n\n    def _enter_idle(self):\n        self.state = 'IDLE'\n        self.ticks_remaining = 0\n        self._publish_zero_velocity()\n\n    def _enter_executing(self, linear_x: float, angular_z: float, ticks: int):\n        self.state = 'EXECUTING'\n        self.execute_linear_x = linear_x\n        self.execute_angular_z = angular_z\n        self.ticks_remaining = ticks\n\n    def control_loop(self):\n        \"\"\"Called at LOOP_HZ. Manages the execution state machine.\"\"\"\n        if self.state == 'EXECUTING':\n            if self.ticks_remaining > 0:\n                self._publish_velocity(\n                    self.execute_linear_x,\n                    self.execute_angular_z\n                )\n                self.ticks_remaining -= 1\n            else:\n                self.get_logger().info('Command complete. Returning to IDLE.')\n                self._enter_idle()\n                self._publish_state()\n\n    def watchdog_callback(self):\n        \"\"\"Stop the robot if no command has been received recently.\"\"\"\n        elapsed = (self.get_clock().now() - self.last_command_time).nanoseconds / 1e9\n        if elapsed > 5.0 and self.state == 'EXECUTING':\n            self.get_logger().warn(\n                f'Watchdog: no command for {elapsed:.1f}s. Emergency stop.'\n            )\n            self._enter_idle()\n\n    def _publish_velocity(self, linear_x: float, angular_z: float):\n        msg = Twist()\n        msg.linear.x = float(linear_x)\n        msg.angular.z = float(angular_z)\n        self.cmd_vel_pub.publish(msg)\n\n    def _publish_zero_velocity(self):\n        self._publish_velocity(0.0, 0.0)\n\n    def _publish_state(self):\n        \"\"\"Publish current robot state for the AI agent to observe.\"\"\"\n        state = {\n            'executor_state': self.state,\n            'ticks_remaining': self.ticks_remaining,\n        }\n        msg = String()\n        msg.data = json.dumps(state)\n        self.state_pub.publish(msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = RobotExecutorNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node._enter_idle()  # Safety: stop on Ctrl+C\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"45-testing-the-system-end-to-end",children:"4.5 Testing the System End-to-End"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Run the LLM agent\nros2 run my_robot_pkg llm_agent\n\n# Terminal 2: Run the executor\nros2 run my_robot_pkg robot_executor\n\n# Terminal 3: Monitor velocity commands\nros2 topic echo /cmd_vel\n\n# Terminal 4: Send instructions to the agent\nros2 topic pub /robot/instruction std_msgs/msg/String \\\n    \"data: 'Walk forward 2 metres'\" --once\n\nros2 topic pub /robot/instruction std_msgs/msg/String \\\n    \"data: 'Turn left 90 degrees'\" --once\n\nros2 topic pub /robot/instruction std_msgs/msg/String \\\n    \"data: 'Stop immediately'\" --once\n"})}),"\n",(0,i.jsx)(n.p,{children:"The computation graph now looks like:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"/robot/instruction  \u2500\u2500\u25b6  [llm_agent]  \u2500\u2500\u25b6  /robot/llm_command  \u2500\u2500\u25b6  [robot_executor]  \u2500\u2500\u25b6  /cmd_vel\n                               \u25b2                                              \u2502\n                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  /robot/state  \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"46-connecting-a-real-vision-language-action-model",children:"4.6 Connecting a Real Vision-Language-Action Model"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"llm_agent_node"})," above used a simple stub. Here is how to connect it to a real AI model using the Anthropic Python SDK:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Replace the query_llm() method with:\nfrom anthropic import Anthropic\n\nclass LLMAgentNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_agent\')\n        # Initialise the Anthropic client\n        # Reads ANTHROPIC_API_KEY from environment\n        self.anthropic_client = Anthropic()\n        self.conversation_history = []\n        # ... rest of __init__ ...\n\n    def query_llm(self, instruction: str) -> str:\n        """Query Claude with full conversation history for context."""\n\n        # Build context message including current robot state\n        user_content = (\n            f"Current robot state: {json.dumps(self.current_state)}\\n"\n            f"Instruction: {instruction}"\n        )\n\n        # Append to conversation history (multi-turn context)\n        self.conversation_history.append({\n            "role": "user",\n            "content": user_content\n        })\n\n        response = self.anthropic_client.messages.create(\n            model="claude-opus-4-6",\n            max_tokens=256,\n            system=self.SYSTEM_PROMPT,\n            messages=self.conversation_history\n        )\n\n        assistant_text = response.content[0].text\n\n        # Append assistant response to history\n        self.conversation_history.append({\n            "role": "assistant",\n            "content": assistant_text\n        })\n\n        # Keep conversation history to last 20 turns to avoid token overflow\n        if len(self.conversation_history) > 40:\n            self.conversation_history = self.conversation_history[-40:]\n\n        return assistant_text\n'})}),"\n",(0,i.jsx)(n.h3,{id:"using-vision-input",children:"Using Vision Input"}),"\n",(0,i.jsx)(n.p,{children:"To give the LLM visual context of what the robot sees:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import base64\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass VisionLLMAgentNode(Node):\n    def __init__(self):\n        super().__init__(\'vision_llm_agent\')\n        self.bridge = CvBridge()\n        self.latest_image_b64 = None\n        self.anthropic_client = Anthropic()\n\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 1\n        )\n        # ... other setup ...\n\n    def image_callback(self, msg: Image):\n        """Convert ROS Image to base64 JPEG for the vision LLM."""\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n        _, buffer = cv2.imencode(\'.jpg\', cv_image, [cv2.IMWRITE_JPEG_QUALITY, 80])\n        self.latest_image_b64 = base64.b64encode(buffer).decode(\'utf-8\')\n\n    def query_llm_with_vision(self, instruction: str) -> str:\n        """Send both image and instruction to the vision LLM."""\n        content = [{"type": "text", "text": f"Instruction: {instruction}"}]\n\n        if self.latest_image_b64:\n            content.insert(0, {\n                "type": "image",\n                "source": {\n                    "type": "base64",\n                    "media_type": "image/jpeg",\n                    "data": self.latest_image_b64,\n                },\n            })\n\n        response = self.anthropic_client.messages.create(\n            model="claude-opus-4-6",\n            max_tokens=256,\n            system=self.SYSTEM_PROMPT,\n            messages=[{"role": "user", "content": content}]\n        )\n\n        return response.content[0].text\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"47-safety-design-for-ai-controlled-robots",children:"4.7 Safety Design for AI-Controlled Robots"}),"\n",(0,i.jsx)(n.p,{children:"Connecting an AI model to a physical robot introduces failure modes that do not exist in pure software systems. A responsible engineer must design for these explicitly."}),"\n",(0,i.jsx)(n.h3,{id:"safety-layer-architecture",children:"Safety Layer Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"AI Output\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Safety Filter               \u2502\n\u2502  1. Schema validation (valid JSON?) \u2502\n\u2502  2. Confidence threshold check      \u2502\n\u2502  3. Velocity hard clipping          \u2502\n\u2502  4. Workspace boundary check        \u2502\n\u2502  5. Emergency stop override         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u25bc\n         Robot Controllers\n"})}),"\n",(0,i.jsx)(n.h3,{id:"implementing-a-velocity-safety-filter",children:"Implementing a Velocity Safety Filter"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SafetyFilter:\n    \"\"\"\n    Validates and clips robot commands before execution.\n    This class should be used as a standalone component,\n    not mixed into the AI agent logic.\n    \"\"\"\n\n    # Hard velocity limits \u2014 never exceed these regardless of AI output\n    MAX_LINEAR_SPEED = 0.5    # m/s\n    MAX_ANGULAR_SPEED = 1.0   # rad/s\n    MAX_DISTANCE = 5.0        # metres per command\n    MAX_ANGLE = 2 * 3.14159   # radians (one full rotation)\n\n    def validate(self, command: dict) -> tuple[bool, str, dict]:\n        \"\"\"\n        Returns: (is_safe, reason, sanitised_command)\n        \"\"\"\n        action = command.get('action', '')\n        params = command.get('parameters', {})\n\n        # Reject unknown actions\n        allowed_actions = {'move', 'turn', 'stop', 'wave_hand', 'nod'}\n        if action not in allowed_actions:\n            return False, f'Unknown action: {action}', {}\n\n        # Clip movement parameters\n        if action == 'move':\n            params['speed'] = min(\n                abs(params.get('speed', 0.3)),\n                self.MAX_LINEAR_SPEED\n            )\n            params['distance'] = min(\n                abs(params.get('distance', 0.0)),\n                self.MAX_DISTANCE\n            )\n\n        if action == 'turn':\n            params['speed'] = min(\n                abs(params.get('speed', 0.5)),\n                self.MAX_ANGULAR_SPEED\n            )\n            params['angle'] = max(\n                min(params.get('angle', 0.0), self.MAX_ANGLE),\n                -self.MAX_ANGLE\n            )\n\n        command['parameters'] = params\n        return True, 'OK', command\n"})}),"\n",(0,i.jsx)(n.h3,{id:"the-watchdog-pattern",children:"The Watchdog Pattern"}),"\n",(0,i.jsxs)(n.p,{children:["A ",(0,i.jsx)(n.strong,{children:"watchdog"}),' is a timer that triggers a safety action if it is not regularly "kicked" (reset). Use a watchdog whenever an AI is in control of a robot:']}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class WatchdogNode(Node):\n    \"\"\"\n    Independent safety node.\n    If no heartbeat is received for TIMEOUT_SECONDS,\n    publishes an emergency stop.\n    \"\"\"\n    TIMEOUT_SECONDS = 2.0\n\n    def __init__(self):\n        super().__init__('safety_watchdog')\n\n        self.last_heartbeat = self.get_clock().now()\n        self.estop_pub = self.create_publisher(Twist, '/cmd_vel', 1)\n\n        # Heartbeat subscription (LLM agent publishes this)\n        self.heartbeat_sub = self.create_subscription(\n            String, '/llm_agent/heartbeat', self.heartbeat_callback, 10\n        )\n\n        # Check every 0.5 seconds\n        self.check_timer = self.create_timer(0.5, self.check_watchdog)\n\n        self.get_logger().warn(\n            f'Safety watchdog active. Timeout: {self.TIMEOUT_SECONDS}s'\n        )\n\n    def heartbeat_callback(self, msg):\n        self.last_heartbeat = self.get_clock().now()\n\n    def check_watchdog(self):\n        elapsed = (\n            self.get_clock().now() - self.last_heartbeat\n        ).nanoseconds / 1e9\n\n        if elapsed > self.TIMEOUT_SECONDS:\n            self.get_logger().error(\n                f'WATCHDOG TIMEOUT ({elapsed:.1f}s). Publishing E-STOP.'\n            )\n            self.estop_pub.publish(Twist())  # Zero velocity\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"48-complete-launch-file-for-the-ai-robot-system",children:"4.8 Complete Launch File for the AI Robot System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/ai_robot.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    return LaunchDescription([\n\n        DeclareLaunchArgument('use_sim', default_value='true'),\n\n        # 1. Robot state publisher (reads URDF, publishes TF)\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            name='robot_state_publisher',\n            output='screen',\n            parameters=[{\n                'robot_description':\n                    open('simple_humanoid.urdf').read()\n            }],\n        ),\n\n        # 2. LLM Agent\n        Node(\n            package='my_robot_pkg',\n            executable='llm_agent',\n            name='llm_agent',\n            output='screen',\n        ),\n\n        # 3. Robot Executor\n        Node(\n            package='my_robot_pkg',\n            executable='robot_executor',\n            name='robot_executor',\n            output='screen',\n            parameters=[{\n                'max_linear_speed': 0.5,\n                'max_angular_speed': 1.0,\n            }],\n        ),\n\n        # 4. Safety Watchdog\n        Node(\n            package='my_robot_pkg',\n            executable='safety_watchdog',\n            name='safety_watchdog',\n            output='screen',\n        ),\n    ])\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"49-design-patterns-and-best-practices",children:"4.9 Design Patterns and Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"principle-1-separate-reasoning-from-control",children:"Principle 1: Separate Reasoning from Control"}),"\n",(0,i.jsx)(n.p,{children:"Never put LLM API calls inside a high-frequency control loop. The LLM operates at 1\u20132 Hz; the controller operates at 10\u2013100 Hz. They belong in different nodes with well-defined interfaces."}),"\n",(0,i.jsx)(n.h3,{id:"principle-2-structured-output-over-free-form-text",children:"Principle 2: Structured Output Over Free-Form Text"}),"\n",(0,i.jsx)(n.p,{children:"Always prompt the LLM to output structured JSON and parse it programmatically. Avoid parsing natural language robot commands with regex \u2014 it breaks under edge cases. Validate the schema strictly."}),"\n",(0,i.jsx)(n.h3,{id:"principle-3-the-stopwatch-rule",children:"Principle 3: The Stopwatch Rule"}),"\n",(0,i.jsx)(n.p,{children:"If the AI has not confirmed a decision within N milliseconds, stop the robot. A stopped robot is safe; a robot executing an outdated command is dangerous."}),"\n",(0,i.jsx)(n.h3,{id:"principle-4-log-everything",children:"Principle 4: Log Everything"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Log the full LLM prompt and response for every command\nself.get_logger().debug(f'PROMPT: {prompt}')\nself.get_logger().debug(f'RESPONSE: {response}')\nself.get_logger().info(f'COMMAND: {command}')\n"})}),"\n",(0,i.jsx)(n.p,{children:"Robot failures are often caused by unexpected AI outputs that occur rarely. Comprehensive logging allows post-hoc analysis."}),"\n",(0,i.jsx)(n.h3,{id:"principle-5-test-in-simulation-first",children:"Principle 5: Test in Simulation First"}),"\n",(0,i.jsx)(n.p,{children:"Always develop AI-robot systems in a simulator (Gazebo, Isaac Sim, PyBullet) before running on hardware. The simulator lets you:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Run at accelerated speed (10\xd7 faster than real-time)"}),"\n",(0,i.jsx)(n.li,{children:"Reset the environment after crashes"}),"\n",(0,i.jsx)(n.li,{children:"Test edge cases safely"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"410-chapter-summary",children:"4.10 Chapter Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter assembled all previous knowledge into a complete AI-robot integration:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"The AI-ROS 2 bridge"})," separates the AI reasoning layer (slow, Python, GPU) from the robot control layer (fast, C++, real-time) using well-defined topic and action interfaces."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"URDF"})," describes the robot's physical structure as a tree of links and joints. Every joint has a type, limits, and dynamics. The ",(0,i.jsx)(n.code,{children:"robot_state_publisher"})," broadcasts this as live TF transforms."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Three architectural patterns"}),": reactive (topic-based), goal-directed (action-based), and hierarchical (hybrid). Choose based on task requirements."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"The LLM agent node"})," wraps an LLM API call, maintains conversation history, validates output, and publishes structured JSON commands. Vision input is added by converting ROS ",(0,i.jsx)(n.code,{children:"Image"})," messages to base64 JPEGs."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Safety is not optional"}),": every AI-robot system needs velocity clipping, confidence thresholds, a watchdog timer, and comprehensive logging."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Develop in simulation first"}),". Only move to hardware after the AI-simulator loop is reliable."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"module-summary",children:"Module Summary"}),"\n",(0,i.jsx)(n.p,{children:"You have completed Module 1: The Robotic Nervous System. Across these four chapters, you have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understood why ROS 2 was built and how its layered architecture (DDS \u2192 rmw \u2192 rcl \u2192 rclpy) provides reliability, security, and real-time capability"}),"\n",(0,i.jsx)(n.li,{children:"Written publisher, subscriber, service server, and service client nodes in Python"}),"\n",(0,i.jsx)(n.li,{children:"Structured professional ROS 2 packages with proper manifests, entry points, custom messages, and launch files"}),"\n",(0,i.jsx)(n.li,{children:"Designed and implemented a complete AI-robot integration \u2014 an LLM agent that commands a humanoid robot via structured ROS 2 topics, with safety filtering and watchdog monitoring"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In the next module, you will take these skills into a physics simulator, connecting your AI agent to a virtual humanoid robot in Gazebo and Isaac Sim."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Explain the two-layer architecture (LLM Agent + Robot Executor) used in this chapter. Why is it better to keep these as separate ROS 2 nodes rather than combining them into one?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Examine the ",(0,i.jsx)(n.code,{children:"SYSTEM_PROMPT"})," in ",(0,i.jsx)(n.code,{children:"LLMAgentNode"}),". List three specific engineering decisions embedded in this prompt that affect the safety and reliability of the robot's behaviour."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["A colleague argues that you should simply give the LLM direct access to publish ",(0,i.jsx)(n.code,{children:"Twist"})," messages and let it control ",(0,i.jsx)(n.code,{children:"linear.x"})," and ",(0,i.jsx)(n.code,{children:"angular.z"})," values directly. What are the risks of this approach compared to the structured command interface described in this chapter?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Describe what would happen in the current system if: (a) the LLM API times out mid-operation; (b) the LLM outputs a ",(0,i.jsx)(n.code,{children:"move"})," command with ",(0,i.jsx)(n.code,{children:"speed: 99.0"}),"; (c) the ",(0,i.jsx)(n.code,{children:"robot_executor"})," node crashes. For each scenario, trace through the code and identify what safety mechanism (if any) handles it."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Write a new action type ",(0,i.jsx)(n.code,{children:'"pick_object"'})," for the ",(0,i.jsx)(n.code,{children:"RobotCommand"})," schema. Define its parameters, add it to the ",(0,i.jsx)(n.code,{children:"SYSTEM_PROMPT"})," example list, add handling code in ",(0,i.jsx)(n.code,{children:"robot_executor_node.py"}),"'s ",(0,i.jsx)(n.code,{children:"command_callback"}),", and add the corresponding entry to the ",(0,i.jsx)(n.code,{children:"SafetyFilter.validate()"})," method."]}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},20(e,n,t){t.d(n,{A:()=>p});var s=t(6540),i=t(8774);const r="container_dCV0",o="row_RaZg",a="personalizeBtn_aAG4",l="showOriginalBtn_xRLS",c="badge_sNej",d="personalizedContent_IU8L",h="loginPrompt_KXHJ",m="error_X2St";var u=t(4848);function p(){const[e,n]=(0,s.useState)(!1),[t,p]=(0,s.useState)(null),[g,f]=(0,s.useState)("idle"),[_,x]=(0,s.useState)(""),[b,y]=(0,s.useState)("");(0,s.useEffect)(()=>{n(!0);const e=localStorage.getItem("auth_user");if(e)try{const n=JSON.parse(e);p(n.profile||null)}catch{}},[]);const j=(0,s.useCallback)(async()=>{if(!t)return;f("loading");const e=document.querySelector("article"),n=e?e.innerText:document.body.innerText,s="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${s}/personalize`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({content:n,experience_level:t.programmingExp||"Beginner",has_gpu:"Yes"===t.hasGPU,ros_experience:t.rosExperience||"none",learning_style:t.learningStyle||"Reading"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const i=await e.json();x(i.personalized_content),f("done")}catch(i){y(i.message),f("error")}},[t]),v=(0,s.useCallback)(()=>{f("idle"),x(""),y("")},[]);return e?t?(0,u.jsxs)("div",{className:r,children:["idle"===g&&(0,u.jsx)("button",{className:a,onClick:j,children:"\u2728 Personalize for Me"}),"loading"===g&&(0,u.jsx)("button",{className:a,disabled:!0,children:"\u23f3 Personalizing\u2026"}),"error"===g&&(0,u.jsxs)("div",{className:o,children:[(0,u.jsx)("button",{className:a,onClick:j,children:"\u2728 Personalize for Me"}),(0,u.jsxs)("span",{className:m,children:["Failed: ",b]})]}),"done"===g&&(0,u.jsxs)("div",{children:[(0,u.jsxs)("div",{className:o,children:[(0,u.jsxs)("span",{className:c,children:["\u2728 Personalized for ",t.programmingExp," \xb7 ",t.learningStyle," learner"]}),(0,u.jsx)("button",{className:l,onClick:v,children:"\ud83d\udcc4 Show Original"})]}),(0,u.jsx)("div",{className:d,children:_})]})]}):(0,u.jsx)("div",{className:r,children:(0,u.jsxs)("span",{className:h,children:["\u2728"," ",(0,u.jsx)(i.A,{to:"/signin",children:"Sign in"})," ","to get content personalised to your experience level."]})}):null}},7132(e,n,t){t.d(n,{A:()=>h});var s=t(6540);const i="container_sRRF",r="row_Wfea",o="translateBtn_zwOG",a="showOriginalBtn__IG_",l="translatedContent_uGS7",c="error_H8Lp";var d=t(4848);function h(){const[e,n]=(0,s.useState)("idle"),[t,h]=(0,s.useState)(""),[m,u]=(0,s.useState)(""),p=(0,s.useCallback)(async()=>{n("loading");const e=document.querySelector("article"),t=e?e.innerText:document.body.innerText,s="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${s}/translate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:t,target_language:"urdu"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const i=await e.json();h(i.translated_text),n("translated")}catch(i){u(i.message),n("error")}},[]),g=(0,s.useCallback)(()=>{n("idle"),h(""),u("")},[]);return(0,d.jsxs)("div",{className:i,children:[("idle"===e||"error"===e)&&(0,d.jsxs)("div",{className:r,children:[(0,d.jsx)("button",{className:o,onClick:p,children:"\ud83c\udf10 Translate to Urdu"}),"error"===e&&(0,d.jsxs)("span",{className:c,children:["Translation failed: ",m]})]}),"loading"===e&&(0,d.jsx)("button",{className:o,disabled:!0,children:"\u23f3 Translating\u2026"}),"translated"===e&&(0,d.jsxs)("div",{children:[(0,d.jsx)("button",{className:a,onClick:g,children:"\ud83d\udcd6 Show Original English"}),(0,d.jsx)("div",{className:l,dir:"rtl",lang:"ur",children:t})]})]})}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);