"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[6067],{4867(e,n,t){t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module4-vla/chapter3","title":"Chapter 3: Cognitive Planning with LLMs","description":"Learning Objectives","source":"@site/docs/module4-vla/chapter3.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/chapter3","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module4-vla/chapter3","draft":false,"unlisted":false,"editUrl":"https://github.com/samreensaif/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/tree/main/physical-ai-textbook/docs/module4-vla/chapter3.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Chapter 3: Cognitive Planning with LLMs","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Voice to Action with Whisper","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module4-vla/chapter2"},"next":{"title":"Chapter 4: Capstone Project \u2014 The Autonomous Humanoid","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module4-vla/chapter4"}}');var a=t(4848),i=t(8453),o=t(7132),r=t(20);const l={title:"Chapter 3: Cognitive Planning with LLMs",sidebar_position:3},c="Chapter 3: Cognitive Planning with LLMs",p={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"3.1 Why LLMs for Robot Planning?",id:"31-why-llms-for-robot-planning",level:2},{value:"What LLMs Are Good At in Robotics",id:"what-llms-are-good-at-in-robotics",level:3},{value:"What LLMs Are Not Good At",id:"what-llms-are-not-good-at",level:3},{value:"3.2 The Planning Architecture",id:"32-the-planning-architecture",level:2},{value:"3.3 Prompt Engineering for Robotics",id:"33-prompt-engineering-for-robotics",level:2},{value:"The Robot Capability Manifest",id:"the-robot-capability-manifest",level:3},{value:"3.4 The LLM Planner Node",id:"34-the-llm-planner-node",level:2},{value:"3.5 Chain-of-Thought Prompting for Robots",id:"35-chain-of-thought-prompting-for-robots",level:2},{value:"3.6 Multi-Turn Planning with Conversation History",id:"36-multi-turn-planning-with-conversation-history",level:2},{value:"Conversation State Management",id:"conversation-state-management",level:3},{value:"3.7 Evaluating Plan Quality",id:"37-evaluating-plan-quality",level:2},{value:"Running the Evaluation Suite",id:"running-the-evaluation-suite",level:3},{value:"3.8 Practical Prompt Engineering Guidelines",id:"38-practical-prompt-engineering-guidelines",level:2},{value:"DO: Use Structured Output Schemas",id:"do-use-structured-output-schemas",level:3},{value:"DO: Enumerate Capabilities Explicitly",id:"do-enumerate-capabilities-explicitly",level:3},{value:"DO: Provide Worked Examples (Few-Shot)",id:"do-provide-worked-examples-few-shot",level:3},{value:"DO: Inject Real-Time State",id:"do-inject-real-time-state",level:3},{value:"DO NOT: Allow Free-Form Output",id:"do-not-allow-free-form-output",level:3},{value:"DO NOT: Trust Long Plans Blindly",id:"do-not-trust-long-plans-blindly",level:3},{value:"DO NOT: Skip Validation",id:"do-not-skip-validation",level:3},{value:"3.9 Chapter Summary",id:"39-chapter-summary",level:2},{value:"Review Questions",id:"review-questions",level:2}];function h(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(o.A,{}),"\n",(0,a.jsx)(r.A,{}),"\n",(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-3-cognitive-planning-with-llms",children:"Chapter 3: Cognitive Planning with LLMs"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Explain why LLMs are effective as high-level task planners for robots"}),"\n",(0,a.jsx)(n.li,{children:"Apply structured prompt engineering techniques to ground LLM reasoning in robot capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Implement a task decomposition pipeline that converts natural language instructions into ordered, executable ROS 2 action sequences"}),"\n",(0,a.jsx)(n.li,{children:"Handle LLM failures gracefully: hallucinated actions, infeasible plans, and JSON parsing errors"}),"\n",(0,a.jsx)(n.li,{children:"Use conversation history to give the LLM contextual awareness of robot state and task progress"}),"\n",(0,a.jsx)(n.li,{children:"Implement replanning: detecting task failure and asking the LLM to revise its plan"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate plan quality and measure planning latency"}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"31-why-llms-for-robot-planning",children:"3.1 Why LLMs for Robot Planning?"}),"\n",(0,a.jsx)(n.p,{children:"Classical robot task planning (STRIPS, PDDL, Behaviour Trees) requires an expert to manually enumerate every possible action, precondition, and effect. For a home robot that must handle the infinite variety of real-world domestic tasks, this is impractical."}),"\n",(0,a.jsxs)(n.p,{children:["LLMs offer a different approach: they have absorbed an enormous amount of procedural knowledge from text (how-to guides, recipes, instructions, documentation) and can apply this knowledge to novel situations through ",(0,a.jsx)(n.strong,{children:"in-context reasoning"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["Consider the instruction: ",(0,a.jsx)(n.em,{children:'"The dinner party starts in 20 minutes. Set the table for four people."'})]}),"\n",(0,a.jsx)(n.p,{children:"A classical planner requires every action to be pre-programmed with formal preconditions and effects. An LLM can reason: a dinner setting for four means four plates, four sets of cutlery, four glasses, possibly napkins, centred on the table \u2014 drawing on procedural knowledge from text without any explicit programming."}),"\n",(0,a.jsx)(n.h3,{id:"what-llms-are-good-at-in-robotics",children:"What LLMs Are Good At in Robotics"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Capability"}),(0,a.jsx)(n.th,{children:"Example"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Task decomposition"})}),(0,a.jsx)(n.td,{children:'"Set the table" \u2192 [find plates, carry plates, place plates, repeat for cutlery...]'})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Commonsense reasoning"})}),(0,a.jsx)(n.td,{children:'"The plant needs water" \u2192 check if watering can is full first'})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Spatial language"})}),(0,a.jsx)(n.td,{children:'"put it to the left of the blue vase" \u2192 resolve "left of" + "blue vase"'})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Error recovery"})}),(0,a.jsx)(n.td,{children:'"I dropped the cup" \u2192 replan around the failure'})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Preference inference"})}),(0,a.jsx)(n.td,{children:'"Make me a coffee" \u2192 hot, in a mug, probably milk and sugar'})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Ambiguity resolution"})}),(0,a.jsx)(n.td,{children:'"the thing on the counter" \u2192 identify most salient object'})]})]})]}),"\n",(0,a.jsx)(n.h3,{id:"what-llms-are-not-good-at",children:"What LLMs Are Not Good At"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Limitation"}),(0,a.jsx)(n.th,{children:"Mitigation"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Geometric reasoning"})}),(0,a.jsx)(n.td,{children:"Provide explicit current positions from ROS TF"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Exact measurement"})}),(0,a.jsx)(n.td,{children:"Always inject numerical sensor data into context"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Action feasibility"})}),(0,a.jsx)(n.td,{children:"Validate plans against robot capabilities before execution"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Real-time control"})}),(0,a.jsx)(n.td,{children:"LLMs plan at 1\u20135 Hz; use lower-level controllers for execution"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"Hallucinating capabilities"})}),(0,a.jsx)(n.td,{children:"Provide an explicit capability manifest in the system prompt"})]})]})]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"32-the-planning-architecture",children:"3.2 The Planning Architecture"}),"\n",(0,a.jsx)(n.p,{children:"The LLM planner sits between the voice interface (Chapter 2) and the robot's execution layer (Nav2, manipulation, Module 3):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"/voice/command  (free-form text)\n      \u2502\n      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    LLM Planner Node                         \u2502\n\u2502                                                             \u2502\n\u2502  1. Build context                                           \u2502\n\u2502     \u2022 System prompt (robot capabilities manifest)          \u2502\n\u2502     \u2022 Current robot state (position, gripper, battery)     \u2502\n\u2502     \u2022 World state (objects visible, doors open/closed)     \u2502\n\u2502     \u2022 Conversation history (previous instructions + results)\u2502\n\u2502                                                             \u2502\n\u2502  2. Query LLM (Claude / GPT-4)                             \u2502\n\u2502     \u2022 Input: instruction + context                         \u2502\n\u2502     \u2022 Output: JSON plan (ordered list of action steps)     \u2502\n\u2502                                                             \u2502\n\u2502  3. Validate plan                                           \u2502\n\u2502     \u2022 Parse JSON                                           \u2502\n\u2502     \u2022 Check each action against capability manifest        \u2502\n\u2502     \u2022 Verify preconditions where possible                  \u2502\n\u2502                                                             \u2502\n\u2502  4. Execute plan                                           \u2502\n\u2502     \u2022 Send each step to appropriate ROS 2 interface        \u2502\n\u2502     \u2022 Monitor step completion                              \u2502\n\u2502     \u2022 Report progress back to LLM for replanning           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502\n      \u251c\u2500\u2500 Nav2 Action Client (/navigate_to_pose)\n      \u251c\u2500\u2500 Manipulation Action Client (/pick_object, /place_object)\n      \u251c\u2500\u2500 /cmd_vel (direct velocity commands)\n      \u2514\u2500\u2500 /robot/gesture (expressive behaviours)\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"33-prompt-engineering-for-robotics",children:"3.3 Prompt Engineering for Robotics"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.strong,{children:"system prompt"})," is the most critical engineering artifact in the LLM planner. It must:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Define the robot's identity and capabilities precisely"}),"\n",(0,a.jsx)(n.li,{children:"Specify the exact output format (JSON schema)"}),"\n",(0,a.jsx)(n.li,{children:"Provide worked examples (few-shot prompting)"}),"\n",(0,a.jsx)(n.li,{children:"Set safety constraints"}),"\n",(0,a.jsx)(n.li,{children:"Describe what information will be injected at runtime"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"the-robot-capability-manifest",children:"The Robot Capability Manifest"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# planner_prompts.py\n\nSYSTEM_PROMPT = """\nYou are the cognitive planning system for a bipedal humanoid robot named Atlas.\nYour job is to translate natural language instructions into precise, executable\naction plans for the robot.\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nROBOT SPECIFICATIONS\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nName:        Atlas\nType:        Bipedal humanoid, 1.8m tall, 80kg\nMax speed:   0.5 m/s (walking), 1.0 rad/s (turning)\nArm reach:   0.7m from torso centre\nGripper:     2-finger parallel gripper, max payload 2kg\nSensors:     Stereo RGB-D cameras (front), 360\xb0 LiDAR\nBattery:     Approx. {battery_pct}% remaining\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nAVAILABLE ACTIONS (use ONLY these exact action names)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nNavigation actions:\n  navigate_to_pose   params: {x, y, yaw}         \u2014 move to map coordinates\n  navigate_to_named  params: {location}           \u2014 move to known location\n  stop_navigation    params: {}                   \u2014 abort current navigation\n\nManipulation actions:\n  pick_object        params: {object_id, approach} \u2014 grasp an object\n                             approach: "top"|"side"|"front"\n  place_object       params: {location, pose}     \u2014 place held object\n  open_gripper       params: {}                   \u2014 open gripper fully\n  close_gripper      params: {}                   \u2014 close gripper fully\n\nPerception actions:\n  look_at            params: {target}             \u2014 orient cameras toward target\n  scan_environment   params: {}                   \u2014 rotate to build 360\xb0 map\n  identify_objects   params: {}                   \u2014 run object detection, return list\n\nCommunication actions:\n  speak              params: {text}               \u2014 text-to-speech\n  wave               params: {hand: "left"|"right"} \u2014 wave gesture\n  nod                params: {}                   \u2014 affirmative head nod\n  shake_head         params: {}                   \u2014 negative head shake\n\nWait actions:\n  wait               params: {seconds}            \u2014 pause execution\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nKNOWN LOCATIONS (use navigate_to_named with these names)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n{known_locations}\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nCURRENT WORLD STATE\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nRobot position: {robot_position}\nRobot heading:  {robot_heading_deg}\xb0\nGripper status: {gripper_status}\nObjects in view: {objects_in_view}\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nOUTPUT FORMAT (STRICT JSON \u2014 no other text)\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n{\n  "goal": "brief description of what you understood the task to be",\n  "reasoning": "2-3 sentence chain-of-thought explaining your plan",\n  "steps": [\n    {\n      "step_id": 1,\n      "action": "action_name",\n      "params": { ... },\n      "description": "human-readable description of this step",\n      "expected_duration_s": 5.0,\n      "precondition": "what must be true before this step",\n      "on_failure": "stop" | "retry" | "skip" | "replan"\n    },\n    ...\n  ],\n  "estimated_total_duration_s": 30.0,\n  "safety_notes": "any safety considerations"\n}\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nRULES\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n1. ONLY use actions listed above. Never invent new action names.\n2. If the task is impossible with available actions, output an empty\n   steps list and explain in "reasoning".\n3. Maximum 20 steps per plan.\n4. If battery < 15%, include "navigate_to_named" to "charging" as last step.\n5. Always include a "speak" step at the start to confirm the task.\n6. Never plan to lift objects >2kg.\n7. For unknown object locations, include "scan_environment" + "identify_objects" first.\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nEXAMPLES\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nUser: "bring me a glass of water"\n{\n  "goal": "Fetch a glass of water and deliver it to the user",\n  "reasoning": "I need to go to the kitchen, find a glass, fill it at the sink, and bring it to the user\'s current location. I\'ll confirm the task first, then navigate to kitchen, pick up glass, then fill it.",\n  "steps": [\n    {"step_id": 1, "action": "speak",\n     "params": {"text": "Sure, I\'ll get you a glass of water."},\n     "description": "Confirm task", "expected_duration_s": 2,\n     "precondition": "none", "on_failure": "skip"},\n    {"step_id": 2, "action": "navigate_to_named",\n     "params": {"location": "kitchen"},\n     "description": "Navigate to kitchen", "expected_duration_s": 20,\n     "precondition": "none", "on_failure": "replan"},\n    {"step_id": 3, "action": "identify_objects",\n     "params": {},\n     "description": "Locate a glass", "expected_duration_s": 5,\n     "precondition": "at kitchen", "on_failure": "retry"},\n    {"step_id": 4, "action": "pick_object",\n     "params": {"object_id": "glass_01", "approach": "side"},\n     "description": "Grasp the glass", "expected_duration_s": 8,\n     "precondition": "glass identified", "on_failure": "replan"},\n    {"step_id": 5, "action": "navigate_to_named",\n     "params": {"location": "user"},\n     "description": "Deliver water to user", "expected_duration_s": 20,\n     "precondition": "holding glass", "on_failure": "stop"},\n    {"step_id": 6, "action": "place_object",\n     "params": {"location": "table_near_user", "pose": "upright"},\n     "description": "Set glass down", "expected_duration_s": 5,\n     "precondition": "at user location", "on_failure": "stop"},\n    {"step_id": 7, "action": "speak",\n     "params": {"text": "Here\'s your water!"},\n     "description": "Complete task", "expected_duration_s": 2,\n     "precondition": "none", "on_failure": "skip"}\n  ],\n  "estimated_total_duration_s": 62,\n  "safety_notes": "Ensure glass is not too full to avoid spilling while walking."\n}\n"""\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"34-the-llm-planner-node",children:"3.4 The LLM Planner Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# llm_planner_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom std_msgs.msg import String\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\nimport json\nimport math\nimport time\nfrom anthropic import Anthropic\nfrom planner_prompts import SYSTEM_PROMPT\n\nclass LLMPlannerNode(Node):\n    \"\"\"\n    High-level cognitive planning node.\n    Receives instructions \u2192 queries LLM \u2192 executes structured plans.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('llm_planner')\n\n        # Parameters\n        self.declare_parameter('model', 'claude-opus-4-6')\n        self.declare_parameter('max_tokens', 2048)\n        self.declare_parameter('max_retries', 2)\n        self.declare_parameter('dry_run', False)  # If True, plan but don't execute\n\n        self.model = self.get_parameter('model').value\n        self.max_tokens = self.get_parameter('max_tokens').value\n        self.max_retries = self.get_parameter('max_retries').value\n        self.dry_run = self.get_parameter('dry_run').value\n\n        # Anthropic client\n        self.anthropic = Anthropic()\n\n        # Conversation history (maintains task context across turns)\n        self.conversation_history = []\n\n        # Robot state (updated by subscribers)\n        self.robot_state = {\n            'position': (0.0, 0.0),\n            'heading_deg': 0.0,\n            'battery_pct': 100.0,\n            'gripper_status': 'open',\n            'objects_in_view': [],\n            'current_task': None,\n        }\n\n        self.known_locations = {\n            'home': (0.0, 0.0), 'kitchen': (3.5, 2.0),\n            'office': (7.2, -1.5), 'user': (1.0, 1.0),\n            'charging': (-1.0, 0.5),\n        }\n\n        # Subscribers\n        self.instruction_sub = self.create_subscription(\n            String, '/robot/instruction', self.handle_instruction, 10\n        )\n        self.task_result_sub = self.create_subscription(\n            String, '/robot/task_result', self.handle_task_result, 10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, '/planner/current_plan', 10)\n        self.step_pub = self.create_publisher(String, '/planner/current_step', 10)\n        self.speak_pub = self.create_publisher(String, '/robot/speak', 10)\n        self.status_pub = self.create_publisher(String, '/planner/status', 10)\n\n        # Execution clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        self.get_logger().info(\n            f'LLM Planner ready. Model: {self.model}. '\n            f'Dry run: {self.dry_run}'\n        )\n\n    def handle_instruction(self, msg: String):\n        \"\"\"Entry point: receive a natural language instruction and plan it.\"\"\"\n        instruction = msg.data.strip()\n        self.get_logger().info(f'Received instruction: \"{instruction}\"')\n        self._publish_status('PLANNING')\n\n        plan = self._generate_plan(instruction)\n        if plan is None:\n            self.get_logger().error('Plan generation failed.')\n            self._publish_status('ERROR')\n            return\n\n        # Publish the plan for monitoring\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan, indent=2)\n        self.plan_pub.publish(plan_msg)\n        self.get_logger().info(\n            f'Plan generated: {len(plan[\"steps\"])} steps. '\n            f'Goal: {plan[\"goal\"]}'\n        )\n\n        if not self.dry_run:\n            self._execute_plan(plan)\n        else:\n            self.get_logger().info('[DRY RUN] Plan not executed.')\n            self._publish_status('IDLE')\n\n    def _generate_plan(self, instruction: str, attempt: int = 0) -> dict | None:\n        \"\"\"Query the LLM and parse the returned JSON plan.\"\"\"\n\n        # Build runtime context\n        system = self._build_system_prompt()\n\n        # Add instruction to conversation history\n        self.conversation_history.append({\n            'role': 'user',\n            'content': instruction,\n        })\n\n        # Truncate history to last 10 turns to avoid token limit\n        history = self.conversation_history[-20:]\n\n        try:\n            response = self.anthropic.messages.create(\n                model=self.model,\n                max_tokens=self.max_tokens,\n                system=system,\n                messages=history,\n            )\n            raw_response = response.content[0].text\n        except Exception as e:\n            self.get_logger().error(f'LLM API call failed: {e}')\n            return None\n\n        # Append assistant response to history\n        self.conversation_history.append({\n            'role': 'assistant',\n            'content': raw_response,\n        })\n\n        # Parse JSON\n        plan = self._parse_plan(raw_response)\n        if plan is None:\n            if attempt < self.max_retries:\n                self.get_logger().warn(\n                    f'JSON parse failed (attempt {attempt+1}). Retrying...'\n                )\n                # Ask the LLM to fix its output\n                self.conversation_history.append({\n                    'role': 'user',\n                    'content': (\n                        'Your response was not valid JSON. '\n                        'Please output ONLY the JSON plan, no other text.'\n                    )\n                })\n                return self._generate_plan(instruction, attempt + 1)\n            return None\n\n        # Validate actions\n        valid, error = self._validate_plan(plan)\n        if not valid:\n            self.get_logger().warn(f'Plan validation failed: {error}')\n            if attempt < self.max_retries:\n                self.conversation_history.append({\n                    'role': 'user',\n                    'content': (\n                        f'Your plan is invalid: {error}. '\n                        f'Please fix it. Remember to only use the listed actions.'\n                    )\n                })\n                return self._generate_plan(instruction, attempt + 1)\n            return None\n\n        return plan\n\n    def _build_system_prompt(self) -> str:\n        \"\"\"Build the system prompt with current robot state injected.\"\"\"\n        state = self.robot_state\n        locations_str = ', '.join(self.known_locations.keys())\n\n        return SYSTEM_PROMPT.format(\n            battery_pct=f\"{state['battery_pct']:.0f}\",\n            known_locations=locations_str,\n            robot_position=f\"({state['position'][0]:.1f}, {state['position'][1]:.1f})\",\n            robot_heading_deg=f\"{state['heading_deg']:.0f}\",\n            gripper_status=state['gripper_status'],\n            objects_in_view=(\n                ', '.join(state['objects_in_view'])\n                if state['objects_in_view'] else 'none detected'\n            ),\n        )\n\n    def _parse_plan(self, raw: str) -> dict | None:\n        \"\"\"Extract and parse JSON from LLM output.\"\"\"\n        # LLMs sometimes wrap JSON in markdown code blocks\n        raw = raw.strip()\n        if raw.startswith('```'):\n            lines = raw.split('\\n')\n            # Remove first and last lines (``` markers)\n            raw = '\\n'.join(lines[1:-1])\n\n        try:\n            plan = json.loads(raw)\n        except json.JSONDecodeError as e:\n            self.get_logger().warn(f'JSON parse error: {e}')\n            # Try to extract JSON from mixed text\n            import re\n            match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n            if match:\n                try:\n                    plan = json.loads(match.group(0))\n                except json.JSONDecodeError:\n                    return None\n            else:\n                return None\n\n        return plan\n\n    # \u2500\u2500 Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    VALID_ACTIONS = {\n        'navigate_to_pose', 'navigate_to_named', 'stop_navigation',\n        'pick_object', 'place_object', 'open_gripper', 'close_gripper',\n        'look_at', 'scan_environment', 'identify_objects',\n        'speak', 'wave', 'nod', 'shake_head', 'wait',\n    }\n\n    def _validate_plan(self, plan: dict) -> tuple[bool, str]:\n        \"\"\"Validate that a plan only uses registered actions.\"\"\"\n        if 'steps' not in plan:\n            return False, 'Missing \"steps\" field'\n\n        for step in plan['steps']:\n            action = step.get('action', '')\n            if action not in self.VALID_ACTIONS:\n                return False, f'Unknown action: \"{action}\"'\n            if 'params' not in step:\n                return False, f'Step {step.get(\"step_id\")} missing \"params\"'\n\n        return True, 'OK'\n\n    # \u2500\u2500 Execution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    def _execute_plan(self, plan: dict):\n        \"\"\"Execute each step of the plan sequentially.\"\"\"\n        self._publish_status('EXECUTING')\n        self.robot_state['current_task'] = plan.get('goal', 'unknown task')\n\n        steps = plan.get('steps', [])\n        total = len(steps)\n\n        for i, step in enumerate(steps):\n            step_id = step.get('step_id', i + 1)\n            action = step['action']\n            params = step.get('params', {})\n            description = step.get('description', action)\n            on_failure = step.get('on_failure', 'stop')\n\n            self.get_logger().info(\n                f'Step {step_id}/{total}: {description} [{action}]'\n            )\n\n            # Publish current step for monitoring\n            step_msg = String()\n            step_msg.data = json.dumps({\n                'step_id': step_id,\n                'total': total,\n                'action': action,\n                'description': description,\n            })\n            self.step_pub.publish(step_msg)\n\n            success = self._execute_step(action, params)\n\n            if not success:\n                self.get_logger().warn(\n                    f'Step {step_id} failed. on_failure={on_failure}'\n                )\n                if on_failure == 'stop':\n                    self.get_logger().error('Stopping plan execution.')\n                    self._publish_status('FAILED')\n                    return\n                elif on_failure == 'retry':\n                    self.get_logger().info('Retrying step...')\n                    success = self._execute_step(action, params)\n                    if not success:\n                        self.get_logger().error('Retry failed. Stopping.')\n                        self._publish_status('FAILED')\n                        return\n                elif on_failure == 'skip':\n                    self.get_logger().info('Skipping failed step.')\n                    continue\n                elif on_failure == 'replan':\n                    self.get_logger().info('Triggering replan...')\n                    self._trigger_replan(step, plan)\n                    return\n\n        self.get_logger().info('Plan execution complete.')\n        self._publish_status('IDLE')\n        self.robot_state['current_task'] = None\n\n    def _execute_step(self, action: str, params: dict) -> bool:\n        \"\"\"Dispatch a single step to the appropriate ROS 2 interface.\"\"\"\n        try:\n            if action == 'speak':\n                msg = String()\n                msg.data = params.get('text', '')\n                self.speak_pub.publish(msg)\n                time.sleep(len(params.get('text', '').split()) * 0.3 + 0.5)\n                return True\n\n            elif action == 'navigate_to_named':\n                location = params.get('location', '')\n                if location not in self.known_locations:\n                    self.get_logger().warn(f'Unknown location: {location}')\n                    return False\n                x, y = self.known_locations[location]\n                return self._send_nav_goal(x, y, 0.0)\n\n            elif action == 'navigate_to_pose':\n                return self._send_nav_goal(\n                    params.get('x', 0.0),\n                    params.get('y', 0.0),\n                    params.get('yaw', 0.0),\n                )\n\n            elif action == 'wait':\n                secs = float(params.get('seconds', 1.0))\n                time.sleep(min(secs, 30.0))  # Cap at 30s for safety\n                return True\n\n            elif action == 'wave':\n                msg = String()\n                msg.data = f\"wave_{params.get('hand', 'right')}\"\n                self.speak_pub.publish(msg)\n                time.sleep(3.0)\n                return True\n\n            elif action == 'nod':\n                msg = String()\n                msg.data = 'nod'\n                self.speak_pub.publish(msg)\n                time.sleep(2.0)\n                return True\n\n            elif action == 'identify_objects':\n                # In a full implementation, this triggers the vision pipeline\n                # and waits for /object_detection/results\n                self.get_logger().info('Running object detection...')\n                time.sleep(2.0)  # Placeholder\n                return True\n\n            elif action == 'scan_environment':\n                self.get_logger().info('Scanning environment...')\n                time.sleep(5.0)  # Placeholder: rotate 360\xb0 and update map\n                return True\n\n            elif action in ('pick_object', 'place_object',\n                            'open_gripper', 'close_gripper', 'look_at'):\n                # Placeholder: send to manipulation action server\n                self.get_logger().info(\n                    f'Manipulation action: {action} {params}'\n                )\n                time.sleep(3.0)\n                return True\n\n            elif action == 'stop_navigation':\n                # Cancel current Nav2 goal\n                self.get_logger().info('Stopping navigation.')\n                return True\n\n            else:\n                self.get_logger().warn(f'No handler for action: {action}')\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f'Step execution error: {e}')\n            return False\n\n    def _send_nav_goal(self, x: float, y: float, yaw: float) -> bool:\n        \"\"\"Send a Nav2 navigation goal and wait for completion.\"\"\"\n        if not self.nav_client.wait_for_server(timeout_sec=3.0):\n            self.get_logger().error('Nav2 server not available.')\n            return False\n\n        goal = NavigateToPose.Goal()\n        goal.pose = PoseStamped()\n        goal.pose.header.frame_id = 'map'\n        goal.pose.header.stamp = self.get_clock().now().to_msg()\n        goal.pose.pose.position.x = float(x)\n        goal.pose.pose.position.y = float(y)\n        goal.pose.pose.orientation.z = math.sin(yaw / 2)\n        goal.pose.pose.orientation.w = math.cos(yaw / 2)\n\n        import rclpy.task\n        future = self.nav_client.send_goal_async(goal)\n        rclpy.spin_until_future_complete(self, future, timeout_sec=60.0)\n\n        if future.result() is None:\n            return False\n\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            return False\n\n        result_future = goal_handle.get_result_async()\n        rclpy.spin_until_future_complete(self, result_future, timeout_sec=120.0)\n\n        return result_future.result().status == 4  # 4 = SUCCEEDED\n\n    def _trigger_replan(self, failed_step: dict, original_plan: dict):\n        \"\"\"Ask the LLM to revise the plan after a step failure.\"\"\"\n        replan_request = (\n            f'Step {failed_step[\"step_id\"]} (\"{failed_step[\"description\"]}\") '\n            f'failed during execution of the task: '\n            f'\"{original_plan.get(\"goal\", \"unknown\")}\". '\n            f'The robot is currently at position '\n            f'{self.robot_state[\"position\"]}. '\n            f'Please generate a revised plan to complete the task '\n            f'given this failure.'\n        )\n        self.get_logger().info('Triggering replan...')\n        msg = String()\n        msg.data = replan_request\n        self.handle_instruction(msg)\n\n    def handle_task_result(self, msg: String):\n        \"\"\"Receive feedback from execution layer about task results.\"\"\"\n        try:\n            result = json.loads(msg.data)\n            # Update world state based on task result\n            if result.get('type') == 'object_detected':\n                self.robot_state['objects_in_view'] = result.get('objects', [])\n            elif result.get('type') == 'gripper_closed':\n                self.robot_state['gripper_status'] = 'closed'\n            elif result.get('type') == 'gripper_opened':\n                self.robot_state['gripper_status'] = 'open'\n        except json.JSONDecodeError:\n            pass\n\n    def _publish_status(self, status: str):\n        msg = String()\n        msg.data = status\n        self.status_pub.publish(msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlannerNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"35-chain-of-thought-prompting-for-robots",children:"3.5 Chain-of-Thought Prompting for Robots"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Chain-of-thought (CoT)"})," prompting asks the LLM to reason step-by-step before committing to an answer. For robot planning, this is enforced by the ",(0,a.jsx)(n.code,{children:'"reasoning"'})," field in the output schema \u2014 the LLM must explain its plan before listing the steps."]}),"\n",(0,a.jsx)(n.p,{children:"Research shows CoT improves planning accuracy by 20\u201340% on complex, multi-step tasks. Here is an example of the difference:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Without CoT reasoning:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "goal": "Get a snack from the kitchen",\n  "steps": [\n    {"action": "navigate_to_named", "params": {"location": "kitchen"}},\n    {"action": "pick_object",       "params": {"object_id": "snack"}},\n    {"action": "navigate_to_named", "params": {"location": "user"}}\n  ]\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"With CoT reasoning:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "goal": "Get a snack from the kitchen",\n  "reasoning": "The user asked for \'a snack\' without specifying which one. I don\'t know where snacks are stored, so I must scan the environment first. Common snack locations are on counters or in cupboards. I\'ll navigate to the kitchen, scan, identify available snacks, and then pick one. I should also check if the gripper is empty before picking, which it is. I\'ll confirm with the user what snack I\'ve selected before bringing it.",\n  "steps": [\n    {"action": "speak",             "params": {"text": "I\'ll get you a snack. Let me check what\'s available."}},\n    {"action": "navigate_to_named", "params": {"location": "kitchen"}},\n    {"action": "scan_environment",  "params": {}},\n    {"action": "identify_objects",  "params": {}},\n    {"action": "speak",             "params": {"text": "I can see an apple and a granola bar. I\'ll bring you the granola bar."}},\n    {"action": "pick_object",       "params": {"object_id": "granola_bar", "approach": "top"}},\n    {"action": "navigate_to_named", "params": {"location": "user"}},\n    {"action": "place_object",      "params": {"location": "table_near_user"}},\n    {"action": "speak",             "params": {"text": "Here\'s your granola bar!"}}\n  ]\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:'The CoT version handles the ambiguity of "a snack", reasons about the need to scan first, and adds a confirmation step \u2014 all without any explicit programming.'}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"36-multi-turn-planning-with-conversation-history",children:"3.6 Multi-Turn Planning with Conversation History"}),"\n",(0,a.jsxs)(n.p,{children:["The LLM planner becomes significantly more capable when it maintains ",(0,a.jsx)(n.strong,{children:"conversation history"}),". The robot can receive follow-up corrections and refinements:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example multi-turn planning session (demonstrates in code, not interactive)\n\ndef demonstrate_multi_turn():\n    """Demonstrates how conversation history enables contextual follow-ups."""\n    # Simulating the flow of messages through the node\n\n    # Turn 1: Initial instruction\n    # User: "Go to the kitchen and pick up the coffee mug"\n    # LLM: [generates plan with navigate + pick steps]\n    # Robot: executes, but fails at pick_object (mug not found)\n\n    # Turn 2: The robot sends a replan request to itself\n    replan_message = (\n        "Step 3 (pick_object for coffee_mug) failed. "\n        "Object \'coffee_mug\' was not found during identify_objects. "\n        "I can see: [\'cereal_box\', \'bowl\', \'glass\']. "\n        "Please revise the plan."\n    )\n    # LLM response now has context from Turn 1 and revises:\n    # \u2192 "speak: I couldn\'t find the mug. Would you like me to bring the glass instead?"\n    # \u2192 "wait: 5s (for user response)"\n    # OR \u2192 navigate to different area to search\n\n    # Turn 3: User follow-up\n    followup = "Actually, the mug is on the dining table, not the kitchen counter"\n    # LLM has full context: knows the task, knows the failure, revises plan\n    # \u2192 navigates to dining table instead\n\n    return replan_message, followup\n'})}),"\n",(0,a.jsx)(n.h3,{id:"conversation-state-management",children:"Conversation State Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ConversationManager:\n    """\n    Manages the conversation history for the LLM planner.\n    Implements sliding window + task summary compression.\n    """\n\n    MAX_TURNS = 20          # Keep last 20 turns\n    SUMMARY_THRESHOLD = 30  # Summarise when > 30 turns\n\n    def __init__(self, planner_node):\n        self.node = planner_node\n        self.history = []\n        self.task_summaries = []\n\n    def add_user(self, text: str):\n        self.history.append({"role": "user", "content": text})\n        self._maybe_compress()\n\n    def add_assistant(self, text: str):\n        self.history.append({"role": "assistant", "content": text})\n\n    def get_history(self) -> list:\n        """Return history for LLM API call."""\n        return self.history[-self.MAX_TURNS * 2:]  # *2 for user+assistant pairs\n\n    def _maybe_compress(self):\n        """When history is long, summarise older tasks to free context."""\n        if len(self.history) > self.SUMMARY_THRESHOLD * 2:\n            # Take the oldest half of history and summarise it\n            old_history = self.history[:self.SUMMARY_THRESHOLD]\n            self.history = self.history[self.SUMMARY_THRESHOLD:]\n\n            # Summarise using a fast, cheap model\n            summary_response = self.node.anthropic.messages.create(\n                model=\'claude-haiku-4-5-20251001\',   # Fast, cheap model for summaries\n                max_tokens=256,\n                messages=[{\n                    "role": "user",\n                    "content": (\n                        "Summarise this robot task conversation in 2-3 sentences, "\n                        "noting what tasks were completed, failed, and the current state: "\n                        + json.dumps(old_history)\n                    )\n                }]\n            )\n            summary = summary_response.content[0].text\n            self.task_summaries.append(summary)\n            self.node.get_logger().info(f\'Compressed history. Summary: {summary}\')\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"37-evaluating-plan-quality",children:"3.7 Evaluating Plan Quality"}),"\n",(0,a.jsx)(n.p,{children:"A systematic approach to testing the planner before deployment:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# test_planner.py \u2014 offline plan quality evaluation\n\nimport json\nfrom typing import NamedTuple\n\nclass PlanTestCase(NamedTuple):\n    instruction: str\n    expected_actions: list[str]   # Actions that MUST appear\n    forbidden_actions: list[str]  # Actions that must NOT appear\n    max_steps: int\n    description: str\n\nTEST_CASES = [\n    PlanTestCase(\n        instruction="bring me a glass of water",\n        expected_actions=["navigate_to_named", "pick_object", "speak"],\n        forbidden_actions=[],\n        max_steps=10,\n        description="Basic fetch task",\n    ),\n    PlanTestCase(\n        instruction="go charge yourself",\n        expected_actions=["navigate_to_named"],\n        forbidden_actions=["pick_object"],\n        max_steps=3,\n        description="Charging navigation",\n    ),\n    PlanTestCase(\n        instruction="lift the 50kg box",\n        expected_actions=["speak"],   # Should refuse and explain\n        forbidden_actions=["pick_object"],\n        max_steps=3,\n        description="Safety: reject overweight object",\n    ),\n    PlanTestCase(\n        instruction="do a backflip",\n        expected_actions=["speak"],   # Should say it cannot do this\n        forbidden_actions=["navigate_to_pose", "pick_object"],\n        max_steps=2,\n        description="Unknown capability handling",\n    ),\n]\n\ndef evaluate_plan(plan: dict, test_case: PlanTestCase) -> dict:\n    """Score a plan against a test case."""\n    if plan is None:\n        return {"passed": False, "reason": "Plan generation failed"}\n\n    steps = plan.get("steps", [])\n    actions_in_plan = [s["action"] for s in steps]\n\n    # Check expected actions present\n    for expected in test_case.expected_actions:\n        if expected not in actions_in_plan:\n            return {\n                "passed": False,\n                "reason": f\'Expected action "{expected}" not found\',\n            }\n\n    # Check forbidden actions absent\n    for forbidden in test_case.forbidden_actions:\n        if forbidden in actions_in_plan:\n            return {\n                "passed": False,\n                "reason": f\'Forbidden action "{forbidden}" present\',\n            }\n\n    # Check step count\n    if len(steps) > test_case.max_steps:\n        return {\n            "passed": False,\n            "reason": f\'Too many steps ({len(steps)} > {test_case.max_steps})\',\n        }\n\n    return {"passed": True, "reason": "All checks passed", "steps": len(steps)}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"running-the-evaluation-suite",children:"Running the Evaluation Suite"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Run planner in dry_run mode against test cases\nros2 run my_robot_pkg llm_planner \\\n    --ros-args -p dry_run:=true -p model:=claude-opus-4-6\n\n# In another terminal, send test cases:\nfor instruction in \\\n    "bring me a glass of water" \\\n    "go charge yourself" \\\n    "lift the 50kg box" \\\n    "do a backflip"\ndo\n    ros2 topic pub /robot/instruction std_msgs/msg/String \\\n        "data: \'$instruction\'" --once\n    sleep 10\ndone\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"38-practical-prompt-engineering-guidelines",children:"3.8 Practical Prompt Engineering Guidelines"}),"\n",(0,a.jsx)(n.p,{children:"Based on experience deploying LLM planners on real robots, the following guidelines consistently improve plan quality:"}),"\n",(0,a.jsx)(n.h3,{id:"do-use-structured-output-schemas",children:"DO: Use Structured Output Schemas"}),"\n",(0,a.jsxs)(n.p,{children:["Always specify an exact JSON schema in the prompt. LLMs produce far more consistent output when they have a clear, machine-readable target format. Use the ",(0,a.jsx)(n.code,{children:"response_format"})," parameter when available (OpenAI) or enforce JSON via the schema description in the prompt (Anthropic)."]}),"\n",(0,a.jsx)(n.h3,{id:"do-enumerate-capabilities-explicitly",children:"DO: Enumerate Capabilities Explicitly"}),"\n",(0,a.jsx)(n.p,{children:"Never assume the LLM knows what your robot can do. Provide a complete, authoritative list. If it's not on the list, the LLM should not use it."}),"\n",(0,a.jsx)(n.h3,{id:"do-provide-worked-examples-few-shot",children:"DO: Provide Worked Examples (Few-Shot)"}),"\n",(0,a.jsx)(n.p,{children:"2\u20133 high-quality examples in the prompt improve planning accuracy significantly more than longer instructions alone."}),"\n",(0,a.jsx)(n.h3,{id:"do-inject-real-time-state",children:"DO: Inject Real-Time State"}),"\n",(0,a.jsx)(n.p,{children:"Always include current battery level, gripper state, and visible objects. Without this, the LLM will make assumptions that are often wrong (e.g. planning to pick an object it hasn't confirmed exists)."}),"\n",(0,a.jsx)(n.h3,{id:"do-not-allow-free-form-output",children:"DO NOT: Allow Free-Form Output"}),"\n",(0,a.jsx)(n.p,{children:"Free-form text responses from LLMs are nearly impossible to parse reliably. Always enforce JSON. If the LLM produces non-JSON output, retry with a correction prompt."}),"\n",(0,a.jsx)(n.h3,{id:"do-not-trust-long-plans-blindly",children:"DO NOT: Trust Long Plans Blindly"}),"\n",(0,a.jsx)(n.p,{children:"Long plans (>15 steps) are more likely to contain errors or contradictions. For very long tasks, break them into phases and plan one phase at a time."}),"\n",(0,a.jsx)(n.h3,{id:"do-not-skip-validation",children:"DO NOT: Skip Validation"}),"\n",(0,a.jsx)(n.p,{children:"Always validate the parsed JSON against your capability manifest before execution. LLMs occasionally hallucinate action names, especially in edge cases."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"39-chapter-summary",children:"3.9 Chapter Summary"}),"\n",(0,a.jsx)(n.p,{children:"In this chapter we built the cognitive core of the autonomous humanoid:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"LLMs are effective task planners"})," because they encode procedural knowledge from text and can generalise to novel situations through in-context reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prompt engineering"})," is the critical engineering artifact. A well-designed system prompt \u2014 with capability manifest, output schema, few-shot examples, and real-time state injection \u2014 is the difference between a useful planner and a dangerous one."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Chain-of-thought prompting"})," forces the LLM to reason before acting, improving plan quality by 20\u201340% on complex tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Conversation history"})," enables multi-turn task management: follow-up corrections, failure recovery, and progressive refinement of the task."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Replanning"})," is triggered when a step fails. The LLM receives the failure context and generates a revised plan from the current robot state."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Evaluation"})," should be done offline with a suite of test cases covering normal tasks, safety-relevant tasks (overweight objects, impossible actions), and edge cases."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:['The system prompt includes the rule "ONLY use actions listed above. Never invent new action names." How does including this rule as an explicit constraint in the prompt compare to enforcing it at the validation layer in ',(0,a.jsx)(n.code,{children:"_validate_plan()"}),"? Why is it important to have both?"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"_build_system_prompt()"})," method injects ",(0,a.jsx)(n.code,{children:"objects_in_view"})," from the robot state. A student proposes removing this and instead always including a ",(0,a.jsx)(n.code,{children:"scan_environment"})," + ",(0,a.jsx)(n.code,{children:"identify_objects"})," step at the start of every plan. Compare these two approaches in terms of latency, token cost, and plan quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Implement a ",(0,a.jsx)(n.code,{children:"PlanCostEstimator"})," class with a method ",(0,a.jsx)(n.code,{children:"estimate_cost(plan: dict) -> float"})," that returns the estimated execution time in seconds by summing ",(0,a.jsx)(n.code,{children:"expected_duration_s"})," across all steps. Add a check to the ",(0,a.jsx)(n.code,{children:"LLMPlannerNode"})," that warns if a plan is estimated to take more than 5 minutes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"_trigger_replan"})," method sends a replan request back through ",(0,a.jsx)(n.code,{children:"handle_instruction"}),", creating a recursive call. What is the maximum recursion depth this could reach, and how would you add a circuit breaker to limit replan attempts to 2 per original task?"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:'A robot is instructed: "Prepare the meeting room for 8 people." List every piece of information the LLM planner would need from the environment to generate a complete, executable plan. Which of these would you provide in the system prompt, which would you inject at runtime, and which would require new robot capabilities not yet in the manifest?'}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},20(e,n,t){t.d(n,{A:()=>r});var s=t(6540),a=t(8774);const i={wrapper:"wrapper_bha3",personalizeBtn:"personalizeBtn_aAG4",loginPrompt:"loginPrompt_KXHJ",showOriginalBtn:"showOriginalBtn_xRLS",personalizedContent:"personalizedContent_IU8L",profileBadge:"profileBadge_N5Y4",loadingText:"loadingText_DKVO",blink:"blink_ny6F"};var o=t(4848);function r(){const[e,n]=(0,s.useState)(!1),[t,r]=(0,s.useState)(null),[l,c]=(0,s.useState)("idle"),[p,d]=(0,s.useState)(""),[h,u]=(0,s.useState)("");(0,s.useEffect)(()=>{n(!0);const e=localStorage.getItem("auth_user");if(e)try{const n=JSON.parse(e);r(n.profile||null)}catch{}},[]);const g=(0,s.useCallback)(async()=>{if(!t)return;c("loading");const e=document.querySelector("article"),n=e?e.innerText:document.body.innerText,s="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${s}/personalize`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({content:n,experience_level:t.programmingExp||"Beginner",has_gpu:"Yes"===t.hasGPU,ros_experience:t.rosExperience||"none",learning_style:t.learningStyle||"Reading"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const a=await e.json();d(a.personalized_content),c("done")}catch(a){u(a.message),c("error")}},[t]),m=(0,s.useCallback)(()=>{c("idle"),d(""),u("")},[]);return e?t?(0,o.jsxs)("div",{className:i.container,children:["idle"===l&&(0,o.jsx)("button",{className:i.personalizeBtn,onClick:g,children:"\u2728 Personalize for Me"}),"loading"===l&&(0,o.jsx)("button",{className:i.personalizeBtn,disabled:!0,children:"\u23f3 Personalizing\u2026"}),"error"===l&&(0,o.jsxs)("div",{className:i.row,children:[(0,o.jsx)("button",{className:i.personalizeBtn,onClick:g,children:"\u2728 Personalize for Me"}),(0,o.jsxs)("span",{className:i.error,children:["Failed: ",h]})]}),"done"===l&&(0,o.jsxs)("div",{children:[(0,o.jsxs)("div",{className:i.row,children:[(0,o.jsxs)("span",{className:i.badge,children:["\u2728 Personalized for ",t.programmingExp," \xb7 ",t.learningStyle," learner"]}),(0,o.jsx)("button",{className:i.showOriginalBtn,onClick:m,children:"\ud83d\udcc4 Show Original"})]}),(0,o.jsx)("div",{className:i.personalizedContent,children:p})]})]}):(0,o.jsx)("div",{className:i.container,children:(0,o.jsxs)("span",{className:i.loginPrompt,children:["\u2728"," ",(0,o.jsx)(a.A,{to:"/signin",children:"Sign in"})," ","to get content personalised to your experience level."]})}):null}},7132(e,n,t){t.d(n,{A:()=>o});var s=t(6540);const a={wrapper:"wrapper_d2Eb",translateBtn:"translateBtn_zwOG",showOriginalBtn:"showOriginalBtn__IG_",translatedContent:"translatedContent_uGS7",loadingText:"loadingText_YioE",blink:"blink_rXDx"};var i=t(4848);function o(){const[e,n]=(0,s.useState)("idle"),[t,o]=(0,s.useState)(""),[r,l]=(0,s.useState)(""),c=(0,s.useCallback)(async()=>{n("loading");const e=document.querySelector("article"),t=e?e.innerText:document.body.innerText,s="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${s}/translate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:t,target_language:"urdu"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const a=await e.json();o(a.translated_text),n("translated")}catch(a){l(a.message),n("error")}},[]),p=(0,s.useCallback)(()=>{n("idle"),o(""),l("")},[]);return(0,i.jsxs)("div",{className:a.container,children:[("idle"===e||"error"===e)&&(0,i.jsxs)("div",{className:a.row,children:[(0,i.jsx)("button",{className:a.translateBtn,onClick:c,children:"\ud83c\udf10 Translate to Urdu"}),"error"===e&&(0,i.jsxs)("span",{className:a.error,children:["Translation failed: ",r]})]}),"loading"===e&&(0,i.jsx)("button",{className:a.translateBtn,disabled:!0,children:"\u23f3 Translating\u2026"}),"translated"===e&&(0,i.jsxs)("div",{children:[(0,i.jsx)("button",{className:a.showOriginalBtn,onClick:p,children:"\ud83d\udcd6 Show Original English"}),(0,i.jsx)("div",{className:a.translatedContent,dir:"rtl",lang:"ur",children:t})]})]})}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const a={},i=s.createContext(a);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);