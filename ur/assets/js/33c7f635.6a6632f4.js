"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[9959],{9184(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"module4-vla/chapter1","title":"Chapter 1: Introduction to Vision-Language-Action Models","description":"Learning Objectives","source":"@site/docs/module4-vla/chapter1.mdx","sourceDirName":"module4-vla","slug":"/module4-vla/chapter1","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module4-vla/chapter1","draft":false,"unlisted":false,"editUrl":"https://github.com/samreensaif/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/tree/main/physical-ai-textbook/docs/module4-vla/chapter1.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Chapter 1: Introduction to Vision-Language-Action Models","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Nav2 and Path Planning for Humanoids","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module3-isaac/chapter4"},"next":{"title":"Chapter 2: Voice to Action with Whisper","permalink":"/Hackathon-One-Physical-AI-Humanoid-Robotics-Course-/ur/docs/module4-vla/chapter2"}}');var s=t(4848),o=t(8453),r=t(7132),a=t(20);const l={title:"Chapter 1: Introduction to Vision-Language-Action Models",sidebar_position:1},d="Chapter 1: Introduction to Vision-Language-Action Models",c={},h=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1.1 The Grand Challenge of Robot Intelligence",id:"11-the-grand-challenge-of-robot-intelligence",level:2},{value:"1.2 From Language Models to Robot Foundation Models",id:"12-from-language-models-to-robot-foundation-models",level:2},{value:"The Foundation Model Revolution (2017\u20132022)",id:"the-foundation-model-revolution-20172022",level:3},{value:"Why Language Matters for Robots",id:"why-language-matters-for-robots",level:3},{value:"1.3 The VLA Architecture",id:"13-the-vla-architecture",level:2},{value:"The Vision Encoder",id:"the-vision-encoder",level:3},{value:"The Language Backbone",id:"the-language-backbone",level:3},{value:"The Action Head",id:"the-action-head",level:3},{value:"1.4 Landmark VLA Systems",id:"14-landmark-vla-systems",level:2},{value:"RT-2: Robotic Transformer 2 (Google DeepMind, 2023)",id:"rt-2-robotic-transformer-2-google-deepmind-2023",level:3},{value:"OpenVLA (Berkeley, 2024)",id:"openvla-berkeley-2024",level:3},{value:"\u03c00 (Physical Intelligence, 2024)",id:"\u03c00-physical-intelligence-2024",level:3},{value:"Octo (UC Berkeley, 2023)",id:"octo-uc-berkeley-2023",level:3},{value:"1.5 Training Data: The Open X-Embodiment Dataset",id:"15-training-data-the-open-x-embodiment-dataset",level:2},{value:"1.6 The Inference Pipeline",id:"16-the-inference-pipeline",level:2},{value:"Latency Breakdown",id:"latency-breakdown",level:3},{value:"1.7 Principal Research Challenges",id:"17-principal-research-challenges",level:2},{value:"1. Inference Speed",id:"1-inference-speed",level:3},{value:"2. Generalisation vs Overfitting",id:"2-generalisation-vs-overfitting",level:3},{value:"3. Safety and Predictability",id:"3-safety-and-predictability",level:3},{value:"4. Long-Horizon Planning",id:"4-long-horizon-planning",level:3},{value:"1.8 The VLA Landscape: A Taxonomy",id:"18-the-vla-landscape-a-taxonomy",level:2},{value:"1.9 Chapter Summary",id:"19-chapter-summary",level:2},{value:"Review Questions",id:"review-questions",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.A,{}),"\n",(0,s.jsx)(a.A,{}),"\n",(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-1-introduction-to-vision-language-action-models",children:"Chapter 1: Introduction to Vision-Language-Action Models"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Define Vision-Language-Action (VLA) models and explain how they differ from previous approaches to robot control"}),"\n",(0,s.jsx)(n.li,{children:"Trace the intellectual lineage from large language models to robot foundation models"}),"\n",(0,s.jsx)(n.li,{children:"Describe the architectural components of a VLA model: the vision encoder, language backbone, and action decoder"}),"\n",(0,s.jsx)(n.li,{children:"Compare major VLA systems (RT-2, OpenVLA, \u03c00, Octo) across training data, architecture, and capability"}),"\n",(0,s.jsx)(n.li,{children:"Explain the Open X-Embodiment dataset and why cross-embodiment training matters"}),"\n",(0,s.jsx)(n.li,{children:"Identify the principal unsolved challenges in VLA research: generalisation, speed, safety, and embodiment mismatch"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"11-the-grand-challenge-of-robot-intelligence",children:"1.1 The Grand Challenge of Robot Intelligence"}),"\n",(0,s.jsx)(n.p,{children:"Consider what a two-year-old human child can do that no robot could reliably replicate as recently as 2021:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Pick up a toy it has never seen before"}),"\n",(0,s.jsx)(n.li,{children:'Follow the instruction "put the blue thing near the plant"'}),"\n",(0,s.jsx)(n.li,{children:"Recover gracefully when it drops something"}),"\n",(0,s.jsx)(n.li,{children:'Understand that "the cup is empty" means it should refill it'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Each of these requires the seamless integration of ",(0,s.jsx)(n.strong,{children:"vision"})," (perceiving the world), ",(0,s.jsx)(n.strong,{children:"language"})," (understanding intent), and ",(0,s.jsx)(n.strong,{children:"action"})," (physically manipulating objects). No amount of explicit programming captures the full breadth of common-sense understanding these tasks demand."]}),"\n",(0,s.jsx)(n.p,{children:"The field's dominant response before 2022 was to treat these as separate engineering problems: a perception module, a natural language parser, a task planner, and a motion controller \u2014 all hand-designed and hand-integrated. This modular approach produces systems that work well in controlled conditions but fail spectacularly outside their training distribution."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Vision-Language-Action (VLA) models"})," represent a fundamentally different approach: train a single neural network \u2014 large enough to absorb enormous amounts of data \u2014 end-to-end to map raw sensory inputs and language instructions directly to robot actions. The bet is that sufficient scale and diverse data will produce emergent generalisation that no modular pipeline can match."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"12-from-language-models-to-robot-foundation-models",children:"1.2 From Language Models to Robot Foundation Models"}),"\n",(0,s.jsx)(n.h3,{id:"the-foundation-model-revolution-20172022",children:"The Foundation Model Revolution (2017\u20132022)"}),"\n",(0,s.jsxs)(n.p,{children:["The breakthrough that made VLA possible was the development of large ",(0,s.jsx)(n.strong,{children:"foundation models"})," \u2014 neural networks trained on internet-scale data that learn rich, general-purpose representations."]}),"\n",(0,s.jsx)(n.p,{children:"The key milestones:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Year"}),(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Significance"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2017"}),(0,s.jsx)(n.td,{children:"Transformer (Vaswani et al.)"}),(0,s.jsx)(n.td,{children:"Architecture that scales to billion+ parameters"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2020"}),(0,s.jsx)(n.td,{children:"GPT-3"}),(0,s.jsx)(n.td,{children:"First demonstration that scale produces emergent reasoning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2021"}),(0,s.jsx)(n.td,{children:"CLIP (Radford et al.)"}),(0,s.jsx)(n.td,{children:"Joint vision-language embedding space via contrastive learning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2022"}),(0,s.jsx)(n.td,{children:"ChatGPT"}),(0,s.jsx)(n.td,{children:"Instruction-following via RLHF; language AI becomes mainstream"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2022"}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.strong,{children:"RT-1"})," (Google)"]}),(0,s.jsx)(n.td,{children:"First large-scale robot transformer; 130,000 demonstrations"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2023"}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.strong,{children:"RT-2"})," (Google DeepMind)"]}),(0,s.jsx)(n.td,{children:"VLM weights initialised from web data, fine-tuned on robot data"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2023"}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.strong,{children:"Octo"})," (Berkeley)"]}),(0,s.jsx)(n.td,{children:"Open-source robot foundation model"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2024"}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.strong,{children:"\u03c00 (pi-zero)"})," (Physical Intelligence)"]}),(0,s.jsx)(n.td,{children:"First VLA for dexterous manipulation at commercial scale"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2024"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"OpenVLA"})}),(0,s.jsx)(n.td,{children:"Open-source VLA trained on Open X-Embodiment"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"why-language-matters-for-robots",children:"Why Language Matters for Robots"}),"\n",(0,s.jsxs)(n.p,{children:["Language is not just a convenience interface. It is the ",(0,s.jsx)(n.strong,{children:"compressed representation of human intent and world knowledge"}),'. When a model trained on internet text learns that "the mug is to the left of the laptop," it encodes spatial relationships, object permanence, and contextual understanding that would take years to manually program.']}),"\n",(0,s.jsx)(n.p,{children:"VLA models exploit this by initialising robot controllers from pretrained vision-language models (VLMs), which already understand the visual and semantic structure of the world. Robot-specific fine-tuning then teaches the model to translate this understanding into actions."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"13-the-vla-architecture",children:"1.3 The VLA Architecture"}),"\n",(0,s.jsx)(n.p,{children:"A VLA model has three functional components, though they are typically implemented as a single end-to-end model:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        VLA Model Architecture                         \u2502\n\u2502                                                                       \u2502\n\u2502  Inputs:                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Camera Images \u2502  \u2502  Language Instruction                     \u2502   \u2502\n\u2502  \u2502  (RGB or RGBD) \u2502  \u2502  "Pick up the red cup and place it on     \u2502   \u2502\n\u2502  \u2502  256\xd7256\xd73     \u2502  \u2502   the tray to the right of the plate"    \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502          \u2502                              \u2502                             \u2502\n\u2502          \u25bc                              \u25bc                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502  \u2502 Vision Encoder\u2502              \u2502 Text Tokeniser\u2502                    \u2502\n\u2502  \u2502 (ViT-B/16 or  \u2502              \u2502 (BPE, 32k     \u2502                    \u2502\n\u2502  \u2502  SigLIP)      \u2502              \u2502  vocabulary)  \u2502                    \u2502\n\u2502  \u2502               \u2502              \u2502               \u2502                    \u2502\n\u2502  \u2502 Outputs:      \u2502              \u2502 Outputs:      \u2502                    \u2502\n\u2502  \u2502 256 patch     \u2502              \u2502 N language    \u2502                    \u2502\n\u2502  \u2502 tokens        \u2502              \u2502 tokens        \u2502                    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2502          \u2502                              \u2502                             \u2502\n\u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                            \u2502\n\u2502                         \u25bc                                             \u2502\n\u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n\u2502                \u2502  Language Model  \u2502                                   \u2502\n\u2502                \u2502  Backbone        \u2502                                   \u2502\n\u2502                \u2502  (LLaMA-7B /     \u2502                                   \u2502\n\u2502                \u2502   PaLI-X / etc.) \u2502                                   \u2502\n\u2502                \u2502                  \u2502                                   \u2502\n\u2502                \u2502  Processes joint \u2502                                   \u2502\n\u2502                \u2502  vision+language \u2502                                   \u2502\n\u2502                \u2502  token sequence  \u2502                                   \u2502\n\u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n\u2502                         \u2502                                             \u2502\n\u2502                         \u25bc                                             \u2502\n\u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n\u2502                \u2502  Action Head /   \u2502                                   \u2502\n\u2502                \u2502  Decoder         \u2502                                   \u2502\n\u2502                \u2502                  \u2502                                   \u2502\n\u2502                \u2502  Converts LM     \u2502                                   \u2502\n\u2502                \u2502  output tokens   \u2502                                   \u2502\n\u2502                \u2502  to robot joint  \u2502                                   \u2502\n\u2502                \u2502  positions or    \u2502                                   \u2502\n\u2502                \u2502  end-effector    \u2502                                   \u2502\n\u2502                \u2502  deltas          \u2502                                   \u2502\n\u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n\u2502                         \u2502                                             \u2502\n\u2502  Output:                \u25bc                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Robot Action: [\u0394x, \u0394y, \u0394z, \u0394roll, \u0394pitch, \u0394yaw, gripper]  \u2502    \u2502\n\u2502  \u2502  7-dimensional end-effector delta @ 5\u201350 Hz                  \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(n.h3,{id:"the-vision-encoder",children:"The Vision Encoder"}),"\n",(0,s.jsxs)(n.p,{children:["The vision encoder converts pixel images into a sequence of ",(0,s.jsx)(n.strong,{children:"patch tokens"})," \u2014 compact vector representations of image regions. The standard approach is a ",(0,s.jsx)(n.strong,{children:"Vision Transformer (ViT)"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The image (e.g. 224\xd7224) is divided into a grid of 16\xd716 patches \u2192 196 patches"}),"\n",(0,s.jsx)(n.li,{children:"Each patch is projected to a high-dimensional vector (typically 768 or 1024 dimensions)"}),"\n",(0,s.jsx)(n.li,{children:"Positional embeddings are added and the sequence is processed by transformer layers"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"SigLIP"})," (Sigmoid Loss for Language-Image Pre-training) is the vision encoder used in OpenVLA and many recent VLA systems. It produces higher-quality image representations than CLIP for robotic manipulation tasks."]}),"\n",(0,s.jsx)(n.h3,{id:"the-language-backbone",children:"The Language Backbone"}),"\n",(0,s.jsxs)(n.p,{children:["The core of a VLA model is a ",(0,s.jsx)(n.strong,{children:"large language model"})," (LLaMA, PaLM, Gemma, etc.) pre-trained on internet text and image-text pairs. The vision tokens are prepended to the language token sequence, allowing the model to attend jointly to visual and linguistic context."]}),"\n",(0,s.jsxs)(n.p,{children:["This is the key insight of VLA models: ",(0,s.jsx)(n.strong,{children:"vision tokens are treated exactly like language tokens"}),'. The LLM attends across both, allowing visual grounding \u2014 understanding that "the blue one" refers to the blue object in the image, not some abstract "blue thing."']}),"\n",(0,s.jsx)(n.h3,{id:"the-action-head",children:"The Action Head"}),"\n",(0,s.jsx)(n.p,{children:"The action head is a small network attached to the LLM's output that maps language model hidden states to robot actions. Two strategies are common:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tokenised actions"}),": Actions are discretised into bins (e.g. 256 bins per dimension) and represented as special vocabulary tokens. The LLM's output distribution over its vocabulary implicitly encodes action distributions. RT-2 and OpenVLA use this approach."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'LLM output tokens \u2192 "ACTION_234 ACTION_156 ACTION_089 ..." \u2192 decode bins \u2192 [\u0394x, \u0394y, \u0394z, ...]\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Continuous action diffusion"}),": A diffusion model (like in \u03c00) takes the LLM's hidden states as conditioning and denoises random Gaussian noise into a continuous action trajectory. This produces smoother, more dexterous actions."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"14-landmark-vla-systems",children:"1.4 Landmark VLA Systems"}),"\n",(0,s.jsx)(n.h3,{id:"rt-2-robotic-transformer-2-google-deepmind-2023",children:"RT-2: Robotic Transformer 2 (Google DeepMind, 2023)"}),"\n",(0,s.jsx)(n.p,{children:"RT-2 was the first large-scale demonstration that a web-pretrained VLM could be fine-tuned into a competent robot controller."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),": Built on PaLI-X (55B parameters), a VLM pretrained on billions of image-text pairs from the web."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Training"}),": Co-trained on both the original VLM data (web text + images) and robot demonstration data from a fleet of 13 robots collecting data continuously in Google's offices. Crucially, the robot actions were serialised as text strings and included directly in the LLM's training vocabulary."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key capabilities"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emergent semantic reasoning"}),': When asked "pick up the object that could be used to extinguish a fire," RT-2 correctly identifies and picks up a toy water bottle \u2014 a capability never explicitly demonstrated in training.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Chain-of-thought reasoning"}),': With a special prompt, RT-2 can generate intermediate reasoning steps ("the water bottle is on the left, I should move the arm to the left...") before outputting actions.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cross-embodiment"}),": Skills learned on one robot arm partially transfer to other arm configurations."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Limitations"}),": 55B parameters requires a server-side GPU; cannot run on-robot. 3 Hz inference rate is too slow for fast manipulation."]}),"\n",(0,s.jsx)(n.h3,{id:"openvla-berkeley-2024",children:"OpenVLA (Berkeley, 2024)"}),"\n",(0,s.jsx)(n.p,{children:"OpenVLA is the open-source community's answer to RT-2 \u2014 a 7.5B parameter VLA built on the Prismatic VLM and trained on the Open X-Embodiment dataset."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),": Prismatic (SigLIP vision encoder + LLaMA-2-7B backbone) + tokenised action head."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Training data"}),": The full Open X-Embodiment dataset \u2014 970,000 robot demonstrations across 22 embodiments (robot arms, mobile manipulators) and 50+ environments."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why OpenVLA matters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fully open weights (Apache 2.0 license)"}),"\n",(0,s.jsx)(n.li,{children:"Can be fine-tuned on custom robot data with 4\xd7 A100 GPUs in ~24 hours"}),"\n",(0,s.jsx)(n.li,{children:"Reproducible training pipeline with documented hyperparameters"}),"\n",(0,s.jsx)(n.li,{children:"7.5B parameters fits on a single A100 80GB GPU for inference"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"OpenVLA performance vs RT-2-E (55B):\n  Google robot manipulation tasks:\n    RT-2-E:  62% success\n    OpenVLA: 56% success  (with 8\xd7 fewer parameters)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"\u03c00-physical-intelligence-2024",children:"\u03c00 (Physical Intelligence, 2024)"}),"\n",(0,s.jsxs)(n.p,{children:['\u03c00 ("pi-zero") represents the next generation: a VLA designed specifically for ',(0,s.jsx)(n.strong,{children:"dexterous manipulation"})," at the pace required for real-world deployment."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key innovation"}),": \u03c00 uses a ",(0,s.jsx)(n.strong,{children:"flow matching"})," action decoder (a form of diffusion) instead of tokenised actions. This produces smooth, 50 Hz action trajectories suitable for tasks like folding laundry, assembling objects, and packaging items."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Training scale"}),": Trained on data from 7 different robot types covering 68 tasks \u2014 the largest cross-embodiment dataset for manipulation at the time of release."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Capabilities"}),": \u03c00 can fold T-shirts, assemble a circuit board, pack a grocery bag, and clean a kitchen \u2014 tasks that previous robot systems could not approach."]}),"\n",(0,s.jsx)(n.h3,{id:"octo-uc-berkeley-2023",children:"Octo (UC Berkeley, 2023)"}),"\n",(0,s.jsx)(n.p,{children:"Octo is a smaller (93M parameters), faster open-source robot transformer designed for real-time on-robot deployment."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),": Sequence-to-action transformer trained on Open X-Embodiment. Inputs: images + language goals. Output: action chunks (multiple steps at once, for smoother motion)."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key design choices"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action chunking"}),": predicts 4 future actions at once, reducing the effective inference frequency requirement to 12 Hz from 50 Hz"]}),"\n",(0,s.jsx)(n.li,{children:"Small enough to run on a laptop GPU (RTX 3080) at 10+ Hz"}),"\n",(0,s.jsx)(n.li,{children:"Easy fine-tuning on custom data in ~2 hours on a single GPU"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"15-training-data-the-open-x-embodiment-dataset",children:"1.5 Training Data: The Open X-Embodiment Dataset"}),"\n",(0,s.jsxs)(n.p,{children:["VLA models are data-hungry. The ",(0,s.jsx)(n.strong,{children:"Open X-Embodiment (OXE)"})," dataset, published by a consortium of 33 research labs, is the most important public training corpus:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Open X-Embodiment Dataset (2024):\n  Total demonstrations:  970,000+\n  Robot types:           22 embodiments\n    \u2022 Robot arms: Franka, UR5, Kuka, xArm, Jaco\n    \u2022 Mobile manipulators: RT-1 robot, Stretch, HSR\n    \u2022 Dexterous hands: Allegro, Shadow\n  Tasks:\n    \u2022 Pick and place (most common)\n    \u2022 Drawer/cabinet opening\n    \u2022 Folding, wiping, pouring\n    \u2022 Navigation + manipulation\n  Environments:\n    \u2022 Kitchen counters\n    \u2022 Office desks\n    \u2022 Factory bins\n    \u2022 Lab benches\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why cross-embodiment training matters"}),': A model trained only on Franka Panda demonstrations learns Franka-specific biases \u2014 specific workspace geometry, gripper size, torque profiles. Training across 22 robot types forces the model to learn embodiment-invariant representations of tasks (what a "pick" motion looks like conceptually) separate from embodiment-specific representations (how this specific robot executes it). This generalises better to new robots.']}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"16-the-inference-pipeline",children:"1.6 The Inference Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"At inference time (on a deployed robot), a VLA model runs the following pipeline every control step:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Conceptual VLA inference loop (pseudocode)\nimport torch\n\ndef vla_control_loop(vla_model, instruction: str, camera_topic: str):\n    """\n    Runs the VLA model at control frequency.\n    In practice this runs inside a ROS 2 node.\n    """\n    while robot_is_running:\n        # 1. Capture current image\n        image = capture_camera_frame(camera_topic)   # (H, W, 3) numpy array\n\n        # 2. Encode inputs\n        vision_tokens = vla_model.encode_image(image)         # (N_vis, D)\n        language_tokens = vla_model.tokenize(instruction)     # (N_lang,)\n\n        # 3. Forward pass through LLM backbone\n        # Vision and language tokens are concatenated and processed jointly\n        joint_tokens = torch.cat([vision_tokens, language_tokens], dim=0)\n        hidden_states = vla_model.backbone(joint_tokens)      # (N, D)\n\n        # 4. Decode action\n        action = vla_model.action_head(hidden_states)\n        # action shape: (7,) = [\u0394x, \u0394y, \u0394z, \u0394roll, \u0394pitch, \u0394yaw, gripper]\n\n        # 5. Send to robot\n        robot.apply_action(action)\n\n        # 6. Check termination\n        if task_complete(image, instruction):\n            break\n'})}),"\n",(0,s.jsx)(n.h3,{id:"latency-breakdown",children:"Latency Breakdown"}),"\n",(0,s.jsx)(n.p,{children:"The bottleneck in VLA inference is the LLM backbone:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Time (A100 GPU)"}),(0,s.jsx)(n.th,{children:"Time (Jetson Orin)"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Image capture + preprocessing"}),(0,s.jsx)(n.td,{children:"~1 ms"}),(0,s.jsx)(n.td,{children:"~5 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Vision encoder (SigLIP)"}),(0,s.jsx)(n.td,{children:"~15 ms"}),(0,s.jsx)(n.td,{children:"~80 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"LLM backbone (7B params, int4)"}),(0,s.jsx)(n.td,{children:"~50 ms"}),(0,s.jsx)(n.td,{children:"~500 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Action head"}),(0,s.jsx)(n.td,{children:"~1 ms"}),(0,s.jsx)(n.td,{children:"~2 ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Total (effective Hz)"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"~15 Hz"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"~1.6 Hz"})})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"This latency gap is why production humanoid deployments (Figure, Agility, Boston Dynamics) run the VLA on a server or powerful onboard GPU and use a faster local controller for high-frequency actuation."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"17-principal-research-challenges",children:"1.7 Principal Research Challenges"}),"\n",(0,s.jsx)(n.p,{children:"VLA models are impressive but far from solved. Understanding the open problems is essential context for any practitioner:"}),"\n",(0,s.jsx)(n.h3,{id:"1-inference-speed",children:"1. Inference Speed"}),"\n",(0,s.jsx)(n.p,{children:"Current 7B-parameter VLA models run at 10\u201315 Hz on an A100 GPU. Robot manipulation often requires 50\u2013100 Hz. Solutions under active research:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speculative decoding"}),": draft small model, verify with large model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action chunking"}),": predict K future steps, execute between inference calls"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distillation"}),": train a small fast student model from a large slow teacher"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-generalisation-vs-overfitting",children:"2. Generalisation vs Overfitting"}),"\n",(0,s.jsx)(n.p,{children:"VLA models trained on narrow datasets overfit to specific object appearances, lighting conditions, and environments. Solutions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Larger, more diverse training datasets"}),"\n",(0,s.jsx)(n.li,{children:"Synthetic data augmentation (Isaac Sim Replicator \u2014 Module 3, Chapter 2)"}),"\n",(0,s.jsx)(n.li,{children:"In-context learning: feed demonstrations at inference time"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-safety-and-predictability",children:"3. Safety and Predictability"}),"\n",(0,s.jsx)(n.p,{children:"An LLM backbone is inherently stochastic. For a language chatbot, an unexpected output is annoying. For a robot arm near a human, it can be dangerous. Active research areas:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Formal verification of action bounds"}),"\n",(0,s.jsx)(n.li,{children:"Uncertainty estimation and safe fallback to classical controllers"}),"\n",(0,s.jsx)(n.li,{children:"Constrained decoding (never output actions outside joint limits)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-long-horizon-planning",children:"4. Long-Horizon Planning"}),"\n",(0,s.jsx)(n.p,{children:'Current VLA models typically execute 10\u201330 second tasks. For long-horizon tasks ("prepare breakfast"), they struggle with:'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Maintaining task context over many steps"}),"\n",(0,s.jsx)(n.li,{children:"Error recovery when a sub-task fails"}),"\n",(0,s.jsx)(n.li,{children:"Hierarchical decomposition into sub-goals"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The hybrid approach (VLA for primitive skills + LLM planner for task decomposition) explored in Chapter 3 of this module addresses this limitation."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"18-the-vla-landscape-a-taxonomy",children:"1.8 The VLA Landscape: A Taxonomy"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Robot Control Approaches\n\u251c\u2500\u2500 Classical (explicit programming)\n\u2502   \u251c\u2500\u2500 Pros: predictable, fast, safe\n\u2502   \u2514\u2500\u2500 Cons: brittle, requires exact world model\n\u2502\n\u251c\u2500\u2500 Modular Learning (separate perception + planning + control)\n\u2502   \u251c\u2500\u2500 Pros: each component improvable independently\n\u2502   \u2514\u2500\u2500 Cons: compounding errors, no emergent generalisation\n\u2502\n\u2514\u2500\u2500 End-to-End (VLA \u2014 this module)\n    \u251c\u2500\u2500 Behaviour Cloning VLA (RT-2, OpenVLA, Octo)\n    \u2502   \u251c\u2500\u2500 Train: imitation learning from demonstrations\n    \u2502   \u2514\u2500\u2500 Strong on tasks similar to training data\n    \u2502\n    \u251c\u2500\u2500 RL-augmented VLA (\u03c00 + RL fine-tuning)\n    \u2502   \u251c\u2500\u2500 Train: BC pretraining + RL on target tasks\n    \u2502   \u2514\u2500\u2500 Better generalisation, harder to train\n    \u2502\n    \u2514\u2500\u2500 Hierarchical (LLM planner + VLA primitive executor)\n        \u251c\u2500\u2500 LLM: high-level task decomposition\n        \u2514\u2500\u2500 VLA: low-level primitive execution\n            \u2192 This is the architecture we build in Chapter 4\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"19-chapter-summary",children:"1.9 Chapter Summary"}),"\n",(0,s.jsx)(n.p,{children:"Vision-Language-Action models represent the frontier of robot intelligence, integrating three previously separate fields \u2014 computer vision, natural language processing, and robot control \u2014 into unified learned systems."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"VLA models"})," map camera images and language instructions directly to robot actions via a joint vision-language model backbone."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The architecture"})," consists of a vision encoder (ViT/SigLIP), a language model backbone (LLaMA, PaLM), and an action head (tokenised or diffusion-based)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"RT-2"})," demonstrated that web-pretrained VLMs can be fine-tuned into competent robot controllers with emergent semantic reasoning. ",(0,s.jsx)(n.strong,{children:"OpenVLA"})," made this accessible with open weights. ",(0,s.jsx)(n.strong,{children:"\u03c00"})," demonstrated dexterous manipulation via flow matching actions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Open X-Embodiment"})," (970,000 demonstrations across 22 robot types) is the key training dataset enabling cross-embodiment generalisation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Open challenges"}),": inference speed, generalisation beyond training distribution, safety guarantees, and long-horizon planning."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In this module, we build a ",(0,s.jsx)(n.strong,{children:"hierarchical VLA system"}),": voice commands processed by Whisper \u2192 high-level planning by an LLM \u2192 low-level execution via ROS 2 Nav2 and manipulation primitives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Explain in your own words why treating vision tokens and language tokens identically within a transformer backbone is a meaningful design choice for robot control. What does it allow the model to do that separate vision and language encoders cannot?"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Compare OpenVLA and \u03c00 on three dimensions: model size, action representation, and target task complexity. Which would you choose for: (a) a pick-and-place task on a Franka arm; (b) folding a shirt; (c) navigating a mobile robot through a building?"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:'RT-2\'s paper reports that the model can correctly respond to "pick up the object you would use to clean the table" without ever having seen this exact instruction in training. What capability of the pretrained LLM backbone makes this possible, and what does it demonstrate about emergent generalisation?'}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The action chunking technique (predicting K future steps at once) is used in both Octo and Diffusion Policy. Explain what problem it solves and what trade-off it introduces."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"A robotics startup proposes deploying a 7B-parameter VLA model on a Jetson Orin AGX for real-time manipulation at 30 Hz. Based on the latency breakdown in Section 1.6, is this feasible? What architectural choices or hardware changes could make it feasible?"}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},20(e,n,t){t.d(n,{A:()=>g});var i=t(6540),s=t(8774);const o="container_dCV0",r="row_RaZg",a="personalizeBtn_aAG4",l="showOriginalBtn_xRLS",d="badge_sNej",c="personalizedContent_IU8L",h="loginPrompt_KXHJ",p="error_X2St";var u=t(4848);function g(){const[e,n]=(0,i.useState)(!1),[t,g]=(0,i.useState)(null),[m,x]=(0,i.useState)("idle"),[j,b]=(0,i.useState)(""),[f,v]=(0,i.useState)("");(0,i.useEffect)(()=>{n(!0);const e=localStorage.getItem("auth_user");if(e)try{const n=JSON.parse(e);g(n.profile||null)}catch{}},[]);const y=(0,i.useCallback)(async()=>{if(!t)return;x("loading");const e=document.querySelector("article"),n=e?e.innerText:document.body.innerText,i="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${i}/personalize`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({content:n,experience_level:t.programmingExp||"Beginner",has_gpu:"Yes"===t.hasGPU,ros_experience:t.rosExperience||"none",learning_style:t.learningStyle||"Reading"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const s=await e.json();b(s.personalized_content),x("done")}catch(s){v(s.message),x("error")}},[t]),L=(0,i.useCallback)(()=>{x("idle"),b(""),v("")},[]);return e?t?(0,u.jsxs)("div",{className:o,children:["idle"===m&&(0,u.jsx)("button",{className:a,onClick:y,children:"\u2728 Personalize for Me"}),"loading"===m&&(0,u.jsx)("button",{className:a,disabled:!0,children:"\u23f3 Personalizing\u2026"}),"error"===m&&(0,u.jsxs)("div",{className:r,children:[(0,u.jsx)("button",{className:a,onClick:y,children:"\u2728 Personalize for Me"}),(0,u.jsxs)("span",{className:p,children:["Failed: ",f]})]}),"done"===m&&(0,u.jsxs)("div",{children:[(0,u.jsxs)("div",{className:r,children:[(0,u.jsxs)("span",{className:d,children:["\u2728 Personalized for ",t.programmingExp," \xb7 ",t.learningStyle," learner"]}),(0,u.jsx)("button",{className:l,onClick:L,children:"\ud83d\udcc4 Show Original"})]}),(0,u.jsx)("div",{className:c,children:j})]})]}):(0,u.jsx)("div",{className:o,children:(0,u.jsxs)("span",{className:h,children:["\u2728"," ",(0,u.jsx)(s.A,{to:"/signin",children:"Sign in"})," ","to get content personalised to your experience level."]})}):null}},7132(e,n,t){t.d(n,{A:()=>h});var i=t(6540);const s="container_sRRF",o="row_Wfea",r="translateBtn_zwOG",a="showOriginalBtn__IG_",l="translatedContent_uGS7",d="error_H8Lp";var c=t(4848);function h(){const[e,n]=(0,i.useState)("idle"),[t,h]=(0,i.useState)(""),[p,u]=(0,i.useState)(""),g=(0,i.useCallback)(async()=>{n("loading");const e=document.querySelector("article"),t=e?e.innerText:document.body.innerText,i="undefined"!=typeof window&&window.CHATBOT_API_URL?window.CHATBOT_API_URL:"http://localhost:8000";try{const e=await fetch(`${i}/translate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:t,target_language:"urdu"})});if(!e.ok)throw new Error(`Server error ${e.status}`);const s=await e.json();h(s.translated_text),n("translated")}catch(s){u(s.message),n("error")}},[]),m=(0,i.useCallback)(()=>{n("idle"),h(""),u("")},[]);return(0,c.jsxs)("div",{className:s,children:[("idle"===e||"error"===e)&&(0,c.jsxs)("div",{className:o,children:[(0,c.jsx)("button",{className:r,onClick:g,children:"\ud83c\udf10 Translate to Urdu"}),"error"===e&&(0,c.jsxs)("span",{className:d,children:["Translation failed: ",p]})]}),"loading"===e&&(0,c.jsx)("button",{className:r,disabled:!0,children:"\u23f3 Translating\u2026"}),"translated"===e&&(0,c.jsxs)("div",{children:[(0,c.jsx)("button",{className:a,onClick:m,children:"\ud83d\udcd6 Show Original English"}),(0,c.jsx)("div",{className:l,dir:"rtl",lang:"ur",children:t})]})]})}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);